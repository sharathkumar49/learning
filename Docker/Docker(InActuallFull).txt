

Docker Course Notes

1. Course Overview  
Instructor: Mosh Hamedani (CodeWithMosh.com)  
Goal: Take you from zero to pro with Docker—learn core concepts, then deploy a full-stack app.  

What You’ll Do:  
- Start with a simple “Hello, Docker!” project  
- Master 'docker run' and other key commands  
- Build and deploy a front-end + back-end + database stack  



2. Why Docker Matters  
- Isolation: Run apps in containers without polluting your host OS  
- Portability: “It works on my machine” really works across any Docker-enabled host  
- Efficiency: Lightweight compared to full virtual machines  

(Optional ASCII diagram of Docker architecture)

┌─────────────────────────┐
│       Host OS           │
│ ┌─────────────────────┐ │
│ │   Docker Engine     │ │
│ │ ┌───┐  ┌───┐  ┌───┐ │ │
│ │ │ C │  │ C │  │ C │ │ │
│ │ └───┘  └───┘  └───┘ │ │
│ └─────────────────────┘ │
└─────────────────────────┘
C = Container




3. Course Prerequisites  
1. Programming Experience  
   - ~3 months coding  
   - One completed app (any stack)  
2. Key Concepts  
   - Front-end, back-end, APIs, databases  
3. Git Basics  
   - Clone, commit, push, pull  

No need for prior Docker or containerization experience._



4. How to Get the Most from This Course  
1. Be Active  
   - Don’t just watch—pause, type, experiment.  
2. Take Smart Notes  
   - Jot down keywords and short summaries (paper or digital).  
3. Reproduce Every Demo  
   - After each lesson, rerun the commands. Tweak flags, break things, fix them.  
4. Ask Questions  
   - If something doesn’t behave as shown, dig in until it does.  

_Commit to practice, and by the end you’ll “speak Docker” fluently._



 5. What’s Next  
- Installing Docker Desktop / Engine on Windows, macOS, Linux  
- Docker images vs. containers  
- Writing your first Dockerfile  
- Networking containers  
- Docker Compose for multi-container apps  
- Best practices & troubleshooting  



Bonus Tip:  
Create a GitHub repo alongside each lesson. Push your Dockerfiles, 'docker-compose.yml' files, and notes—this builds your portfolio and cements your learning.


---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Section 1: Introduction to Docker

1.1 What You’ll Learn in This Section
- What Docker is and why it’s become so popular  
- The difference between Virtual Machines (VMs) and Containers  
- Docker’s architecture and core components  
- How to install Docker on your platform  
- A typical development workflow using Docker  
- A simple “Docker in Action” example to bring it all together  



1.2 What Is Docker?
- An open-source platform for containerization  
- Packages an application and its dependencies into a lightweight, portable unit called a container  
- Key benefits:  
  - Isolation: each container runs in its own sandbox  
  - Portability: same image works everywhere (dev, CI/CD, prod)  
  - Speed & Efficiency: starts in milliseconds; far less overhead than VMs  


1.3 Virtual Machines vs. Containers
1. Virtual Machines  
   - Hypervisor (e.g., VMware, Hyper-V) sits on the host OS (or bare metal)  
   - Each VM bundles a full guest OS, libraries, and the application  
   - Pros: Strong isolation, runs any OS  
   - Cons: Heavyweight, slower startup, large disk footprint  

2. Containers  
   - Leverage host OS kernel via container runtime (e.g., Docker Engine)  
   - Share the host kernel; only application code + libraries go inside the container  
   - Pros: Lightweight, super-fast startup, smaller images  
   - Cons: Less OS-level isolation (but still secure for most use cases)  

ASCII Illustration:

┌──────────────────────────────────────────────────┐
│                    Host OS                       │ 
│  ┌──────────────┐   ┌──────────────┐             │
│  │  VM #1       │   │  VM #2       │   Docker    │
│  │  (Guest OS)  │   │  (Guest OS)  │   Engine    │
│  └──────────────┘   └──────────────┘     │       │
│     ▲▲▲ Hypervisor                       │       │
│                                          ▼       │
│  ┌───────────┐  ┌───────────┐  ┌───────────┐     │
│  │Container1 │  │Container2 │  │Container3 │     │
│  │(App + Lib)│  │(App + Lib)│  │(App + Lib)│     │
│  └───────────┘  └───────────┘  └───────────┘     │
└──────────────────────────────────────────────────┘




1.4 Docker Architecture
- Docker Client: CLI ('docker' commands)  
- Docker Daemon ('dockerd'): background service that builds, runs, and manages containers  
- Images: read-only templates (built via Dockerfile)  
- Containers: runnable instances of images  
- Registry: stores and distributes images (e.g., Docker Hub, private registries)  

Workflow:
1. 'docker build' → Image  
2. 'docker push' → Registry  
3. 'docker pull' → Image to any host  
4. 'docker run' → Container  



1.5 Installing Docker
1. Windows & macOS  
   - Install Docker Desktop from docker.com  
   - Includes Docker Engine, Docker CLI, Docker Compose  
2. Linux  
   - Install Docker Engine via distro’s package manager (e.g., 'apt', 'yum')  
   - Optional: add your user to the 'docker' group  

Post-install checks:
- 'docker --version'  
- 'docker run hello-world'  



1.6 Development Workflow Overview
1. Write Code → Add a 'Dockerfile'  
2. Build Image → 'docker build -t <your-app> .'  
3. Run Container → 'docker run -d -p 80:80 <your-app>'  
4. Test & Debug → 'docker logs', 'docker exec -it'  
5. Push to Registry → 'docker push <your-repo>/<your-app>'  
6. Deploy Anywhere → Pull image, run container  



1.7 Docker in Action: Simple Example
# 1. Pull official “hello-world” image
docker pull hello-world

# 2. Run container from image
docker run hello-world

- What happens?  
  - Docker client asks daemon to create container  
  - Daemon downloads the image (if missing)  
  - Container runs a tiny program that prints a success message  



By the end of this section, you’ll  
- Understand why Docker matters  
- Know how containers differ from VMs  
- Be able to install Docker and run your first container  
- Have a mental map of the Docker components and workflow  
- Be ready for deeper dives: Dockerfiles, networking, Compose, and more!




Codes and Other Notes in this Discussion: 

In this Section: 
What is Docker?
Virtual Machines vs Containers
Architecture of Docker
Installing Docker
Development Workflow
Docker in Action 



---------------------------------------------------------------------------------------------------------------------------------------------------------------------
What Is Docker?

Docker is an open-source platform that lets you build, run, and ship applications in a consistent, isolated environment called a container. It solves the classic “it works on my machine” problem by packaging your app together with everything it needs—code, runtime, system tools, libraries, and settings—so it behaves identically across all machines that run Docker.



1. The “Works on My Machine” Problem

When you move an application from your dev box to another environment, things break for three common reasons:
1. Missing Files  
   – Some files (scripts, assets, configs) aren’t included in your deployment package.  
2. Software Version Mismatch  
   – Your app requires Node.js v14, but the target host has Node.js v9 installed.  
3. Different Configuration Settings  
   – Environment variables or config files differ between machines.




2. How Docker Solves It

- Containerizes Everything  
  You define a Docker image that bundles your code plus the exact versions of runtimes (e.g., Node.js, MongoDB) and libraries your app needs.

- Portable & Consistent  
  Once you’ve built your image, you can run it anywhere—your laptop, CI server, test cluster, or production host.

- Fast Onboarding  
  New team members don’t install dependencies manually. They run:
    '''bash
    docker-compose up
    '''
  Docker automatically pulls the right images and spins up your app’s containers.

- Version Isolation  
  Multiple apps can use different software versions side by side:
  '''
  ┌─────────────────┐
  │   Host Machine  │
  │ ┌─────────────┐ │    Docker Engine
  │ │ Container A │ │    ┌──────────────┐
  │ │ Node v14    │ │──▶ │ MyApp v1.0   │
  │ └─────────────┘ │    └──────────────┘
  │ ┌─────────────┐ │
  │ │ Container B │ │    ┌──────────────┐
  │ │ Node v9     │ │──▶ │ OtherApp v2.0│
  │ └─────────────┘ │    └──────────────┘
  └─────────────────┘
  '''

- Easy Cleanup  
  When you’re done with a project, remove its containers and images in one go:
  '''bash
  docker-compose down --rmi all
  '''
  No more scattered binaries or libraries cluttering your host.




3. Why Employers Love Docker Skills

- Reliability: Guarantees the same behavior across development, staging, and production.  
- Scalability: Containers spin up in milliseconds, ideal for microservices.  
- Maintainability: Clean separation of concerns; less system “drift.”  
- DevOps Alignment: Containerization is core to modern CI/CD pipelines and cloud deployments.  



4. Key Docker Commands Mentioned

- 'docker-compose up'  
  Starts all services defined in your 'docker-compose.yml' by pulling/building images and creating containers.

- 'docker-compose down --rmi all'  
  Stops and removes containers, networks, and optionally all images created by 'up'.



5. In a Nutshell

Docker lets you consistently build, run, and ship applications by encapsulating them in containers that:

- Include all dependencies and correct versions  
- Run identically anywhere Docker is installed  
- Spin up quickly and clean up cleanly  
- Allow multiple versions side by side  





Codes and Other Notes in this Discussion: 

Reasons:

One or more files missing
Software version mismatch  
Different Configuration settings 



docker-compose up

docker-compose down --rmi all 


---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Virtual Machines vs. Containers

Virtual Machines (VMs)

1 Definition  
- A VM is a full abstraction of physical hardware.  
- Runs a “guest OS” atop a “host OS” (or bare metal).  


2 How VMs Work  
1. Host Machine runs a Hypervisor (also called Virtual Machine Monitor).  
2. Hypervisor (e.g., VirtualBox, VMware, Hyper-V) creates/manages multiple VMs.  
3. Each VM includes:  
   - A full copy of a guest operating system.  
   - The application and its dependencies.  

ASCII Diagram:
'''
┌───────────────────────┐
│      Host OS          │
│  ┌─────────────────┐  │
│  │  Hypervisor     │  │
│  │ (VirtualBox,    │  │
│  │  VMware, Hyper-V)│ │
│  └────────┬────────┘  │
│ ┌─────────▼─────────┐ │
│ │  VM #1            │ │
│ │  (Windows + App A)│ │
│ └───────────────────┘ │
│ ┌───────────────────┐ │
│ │  VM #2            │ │
│ │  (Linux + App B)  │ │
│ └───────────────────┘ │
└───────────────────────┘
'''


3 Why Use VMs?  
- Isolation: Apps run in fully separate OS instances.  
- Flexibility: Each VM can run a different OS.  
- Reproducibility: Identical environments for dev, test, prod.  



4 Drawbacks of VMs  
- Full OS per VM:  
  - Licensing, patching, monitoring required for each guest OS.  
- Slow Startup:  
  - Booting a full OS takes seconds to minutes.  
- Resource-Heavy:  
  - You must allocate CPU, RAM, and disk to each VM upfront.  
  - Limits number of VMs (e.g., 3–5 on an 8 GB host).  





Containers

1 Definition  
- A container is a lightweight, isolated runtime environment that shares the host OS kernel.  
- Packages only the application code, libraries, and minimal dependencies.


2 How Containers Work  
1. Host Machine runs a Container Engine (e.g., Docker Engine).  
2. Engine manages images (templates) and containers (running instances).  
3. Containers share the host OS kernel—no separate guest OS.  

ASCII Diagram:
'''
┌─────────────────────────┐
│       Host OS           │
│ ┌─────────────────────┐ │
│ │  Container Engine   │ │
│ └───┬───────────┬─────┘ │
│  ┌──▼──┐     ┌──▼──┐    │
│  │App A│     │App B│    │
│  │+Libs│     │+Libs│    │
│  └─────┘     └─────┘    │
│  (Share host OS kernel) │
└─────────────────────────┘
'''


3 Benefits of Containers  
- Lightweight: No full guest OS per container.  
- Fast Startup: Typically < 1 second.  
- High Density: Run tens or hundreds of containers on a single host.  
- Single OS Management: Only the host OS needs licensing, patching, monitoring.  
- Dynamic Resource Use: Containers use resources on-demand—no fixed CPU/RAM slices.




Side-by-Side Comparison

| Feature              | Virtual Machines                            | Containers                                      |
|----------------------|---------------------------------------------|-------------------------------------------------|
| OS per instance      | Full guest OS                               | Shared host OS kernel                           |
| Startup time         | Seconds to minutes                          | Milliseconds to seconds                         |
| Resource overhead    | High (CPU, RAM, Disk for each VM)           | Low (no guest OS overhead)                      |
| Isolation level      | Strong (hardware-level via hypervisor)      | Moderate (kernel-level namespaces & cgroups)    |
| Density              | Few VMs per host                            | Dozens–hundreds of containers per host          |
| Management           | Patch & license each guest OS               | Patch & license only host OS                    |



Key Takeaways
- VMs give strong isolation and full-OS flexibility but are slow and resource-intensive.  
- Containers deliver lightweight isolation, rapid startup, and efficient resource usage.  
- Use VMs when you need multiple OSes or maximum isolation.  
- Use Containers for microservices, CI/CD pipelines, and high-density deployments.





Codes and Other Notes in this Discussion: 

Problems with VMs:
Each VM needs a full-blown OS
Slow to start 
Resource intensive


Containers: 
Allow running multiple apps in isolation
Are lightweight 
Use OS of the host
Start quickly 
Need less hardware resources 


---------------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker Architecture:
Understanding Docker’s architecture helps you see how its pieces fit together to build, ship, and run containers. At its core, Docker follows a client–server model and relies on the host’s kernel to isolate workloads.



1. Client–Server Model

- Docker Client ('docker' CLI)  
  - The user-facing command-line tool (or GUI/SDK) you interact with.  
  - Sends requests (e.g., 'docker build', 'docker run') over a RESTful API.

- Docker Daemon (Engine) ('dockerd')  
  - Background service that listens for Docker API requests.  
  - Orchestrates building images, creating containers, networking, and storage.

- Communication  
  '''text
  +------------+           HTTP API           +-------------+
  | Docker CLI | ───────────────────────────> | Docker      |
  |   (Client) |                              | Daemon      |
  +------------+ <──────────────────────────  | (Server)    |
                    status & results          +-------------+
  '''



2. Containers Are Just Processes

- When you 'docker run …', the daemon:
  1. Locates or builds an image (read-only template).
  2. Creates a container: a special OS process isolated via Linux namespaces & cgroups.
  3. Starts the process—nothing more than a normal OS process having its own filesystem, network stack, and resource limits.



3. Sharing the Host Kernel

- Kernel = the core of an OS (manages processes, memory, CPU, drivers).  
- Containers do not include a guest OS—they share the host kernel.  
  - Advantage: No duplicated OS overhead, rapid startup, high density.  
  - Limitation: Containerized apps must be compatible with the host kernel’s API.

'''text
┌─────────────────────────┐
│        Host OS          │
│ ┌─────────────────────┐ │
│ │    Docker Engine    │ │
│ └───┬─────────────▲───┘ │
│     │             │     │
│ ┌───▼───┐     ┌───▼───┐ │
│ │ App A │     │ App B │ │
│ │ +Libs │     │ +Libs │ │
│ └───────┘     └───────┘ │
│ (Share kernel, isolated │
│ by namespaces & cgroups)│
└─────────────────────────┘
'''




4. OS Compatibility

Linux Hosts
- Native support: containers use Linux kernel features directly.
- Only Linux containers run.

Windows Hosts
- Windows containers: share the Windows NT kernel.  
- Linux containers: Windows 10+ includes a custom Linux kernel (via WSL2); Docker spins up Linux containers natively alongside Windows ones.

macOS Hosts
- macOS kernel (XNU) lacks container primitives.  
- Docker for Mac runs a lightweight Linux VM under the hood:  
  - You see 'docker' CLI as if native.  
  - Behind the scenes: Docker Engine runs inside the VM.

'''text
macOS ──┐
        │ ┌──────────────────────────┐
        │ │  Lightweight Linux VM    │
        │ │ ┌──────────────────────┐ │
        │ │ │  Docker Engine       │ │
        │ │ └───┬──────────┬───────┘ │
        │ │ ┌───▼───┐  ┌───▼───┐     │
        │ │ │App A  │  │App B  │     │
        │ │ └───────┘  └───────┘     │
        │ └──────────────────────────┘
        └───────────────────────────
'''



5. Key Components Recap

| Component         | Role                                                |
|-------------------|-----------------------------------------------------|
| Docker Client     | User interface (CLI/GUI) sending API requests       |
| Docker Daemon     | Server that builds images and runs containers       |
| Images            | Read-only templates built from Dockerfiles          |
| Containers        | Running instances (processes) using shared kernel   |
| RESTful API       | Communication layer between client and daemon       |



Next Steps:  
1. Install Docker on your platform.  
2. Write and build your first Dockerfile.  
3. Run and inspect containers to see isolation in action.


---------------------------------------------------------------------------------------------------------------------------------------------------------------------

Installing Docker:

This guide walks you through installing the latest Docker version (e.g., 20.10.5) on macOS, Windows, or Linux. Every step, caveat, and troubleshooting tip is covered—no detail left behind.



1. Before You Begin  
1. Upgrade Existing Docker  
   - If Docker’s already installed, update to the latest release (e.g., 20.10.5) for compatibility with this course.  
2. Check System Requirements  
   - Visit Docker’s docs: https://docs.docker.com/get-started/get-docker/  
   - Confirm CPU architecture, RAM, OS version, virtualization support, etc., meet the prerequisites.



2. Downloading Docker  
Go to “Get Docker” page:  
'''text
https://docs.docker.com/get-started/get-docker/
'''  
Or simply search “install Docker” / “get Docker.”



3. Installing on macOS  
1. Download Docker Desktop  
   - Click “Download for Mac (Intel/M1)” on Docker Hub.  
   - File: '.dmg'.

2. Install  
   - Double-click the downloaded '.dmg'.  
   - Drag Docker.app into your Applications folder.

3. Start Docker  
   - Open Docker.app from Applications.  
   - Watch for the Docker whale icon in the macOS menu bar—it indicates Docker Engine is running.

4. Verify  
   - Open Terminal and run:
     '''bash
     docker version
     '''
   - You should see both Client and Server (Engine) version info.



4. Installing on Windows  
1. Download Docker Desktop  
   - On the same “Get Docker” page, choose Windows.  
   - File: '.exe'.

2. Enable Windows Features  
   - Open “Turn Windows features on or off” in Control Panel.  
   - Check Hyper-V and Containers.  
   - If prompted, reboot.

3. Install  
   - Run the downloaded '.exe' and follow the wizard.

4. Fix WSL2 Kernel Error (if shown)  
   - Error: “WSL2 installation is incomplete.”  
   - Click provided link (points to Microsoft’s WSL2 kernel MSI).  
   - Download & install MSI, then reboot.

5. Start Docker  
   - Launch Docker Desktop; wait for the whale icon in the system tray.

6. Verify  
   '''bash
   docker version
   '''
   - Ensure both Client and Server entries appear.



5. Installing on Linux  
1. Choose Your Distro  
   - Ubuntu/Debian: 'apt'  
   - CentOS/RHEL: 'yum'/'dnf'  
   - Fedora: 'dnf'  
   - Others: refer to official docs.

2. Official Engine Install  
   '''bash
   # Example for Ubuntu
   sudo apt update
   sudo apt install \
     ca-certificates \
     curl \
     gnupg \
     lsb-release

   # Add Docker’s GPG key & repo
   curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
   echo \
     "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] \
     https://download.docker.com/linux/ubuntu \
     $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

   sudo apt update
   sudo apt install docker-ce docker-ce-cli containerd.io
   '''

3. Post-Install (Optional)  
   - Add your user to 'docker' group to run without 'sudo':
     '''bash
     sudo usermod -aG docker $USER
     '''
   - Log out and back in.

4. Verify  
   '''bash
   docker version
   '''




6. Post-Install Verification  
Run in your terminal:
'''bash
docker version
docker run hello-world
'''
- 'docker version' shows Client & Server details.  
- 'docker run hello-world' confirms Docker Engine can pull images and start containers.



7. Installation Workflow Diagram

'''
┌───────────────┐
│  Download     │
│  Installer    │
└──────┬────────┘
       │
       ▼
┌──────┴───────┐
│  Install     │
│  (Drag&Drop  │
│   or Wizard) │
└──────┬───────┘
       │
       ▼
┌──────┴────────┐
│  Start Docker │
│  App/Service  │
└──────┬────────┘
       │
       ▼
┌──────┴──────────┐
│  Verify in CLI  │
│  (docker version│
│   + hello-world)│
└─────────────────┘
'''



8. Troubleshooting Tips  
- Docker icon not visible: Ensure Docker Desktop is running.  
- 'Server' missing in 'docker version': Docker Engine failed to start—restart the app & your machine.  
- Permission errors on Linux: Add your user to the 'docker' group.  
- Search the error message: Hundreds of solutions are available online.  
- Ask the community: forum.codewithmosh.com



9. Additional Resources  
- Official install docs:  
  https://docs.docker.com/get-started/get-docker/  
- Docker Community Forums:  
  https://forums.docker.com  
- Mosh’s support forum:  
  https://forum.codewithmosh.com  




Codes and Other Notes in this Discussion: 

docker version

https://docs.docker.com/get-started/get-docker/


---------------------------------------------------------------------------------------------------------------------------------------------------------------------

Development Workflow: 



Docker Development Workflow:
This workflow shows how you turn any application—regardless of language or framework—into a Docker-powered, portable unit. Every step, file, and component is covered.



1. Dockerizing an Application:
1. Start with your app  
   - Could be Node.js, Python, Java, Go, etc.  
   - Folder contains source code, configs, tests…

2. Add a 'Dockerfile'  
   - A plain-text recipe with instructions for Docker to build an image.  
   - Lives at your project root (named exactly 'Dockerfile').



2. Anatomy of a 'Dockerfile':
A typical 'Dockerfile' includes:

- Base Image  
  'FROM node:14-alpine'  
  (a minimal Node.js + Linux distro)

- Metadata & Labels (optional)  
  'LABEL maintainer="you@example.com"'

- Environment Variables  
  'ENV NODE_ENV=production'

- Working Directory  
  'WORKDIR /app'

- Copy Files  
  'COPY package*.json ./'  
  'COPY src/ ./src/'

- Install Dependencies  
  'RUN npm install --only=production'

- Expose Ports  
  'EXPOSE 3000'

- Startup Command  
  'CMD ["node", "src/index.js"]'




3. Building the Docker Image:
'''bash
docker build -t your-app:1.0.0 .
'''

- '-t your-app:1.0.0' tags the image with name 'your-app' and version '1.0.0'.  
- The context ('.') is the current directory (all files copied in build steps come from here).



4. Image Composition:
Your built image includes:

- A cut-down OS (e.g., Alpine Linux)  
- A runtime (e.g., Node.js, Python interpreter)  
- Application files (source code, assets)  
- Third-party libraries (NPM packages, pip modules)  
- Environment variables and metadata  



5. Running a Container Locally:
'''bash
docker run \
  -d \                   # detached (background)
  -p 3000:3000 \         # map host port 3000 ➔ container port 3000
  --name my-app-container \
  your-app:1.0.0
'''

- Creates a container (process) from 'your-app:1.0.0'.  
- Isolated filesystem comes from the image layers.  
- Networking, PID, memory, and CPU can be limited via flags or Compose.



6. Pushing to a Registry (Docker Hub):
1. Log in to Docker Hub:  
   '''bash
   docker login
   '''
2. Tag for your repo:  
   '''bash
   docker tag your-app:1.0.0 username/your-app:1.0.0
   '''
3. Push the image:  
   '''bash
   docker push username/your-app:1.0.0
   '''

> Docker Hub is to Docker images what GitHub is to Git repositories.




7. Deploying on Any Host:
On any machine with Docker:

1. Pull the image:  
   '''bash
   docker pull username/your-app:1.0.0
   '''
2. Run the container (same as local):  
   '''bash
   docker run -d -p 3000:3000 username/your-app:1.0.0
   '''

_All configuration, dependencies, and code are baked into the image—no more “missing file” or “wrong version” errors._



8. Cleanup & Version Control:
- Remove containers/images when done:  
  '''bash
  docker rm my-app-container
  docker rmi username/your-app:1.0.0
  '''
- Version your images using semver tags ('1.0.0', '1.0.1', 'latest') to track releases.
- Keep Dockerfile under version control (Git)—it’s your single source of truth for builds.




9. Workflow Diagram:
'''
┌─────────────────┐
│  Codebase +     │
│  Dockerfile     │
└─────────────────┘
         │
         ▼
┌─────────────────┐
│ docker build    │─► Builds ► Image (app + OS + runtime + libs)
└─────────────────┘
         │
         ▼
┌─────────────────┐
│ docker run      │─► Spins up ► Container (isolated process)
└─────────────────┘
         │
         ▼
┌───────────────────────────┐
│ docker push (Registry)    │
│ docker pull & run (Prod)  │
└───────────────────────────┘
'''



In Summary  
1. Dockerfile = build instructions.  
2. Image = portable snapshot of your app + all dependencies.  
3. Container = running instance (isolated process).  
4. Registry = share images across environments.  

This workflow eliminates complex release docs—everything your app needs is codified in the Dockerfile.





Codes and Other Notes in this Discussion: 

Image: 

A cut-down OS
A runtime environment (eg. Node)
Application Files 
Third Party Libraries 
Environment Variables 

---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Docker in Action: 

Docker in Action: End-to-End Demo:
This walkthrough shows the big picture of a typical Docker workflow. We’ll dockerize a tiny Node.js program, build an image, run it locally, push to Docker Hub, and pull/run it on a fresh VM.



1. Setup the Project
1. Open a terminal and navigate to your desktop:
   '''bash
   cd ~/Desktop
   '''
2. Create and enter a project folder:
   '''bash
   mkdir hello-docker
   cd hello-docker
   '''
3. Launch your code editor (e.g., VS Code):
   '''bash
   code .
   '''
4. If prompted, install recommended Docker extensions in VS Code—this helps with syntax highlighting and IntelliSense for your Dockerfile.



2. Create the Sample App
1. Add app.js in the project root:
   '''js
   // app.js
   console.log('hello docker');
   '''
2. Verify it runs (requires Node.js installed locally):
   '''bash
   node app.js
   # ➜ hello docker
   '''



3. Without Docker: Four Manual Steps
To run on another machine, you’d need to:
1. Provision an OS  
2. Install Node.js  
3. Copy your app files  
4. Execute 'node app.js'

With complex apps, this becomes a fragile, error-prone release document. Docker automates all four steps.



4. Add Your Dockerfile
1. Create a file named Dockerfile (capital “D”, no extension) in the project root.
2. Populate it with:
   '''
   FROM node:alpine            # 1. Base image = Node on tiny Alpine Linux
   COPY . /app                 # 2. Copy all files into /app in the image
   WORKDIR /app                # 3. Set working directory to /app
   CMD node app.js             # 4. Default command to run your app
   '''
3. Save the file—VS Code will colorize Dockerfile keywords now that you’ve installed the extension.



5. Build the Docker Image
In your terminal, run:
'''bash
docker build -t hello-docker .
'''
- '-t hello-docker' assigns the image name hello-docker and default tag latest.  
- '.' tells Docker to find the Dockerfile and build context here.



6. Inspect Your Image
List all local images:
'''bash
docker image ls
'''
You’ll see something like:
'''
REPOSITORY     TAG       IMAGE ID       CREATED         SIZE
hello-docker   latest    d9f1e0c8...    10 seconds ago  112MB
'''
- 112 MB = Alpine Linux + Node.js + your app files.



7. Run Your Container Locally
'''bash
docker run --rm hello-docker
# ➜ hello docker
'''
- '--rm' auto-removes the container after it exits.  
- The image contains everything—no local Node install needed.



8. Publish to Docker Hub
1. Tag for your Docker Hub repo (replace 'username'):
   '''bash
   docker tag hello-docker username/hello-docker:latest
   '''
2. Login to Docker Hub:
   '''bash
   docker login
   '''
3. Push the image:
   '''bash
   docker push username/hello-docker:latest
   '''

Docker Hub = GitHub for images.



9. Pull & Run on a Fresh VM
Use any host with Docker (e.g., [Play with Docker](https://labs.play-with-docker.com/)):

1. Verify no Node installed:
   '''bash
   node
   # ➜ command not found: node
   '''
2. Check Docker version:
   '''bash
   docker version
   # ➜ 20.10.x
   '''
3. Pull your image:
   '''bash
   docker pull username/hello-docker:latest
   '''
4. List images:
   '''bash
   docker image ls
   '''
5. Run:
   '''bash
   docker run --rm username/hello-docker:latest
   # ➜ hello docker
   '''

Despite the VM lacking Node.js, Docker pulls your image and runs your app identically.




10. Workflow Diagram

'''
Local Dev Machine                Docker Hub                 Test/Prod Host
┌──────────────────┐              ┌───────────┐              ┌──────────────────┐
│  app.js +        │              │           │              │                  │
│  Dockerfile      │              │           │              │                  │
└─────────┬────────┘              │           │              └─────┬────────────┘
          │ docker build          │  docker   │ docker pull        │ docker run
          └─────────────────────► │   push    │ ◄──────────────────┘
                                ┌─┴───────────┴───┐                        │
                                │  username/hello │                        ▼
                                │   -docker:latest│                “hello docker”
                                └─────────────────┘




Summary
1. Write app + Dockerfile  
2. Build image → 'docker build -t hello-docker .'  
3. Run container locally → 'docker run hello-docker'  
4. Push to Hub → 'docker push username/hello-docker'  
5. Pull & Run anywhere → 'docker pull' + 'docker run'





Codes and Other Notes in this Discussion: 

Instructions: 

Start with an OS
Install Node 
Copy app files 
Run node app.js


https://hub.docker.com/



Dockerfile:

FROM node:alpine
COPY . /app
WORKDIR /app
CMD node app.js


docker build -t hello-docker .

docker image ls 

docker run hello-docker


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Summary:

Terms Involved:
Client/server architecture
Containers
Dockerfiles
Docker daemon (engine)
Docker registries
Hypervisors
Images
Kernel
Process
Virtual machines



Summary
• Docker is a platform for consistently building, running, and shipping applications.
• A virtual machine is an abstraction of hardware resources. Using hypervisors we can create and manage virtual machines. The most popular hypervisors are VirtualBox, VMware and Hyper-v (Windows-only).
• A container is an isolated environment for running an application. It’s essentially an operating-system process with its own file system.
• Virtual machines are very resource intensive and slow to start. Containers are very lightweight and start quickly because they share the kernel of the host (which is already started).
• A kernel is the core of an operating system. It’s the part that manages applications and hardware resources. Different operating system kernels have different APIs. That’s why we cannot run a Windows application on Linux because under the hood, that application needs to talk to a Windows kernel.
• Windows 10 now includes a Linux kernel in addition to the Windows kernel. So we can run Linux applications natively on Windows.
• Docker uses client/server architecture. It has a client component that talks to the server using a RESTful API. The server is also called the Docker engine (or daemon) runs in the background and is responsible for doing the actual work.
• Using Docker, we can bundle an application into an image. Once we have an image, we can run it on any machine that runs Docker.
• An image is a bundle of everything needed to run an application. That includes a cutdown OS, a runtime environment (eg Node, Python, etc), application files, thirdparty libraries, environment variables, etc.
• To bundle an application into an image, we need to create a Dockerfile. A Dockerfile contains all the instructions needed to package up an application into an image.
• We can share our images by publishing them on Docker registries. The most popular Docker registry is Docker Hub.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------



The Linux Command Line: 

Introduction: 

Alright, the next stop in our journey is the Linux command line. But why Linux? What if you're a Windows user? Well, you still need to know a bit of Linux for a number of reasons. For starters, Docker has its foundations built on top of basic Linux concepts.

So if you want to be productive and troubleshoot issues easily, you need to know some of the basic Linux commands. Also, most tutorials online are based on Linux commands, so if you don't understand these basic commands, you're not going to get far. In my opinion, learning Linux is like learning English. I think everybody should know some English these days. You don't need to speak it or write a book in it, but you need to understand it.

So unless you're a power Linux user, do not skip this section. It's going to be super easy and extremely useful. So, let's jump in and get started.





Linux Distributions: 

Linux Distributions (Distros)
Linux isn’t a single operating system but a collection of open-source kernels wrapped by various communities into full-fledged operating systems called distributions (or distros). Each distro meets different goals—servers, desktops, embedded devices, minimal footprints, security, etc.



1. Why So Many Distros?

- Open Source Roots  
  Anyone can take the Linux kernel, combine it with tools (GNU coreutils, systemd, etc.), and package a custom OS.  
- Specialized Needs  
  - Servers: stability, long-term support (e.g., CentOS, Ubuntu LTS)  
  - Desktops: user-friendly GUIs and app stores (e.g., Ubuntu Desktop, Fedora Workstation)  
  - Containers/CI: ultralight images (e.g., Alpine Linux)  
  - Security-Focused: hardened defaults, built-in firewalls (e.g., Tails, Qubes)  
- Community & Commercial Backing  
  Some distros are community-driven (Debian), others backed by companies (Red Hat → Fedora/CentOS).



2. Popular Distros

| Distribution | Focus                         | Package Manager      | Latest Notable Release    |
|--------------|-------------------------------|----------------------|---------------------------|
| Ubuntu   | General-purpose, servers, desktop | 'apt' (DEB packages) | 24.04 LTS (“Noble Numbat”) |
| Debian   | Foundation of many distros        | 'apt'                | 12 “Bookworm”             |
| Alpine   | Minimal, container-optimized      | 'apk'                | 3.18                      |
| Fedora   | Cutting-edge desktop/server       | 'dnf'                | 40 (“Shiba Inu”)          |
| CentOS   | Community rebuild of RHEL         | 'yum' / 'dnf'        | Stream 9                  |
| Others   | Arch, Manjaro, OpenSUSE, etc.     | 'pacman', 'zypper', …| —                         |

> Note: Over 1,000 distros exist, but most share core Linux commands. You might see minor quirks (different init systems, repo URLs, service managers).



3. Command Variations

While basic shell commands ('ls', 'cd', 'grep', 'tar') are universal, some administrative tasks differ:

- Installing Software  
  - Debian/Ubuntu: 'sudo apt update && sudo apt install package'  
  - Fedora/CentOS: 'sudo dnf install package' (or 'yum install package')  
  - Alpine: 'sudo apk add package'
- Managing Services  
  - Most modern distros (Ubuntu, Fedora): 'sudo systemctl start|stop|status service'  
  - Older or minimal distros: may use 'service name start' or custom scripts.
- Locating Files  
  - 'which cmd', 'whereis cmd', 'locate cmd' (requires updated database).




4. Which Distro for This Course?

- Primary: Ubuntu Linux (widely used, beginner-friendly, excellent docs).  
- Alternatives: Any distro with Docker support works—feel free to use your preferred flavor.



5. Key Takeaways

1. Distros are variants of the Linux kernel + tooling to meet different use cases.  
2. Most shell commands are consistent, but package/service management commands vary.  
3. Ubuntu is the standard for our examples—others will follow the same principles.  
4. You only need enough Linux fluency to navigate files, install packages, and troubleshoot containers.






Codes and Other Notes in this Discussion: 

Distros:

Ubuntu
Debian 
Alpine 
Fedora
CentOS

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Running Linux:
Running Ubuntu in Docker
This lesson demonstrates how to launch and interact with an Ubuntu container. Take notes on commands, behaviors, and shell tricks—then practice afterward.



1. Pulling & Running Ubuntu

1. In your terminal, run:
   '''bash
   docker run ubuntu
   '''
   - If the ubuntu image isn’t local, Docker downloads it from Docker Hub.
   - Docker then creates a container, runs it, and immediately exits (no interactive shell).

2. Verify what happened:
   - List running containers:
     '''bash
     docker ps
     '''
     → No containers listed (because ours exited right away).
   - List all containers (including stopped):
     '''bash
     docker ps -a
     '''
     → Shows an entry for the Ubuntu container with status “Exited”.




2. Starting an Interactive Shell

To keep the container running and open a shell:

'''bash
docker run -it ubuntu
'''

- '-i' = interactive (keep STDIN open)  
- '-t' = allocate a pseudo-TTY (gives you a shell prompt)  

Now you’re dropped into a Bash shell inside the container.




3. Understanding the Shell Prompt

Example prompt:
'''
root@d8f9a7b1e3c2:/#
'''

Breakdown:
- root → current user (highest privileges); a normal user would see '$' instead of '#'.  
- d8f9a7b1e3c2 → container ID (acts like the machine’s hostname).  
- / → current working directory (root of the filesystem).  
- # → shell ready for commands (as root).




4. Basic Commands Inside the Container

- Print text:
  '''bash
  echo hello
  # → hello
  '''
- Show current user:
  '''bash
  whoami
  # → root
  '''
- Find the shell executable:
  '''bash
  echo $0
  # → /bin/bash
  '''
  - '/bin/bash' is the “Bourne Again SHell,” an enhanced version of the original Unix shell.




5. Linux Path & Case Sensitivity

- Path separators use forward slashes ('/'), not backslashes ('\').  
- Case matters:  
  '''bash
  echo hello    # works
  Echo hello    # “command not found”
  '''
  This applies to filenames, commands, variables—everywhere.




6. Shell Navigation & History

- Arrow keys scroll through recent commands (↑ for previous, ↓ for next).  
- View history:
  '''bash
  history
  '''
- Repeat a command by number:
  '''bash
  !42
  '''
  runs the command listed as entry 42 in your history.



7. Try It Yourself

1. Run 'docker run ubuntu' and inspect 'docker ps -a'.  
2. Launch an interactive shell with 'docker run -it ubuntu'.  
3. Execute 'echo', 'whoami', 'echo $0', and note the prompt structure.  
4. Test case sensitivity and practice history navigation.

After practicing, you’ll be comfortable using basic Linux commands inside any container.



Codes and Other Notes in this Discussion: 

docker run ubuntu 

docker ps 

docker ps -a 

docker run it ubuntu 



--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Managing Packages: 

Managing Packages with apt:
Modern OSes and platforms include package managers to install, update, and remove software. In Ubuntu (and many Debian-based distros), that tool is apt (Advanced Package Tool). You’ve likely used similar tools before: npm, yarn, pip, NuGet, etc. Here’s everything you need to know about apt and a step-by-step example installing, verifying, and removing a package.



1. apt vs. apt-get
- apt  
  - A newer, user-friendly front-end for package management  
  - Combines common commands from 'apt-get', 'apt-cache', and others  
- apt-get  
  - The traditional low-level tool you still see in older tutorials  

> We’ll use 'apt' for simplicity and clarity.



2. Common 'apt' Subcommands

| Subcommand            | Purpose                                      |
|-----------------------|----------------------------------------------|
| 'apt list'            | List available packages (or installed ones)  |
| 'apt search <pkg>'    | Search for packages by name or keyword      |
| 'apt show <pkg>'      | Show package details (version, deps, etc.)   |
| 'apt update'          | Refresh local package database indexes   |
| 'apt install <pkg>'   | Install or upgrade a package               |
| 'apt reinstall <pkg>' | Reinstall an already installed package     |
| 'apt remove <pkg>'    | Uninstall a package (leaves config files)   |
| 'apt purge <pkg>'     | Uninstall and remove config files       |



3. Why Run 'apt update' First?

- The package database (indexes of available software) can become stale.  
- Without updating, 'apt install' may report “Unable to locate package” even if the package exists upstream.  
- Rule of thumb:  
  '''bash
  sudo apt update
  sudo apt install <package>
  '''



4. Example: Installing & Removing 'nano'

4.1 Attempting to Install Without Updating
'''bash
$ sudo apt install nano
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
E: Unable to locate package nano
'''


4.2 Listing Packages
'''bash
$ apt list        # Shows what’s in the local database
Listing... Done
apt/bionic-updates,bionic-security 1.6.12ubuntu0.2 amd64 [installed]
...
# Notice many packages lack "[installed]"—they're not yet available locally
'''


4.3 Updating the Package Database
'''bash
$ sudo apt update
Hit:1 http://us.archive.ubuntu.com/ubuntu bionic InRelease
Get:2 http://us.archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]
...
Reading package lists... Done
Building dependency tree... Done
'''

> The system downloaded the latest package indexes from Ubuntu’s repositories.


4.4 Installing 'nano'
'''bash
$ sudo apt install nano
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following NEW packages will be installed:
  nano
0 upgraded, 1 newly installed, 0 to remove, 0 not upgraded.
Need to get 230 kB of archives.
After this operation, 662 kB of additional disk space will be used.
Do you want to continue? [Y/n] Y
'''


4.5 Verifying 'nano'
'''bash
$ nano
'''
- Opens the nano text editor.  
- Key shortcuts (displayed at bottom):  
  - 'Ctrl+X' → exit  
  - 'Y' / 'N' → save changes?  
- To clear your terminal screen: 'Ctrl+L'.


4.6 Removing 'nano'
'''bash
$ sudo apt remove nano
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages will be REMOVED:
  nano
0 upgraded, 0 newly installed, 1 to remove, 0 not upgraded.
After this operation, 662 kB disk space freed.
Do you want to continue? [Y/n] Y

$ nano
bash: nano: command not found
'''



5. Best Practices & Takeaways

1. Always run 'sudo apt update' before installing.  
2. Use 'apt list' or 'apt search <pkg>' to confirm a package’s name exists.  
3. Install ('apt install'), verify (run the program), then remove ('apt remove') if needed.  
4. For a cleaner uninstall, use 'sudo apt purge <pkg>' to delete config files too.  



6. Practice Exercise

In the same Ubuntu container (or VM), install and remove Python:

1. 'sudo apt update'  
2. 'sudo apt install python3'  
3. Run 'python3 --version' to confirm installation.  
4. 'sudo apt remove python3'  
5. Verify with 'python3' → 'command not found'.



With these steps and commands, you can confidently manage packages in any Debian-based Linux environment.




Codes and Other Notes in this Discussion: 

Package Managers: 
npm
yarn
pip
NuGet


apt install nano 

apt list 

apt update 

apt remove nano 
--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Linux File System: 

Linux File System Hierarchy
Linux organizes all files and directories in a single tree structure, rooted at '/'. Below is the top level of that hierarchy, with each directory’s purpose and origin note.

'''
/
├── bin
├── boot
├── dev
├── etc
├── home
├── lib
├── proc
├── root
└── var
'''




1. Top-Level Directories

1. '/'  
   – Root directory of the entire file system.  
   – All other files and directories branch from here.

2. '/bin'  
   – Binaries: Essential user commands (e.g., 'ls', 'cp', 'bash').  
   – Available to all users and required for single-user mode.

3. '/boot'  
   – Boot loader files: Kernel images ('vmlinuz'), initrd, GRUB configs.  
   – Read-only once the system is running.

4. '/dev'  
   – Device files: Interfaces to hardware and virtual devices (e.g., '/dev/sda', '/dev/null').  
   – In Linux, *everything is a file*—including disks, terminals, and random number generators.

5. '/etc'  
   – Configuration files: System-wide settings (e.g., '/etc/passwd', '/etc/hosts').  
   – Often thought of as “Editable Text Configuration.”

6. '/home'  
   – User home directories:  
     '''
     /home/alice
     /home/bob
     '''
   – Personal files and settings for non-privileged users.

7. '/lib'  
   – Shared libraries and kernel modules required by '/bin' and '/sbin' programs.  
   – Contains '.so' files (shared objects).

8. '/proc'  
   – Process pseudo-filesystem: Virtual files representing running processes and kernel info.  
   – e.g., '/proc/1234' holds data about process ID 1234; '/proc/cpuinfo' shows CPU details.

9. '/root'  
   – Home directory for the root user (system administrator).  
   – Separate from '/home' because root’s privileges are system-wide.

10. '/var'  
    – Variable data: Files that change frequently: logs ('/var/log'), mail spools ('/var/mail'), caches, databases.  




2. Key Concepts

- Single Root Tree  
  Unlike Windows (with separate drives like 'C:\'), Linux has one unified tree under '/'.

- Everything Is a File  
  Devices ('/dev'), directories, sockets, and even processes ('/proc') are represented as files.

- Case Sensitivity  
  File and command names are case‐sensitive:  
  '''bash
  /home/Docs ≠ /home/docs
  '''



3. Why It Matters

- Navigation & Scripting  
  Familiarity with these directories helps you:  
  - Locate config files ('/etc/nginx/nginx.conf')  
  - Inspect logs ('/var/log/syslog')  
  - Access device interfaces ('/dev/ttyUSB0')  
- Docker & Containers  
  Containers mirror this filesystem layout—knowing where things live aids debugging and image creation.



You don’t need to memorize every directory today. As you work in Linux and containers, these locations will become second nature.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Navigating the File System: 

Navigating the Linux File System:
Linux uses a single hierarchical tree of directories, rooted at '/'. You move around this tree with a few basic commands.



1. Seeing Where You Are: 'pwd'
- Command: 'pwd'  
- Prints the absolute path of your current working directory.  
  '''bash
  $ pwd
  /
  '''
  A single '/' means you’re at the root of the file system.



2. Listing Contents: 'ls'
- Basic:  
  '''bash
  ls
  '''
  Lists files and subdirectories in columns.

- One Item per Line:  
  '''bash
  ls -1
  '''
  *(Flag: '-1')*  
- Long (Detailed) Listing:  
  '''bash
  ls -l
  '''
  *(Flag: '-l')*  
  Shows for each entry:
  1. Permissions (e.g., 'drwxr-xr-x')  
  2. Links  
  3. Owner  
  4. Group  
  5. Size (bytes)  
  6. Last-modified date/time  
  7. Name



3. Changing Directory: 'cd'
- Absolute Path (starts with '/'):  
  '''bash
  cd /etc/apt
  '''
- Relative Path (relative to 'pwd'):  
  '''bash
  pwd            # e.g. /
  cd etc/apt     # now at /etc/apt
  '''
- Go Up One Level:  
  '''bash
  cd ..
  '''
- Go Up Two Levels:  
  '''bash
  cd ../..
  '''
- Home Directory Shortcut ('~'):  
  '''bash
  cd ~           # your home (e.g., /root or /home/username)
  cd             # same as 'cd ~'
  '''



4. Tab Completion
- Auto-complete file & directory names with 'Tab'.  
- Single Match: completes it.  
- Multiple Matches: press 'Tab' twice to list all possibilities:
  '''bash
  # Suppose /etc has “apt”, “alternatives”, “adduser.conf”
  cd a <Tab><Tab>
  adduser.conf/  alternatives/  apt/
  '''



5. Listing Another Directory Without 'cd'
- Directly pass a path to 'ls':
  '''bash
  ls /bin
  '''
  You’ll see system binaries ('pwd', 'echo', etc.) without changing your location.



6. Case Sensitivity & Path Separators
- Forward slash ('/') separates directories (not '\' as in Windows).  
- Case matters:  
  '''bash
  ls /Home     # Different from /home → likely “No such file or directory”
  ls /home     # Correct
  '''



7. Quick Reference of Commands

| Action                      | Command                              |
|-----------------------------|--------------------------------------|
| Print current directory     | 'pwd'                                |
| List contents               | 'ls'                                 |
| List one-per-line           | 'ls -1'                              |
| Detailed list               | 'ls -l'                              |
| Change to absolute path     | 'cd /path/to/dir'                    |
| Change to relative path     | 'cd subdir'                          |
| Go up one level             | 'cd ..'                              |
| Go to home directory        | 'cd ~' or simply 'cd'                |
| Auto-complete names         | '<Tab>'                              |
| List another dir’s contents | 'ls /other/directory'                |



Practice:  
1. Start at '/' with 'pwd'.  
2. 'ls', 'ls -1', 'ls -l' to compare outputs.  
3. 'cd etc', 'ls', then 'cd apt' using tab completion.  
4. 'cd ..' twice to return to '/'.  
5. 'ls /bin' and spot common binaries like 'pwd' and 'echo'.  
6. 'cd ~' to confirm you land in your home directory.

With these commands, you’ll confidently explore any Linux filesystem—inside or outside Docker containers.



Codes and Other Notes in this Discussion: 

pwd
ls
ls -1
ls -l

ls /bin


cd /root

or 
cd ..
cd ~

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Manipulating Files and Directories:


Manipulating Files and Directories in Linux
Learn how to create, rename/move, and delete directories and files using basic shell commands. Follow along in your home directory ('~').



1. Get to Your Home Directory  
'''bash
cd ~
'''



2. Create a Directory: 'mkdir'

- Command:  
  '''bash
  mkdir test
  '''
- Verify:  
  '''bash
  ls -l
  '''  
  (Directories may appear in blue in your terminal.)



3. Rename or Move a Directory: 'mv'

- Rename 'test' to 'docker':  
  '''bash
  mv test docker
  '''
- Move to another location (example):  
  '''bash
  mv docker /path/to/target/
  '''



4. Enter the Directory & Create Files: 'touch'

'''bash
cd docker
'''

- Create one file:  
  '''bash
  touch hello.txt
  '''
- Create multiple files at once:  
  '''bash
  touch file1.txt file2.txt file3.txt
  '''
- Verify with one item per line:  
  '''bash
  ls -1
  '''
  '''
  file1.txt
  file2.txt
  file3.txt
  hello.txt
  '''



5. Rename or Move Files: 'mv' & Shell Shortcuts

- Rename 'hello.txt' to 'hello-docker.txt':  
  '''bash
  mv hello.txt hello-docker.txt
  '''
- Shell shortcut:  
  - Press 'Tab' to auto-complete filenames.  
  - Press 'Ctrl+W' to erase the previous word when editing the command line.



6. Remove Files: 'rm'

- Delete specific files:  
  '''bash
  rm file1.txt file2.txt
  '''
- Use patterns (globbing):  
  '''bash
  rm file*
  '''
- Verify remaining files:  
  '''bash
  ls
  # → hello-docker.txt
  '''



7. Delete a Directory: 'rm -r'

- Remove a non-empty directory recursively:  
  '''bash
  cd ~        # go up from ~/docker if inside it
  rm -r docker
  '''
- Error if you omit '-r':  
  '''bash
  rm docker
  # → rm: cannot remove 'docker': Is a directory
  '''



8. Command Summary

| Action                        | Command                                        |
|-------------------------------|------------------------------------------------|
| Change to home                | 'cd ~'                                         |
| Make directory                | 'mkdir <dir>'                                  |
| Rename/move file or directory | 'mv <old> <new>'                               |
| Create empty file(s)          | 'touch <file1> [file2 ...]'                    |
| List one-per-line             | 'ls -1'                                        |
| Delete file(s)                | 'rm <file1> [file2 ...]'                       |
| Delete directory recursively  | 'rm -r <dir>'                                  |
| Auto-complete names           | '<Tab>'                                        |
| Delete previous word in shell | 'Ctrl+W'                                       |



9. Practice Exercise

1. In your home directory, create several directories and files.  
2. Rename/move them using 'mv' and tab completion.  
3. Delete individual files with 'rm' and patterns.  
4. Finally, remove directories with 'rm -r'.  

Mastering these commands gives you full control over your Linux filesystem.


Codes and Other Notes in this Discussion: 

mkdir test 
mv test docker 
touch hello.txt 
touch file1.txt file2.txt file3.txt 

mv hello.txt /etc 
mv hello.txt hello-docker.txt 

rm file1.txt file2.txt 
rm file*


rm -r docker/ 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Editing and Viewing Files: 

Editing and Viewing Files in Linux:
This section covers installing a basic editor, writing to files, and multiple ways to view file contents—step by step with every detail.



1. Install a Text Editor: 'nano'

1. Install 'nano' (if not already present)  
   '''bash
   sudo apt update
   sudo apt install nano
   '''
2. Launch 'nano' with (or without) a filename  
   '''bash
   nano file1.txt
   '''
   - If 'file1.txt' doesn’t exist, 'nano' creates it.

3. Edit text  
   - Type any content you like.

4. Save & Exit  
   - Press 'Ctrl + X'.  
   - When prompted 'Save modified buffer?', press 'Y'.  
   - Confirm the filename ('file1.txt') by pressing 'Enter'.

5. Verify  
   '''bash
   ls -l file1.txt
   # Ensure the file exists and has size > 0
   '''




2. Quick View: 'cat'

- Purpose: Display entire file in one shot (short files only).
- Command:
  '''bash
  cat file1.txt
  '''
- Origin: Short for concatenate (can join multiple files).



3. Paged Viewing: 'more' vs. 'less'

3.1 'more'

1. Use when you want to scroll forward one page/line at a time  
   '''bash
   more /etc/adduser.conf
   '''
2. Controls  
   - 'Space' → next page  
   - 'Enter' → next line  
   - Limitation: cannot scroll back up  
3. Exit:  
   - Press 'q'


3.2 'less'

1. Install 'less' (if missing):  
   '''bash
   sudo apt install less
   '''
2. Browse interactively  
   '''bash
   less /etc/adduser.conf
   '''
3. Controls  
   - 'Space' → next page  
   - 'Enter' → next line  
   - 'Up'/'Down' arrows → scroll line by line backward or forward  
4. Exit:  
   - Press 'q'



4. View File Segments: 'head' & 'tail'
- First N lines:  
  '''bash
  head -n 5 /etc/adduser.conf
  '''
- Last N lines:  
  '''bash
  tail -n 5 /etc/adduser.conf
  '''




5. Command Summary

| Task                           | Command                                   |
|--------------------------------|-------------------------------------------|
| Install 'nano'                 | 'sudo apt update && sudo apt install nano' |
| Edit/create a file             | 'nano <filename>'                         |
| View small file                | 'cat <filename>'                          |
| Page forward only              | 'more <file>'                             |
| Page forward/backward          | 'less <file>'                             |
| Preview first 5 lines          | 'head -n 5 <file>'                        |
| Preview last 5 lines           | 'tail -n 5 <file>'                        |



Practice:  
1. Create a new text file with 'nano', add multiple lines.  
2. View with 'cat' (short file).  
3. Try 'more' and observe inability to scroll up.  
4. Install and use 'less' for full navigation.  
5. Use 'head' and 'tail' to see file excerpts.




Codes and Other Notes in this Discussion: 

apt install nano
nano file1.txt 

cat file1.txt 

more /etc/adduser.conf 

less /etc/adduser.conf 

head -n 5 /etc/adduser.conf  

tail -n 5 /etc/adduser.conf

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Redirection: 


Redirection in Linux Shell
Redirection lets you reroute a command’s standard input (stdin) or standard output (stdout) from/to files instead of the keyboard or screen.



1. Standard Streams

- stdin ('0')  
  - Default: keyboard input  
  - Can be redirected from a file using '<'

- stdout ('1')  
  - Default: terminal (screen) output  
  - Can be redirected to a file using '>'

- stderr ('2')  
  - Default: terminal error output  
  - Can be redirected separately (not covered here)



2. Redirecting Output ('>')

- Overwrite a file (or create if missing) with command output:
  '''bash
  <command> > <filename>
  '''

2.1 Copy a File

'''bash
cat file1.txt > file2.txt
'''
- 'cat' reads 'file1.txt' (stdin), writes its contents to 'file2.txt' (stdout).

2.2 Concatenate Multiple Files

'''bash
cat file1.txt file2.txt > combined.txt
'''
- Combines data from both source files into 'combined.txt'.

2.3 Write a Single Line

'''bash
echo hello > hello.txt
'''
- 'echo' outputs 'hello' to 'hello.txt' instead of the screen.

2.4 Redirecting Command Listings

Exercise solution: Save a long listing of '/etc' to 'files.txt'  
'''bash
ls -l /etc > files.txt
'''
- 'ls -l /etc' output is written into 'files.txt'.



3. Redirecting Input ('<')

- Feed a file into a command’s stdin:
  '''bash
  <command> < <filename>
  '''
- Example (rare use):  
  '''bash
  sort < unsorted-list.txt
  '''
  Sorts lines from 'unsorted-list.txt' instead of typing them manually.



4. Key Points

- Using '>' overwrites existing files.  
- Use '>>' (double-greater-than) to append instead of overwrite.  
- Redirection works with nearly any command, not just 'cat' or 'echo'.  



5. Practice

1. Create two small text files and combine them into a third with 'cat … > combined.txt'.  
2. Use 'echo' and output redirection to generate a log line file.  
3. Redirect the output of 'ps aux' to 'processes.txt'.  

With redirection, you can script and automate data flows between commands and files effortlessly.






Codes and Other Notes in this Discussion: 

cat file1.txt > file2.txt 
cat file1.txt file2.txt > combined.txt

echo hello > hello.txt 

ls -l /etc > files.txt 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Searching for Text:

Searching for Text with grep:
The 'grep' command (short for global regular expression print) lets you search for patterns in one or more files.



1. Basic Usage

'''bash
grep <pattern> <file>
'''

- By default, case‐sensitive  
- Example:  
  '''bash
  grep hello file1.txt
  '''
  If 'file1.txt' contains 'Hello' only with a capital “H,” this returns nothing.



2. Making Searches Case‐Insensitive

- Option: '-i'  
- Usage:
  '''bash
  grep -i hello file1.txt
  '''
- Matches 'hello', 'Hello', 'HELLO', etc.



3. Searching System Files

- Example: find occurrences of “root” in the user database:
  '''bash
  grep -i root /etc/passwd
  '''
- '/etc/passwd' lists local user accounts; you’ll see every 'root' occurrence highlighted.



4. Searching Multiple Files

1. Specify each file:
   '''bash
   grep -i hello file1.txt file2.txt
   '''
2. Use a filename pattern (wildcard):
   '''bash
   grep -i hello file*
   '''
   Searches 'file1.txt', 'file2.txt', 'file3.txt', etc.



5. Searching Directories

- Non‐recursive:
  '''bash
  grep -i hello /etc
  '''
  > Error: '/etc' is a directory.
- Recursive (search all subdirectories):
  '''bash
  grep -i -r hello /etc
  '''
  or combined:
  '''bash
  grep -ir hello /etc
  '''
- You can also search your current directory:
  '''bash
  grep -ir hello .
  '''



6. How It Works

- stdin & stdout:  
  'grep' reads each file, filters lines matching the pattern, and writes them to standard output.
- Highlighting:  
  Many shells auto‐highlight matches (in red) when '--color=auto' is enabled.



7. Key Commands Summary

| Task                                | Command                             |
|-------------------------------------|-------------------------------------|
| Case‐sensitive search               | 'grep hello file1.txt'              |
| Case‐insensitive search             | 'grep -i hello file1.txt'           |
| Search multiple files               | 'grep -i hello file1.txt file2.txt' |
| Search with wildcard                | 'grep -i hello file*'               |
| Recursive directory search          | 'grep -ir hello .'                  |
| Search in system file               | 'grep -i root /etc/passwd'          |



Practice:  
1. Create two text files with repeated words.  
2. Use 'grep' to find a word, then add '-i' to ignore case.  
3. Combine '-r' and '-i' to search across a directory of files.




Codes and Other Notes in this Discussion: 

grep hello file1.txt 
grep -i hello file1.txt 

grep -i root /etc/passwd 

grep -i hello file1.txt file2.txt 
grep -i hello file*

grep -i hello /etc
grep -i -r hello .

grep -ir hello .

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Finding Files and Directories: 

Finding Files and Directories with 'find'
The 'find' command searches for files and directories in a directory hierarchy. Below are every detail and example you need to master it.



1. Basic Usage

- Default (no arguments):  
  '''bash
  find
  '''
  Lists all files and directories, recursively, from the current directory ('.') downward.



2. Viewing Hidden Items

- By default, 'ls' hides names beginning with a dot ('.').  
- Show all entries (including hidden):  
  '''bash
  ls -a
  '''
  Reveals '.bashrc', '.profile', etc., in your home directory.



3. Searching a Specific Path

- Search '/etc':  
  '''bash
  find /etc
  '''
  Recursively lists everything under '/etc'.




4. Filter by Type

- Only directories:  
  '''bash
  find . -type d
  '''
- Only files:  
  '''bash
  find . -type f
  '''

> '-type d' = directories, '-type f' = regular files.



5. Filter by Name

- Name pattern (case‐sensitive):  
  '''bash
  find . -type f -name "f*"
  '''
  Finds files starting with 'f' in their filename.

- No matches for uppercase 'F':  
  '''bash
  find . -type f -name "F*"
  '''
  Returns nothing if no file starts with capital 'F'.



6. Case‐Insensitive Name Search

- Use '-iname' instead of '-name':  
  '''bash
  find . -type f -iname "F*"
  '''
  Matches 'file1.txt', 'File2.txt', etc.



7. Find and Save Results

Exercise: Find all Python files ('*.py') on the entire filesystem and write results to 'python-files.txt'.

'''bash
find / -type f -name "*.py" > python-files.txt
'''

- Step-by-step:  
  1. '/' → search from root directory.  
  2. '-type f' → only regular files.  
  3. '-name "*.py"' → match filenames ending in '.py'.  
  4. '> python-files.txt' → redirect stdout to a file.



8. Summary of Common 'find' Options

| Option              | Description                               | Example                                    |
|---------------------|-------------------------------------------|--------------------------------------------|
| None                | List everything from current directory    | 'find'                                     |
| '<path>'            | Search from a specific path               | 'find /etc'                                |
| '-type d'           | Show only directories                     | 'find . -type d'                           |
| '-type f'           | Show only files                           | 'find . -type f'                           |
| '-name "<pattern>"' | Case‐sensitive filename matching          | 'find . -type f -name "f*"'                |
| '-iname "<pat>"'    | Case‐insensitive filename matching        | 'find . -type f -iname "F*"'               |
| '> <file>'          | Redirect output to a file                 | 'find / -type f -name "*.py" > python-files.txt' |



Practice:  
1. Run 'find' in your home directory and inspect hidden files via 'ls -a'.  
2. Use 'find -type d' and 'find -type f' to separate directories and files.  
3. Experiment with '-name' vs. '-iname' patterns.  
4. Save the output of a 'find' command to a text file with '>' redirection.






Codes and Other Notes in this Discussion: 

find 

ls -a  -> to show the hidden files 

find /etc

find -type d 

find -type f 

find -type f -name "f*"

find -type f -name "F*"

find -type f -iname "F*"


find / -type f -name "*.py" > python-files.txt 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Chaining Commands: 

Chaining Commands in Linux
Linux lets you combine multiple commands in a single line using operators and separators. This boosts productivity and enables complex one-liners. Below are every detail and examples you need.



1. Semicolon (';'): Sequential Execution  
- Runs commands one after another, regardless of success or failure of preceding commands.  
- Syntax:
  '''bash
  cmd1; cmd2; cmd3
  '''
- Example:
  '''bash
  mkdir test; cd test; echo done
  '''
  1. Creates 'test' directory  
  2. Enters it  
  3. Prints 'done'  

- Even if one fails, the rest still run:
  '''bash
  mkdir test; cd test; echo done
  '''
  If 'test' already exists, you’ll see an error for 'mkdir', but 'cd test' and 'echo done' still execute.



2. AND Operator ('&&'): Conditional Execution on Success  
- Runs the next command only if the previous command succeeds (exit status '0').  
- Syntax:
  '''bash
  cmd1 && cmd2 && cmd3
  '''
- Example:
  '''bash
  mkdir test && cd test && echo done
  '''
  - If 'mkdir test' fails (e.g., already exists), none of the subsequent commands run.



3. OR Operator ('||'): Conditional Execution on Failure  
- Runs the next command only if the previous command fails (non-zero exit status).  
- Syntax:
  '''bash
  cmd1 || cmd2
  '''
- Example:
  '''bash
  mkdir test || echo "directory exists"
  '''
  - If 'mkdir test' succeeds, 'echo' is skipped.  
  - If it fails, prints 'directory exists'.



4. Piping ('|'): Redirecting Output Between Commands  
- Connects the stdout of one command to the stdin of another.  
- Syntax:
  '''bash
  cmd1 | cmd2
  '''
- Examples:
  '''bash
  ls /bin | less
  '''
  - Lists '/bin' contents, then opens in the 'less' pager.  
  '''bash
  ls /bin | head -n 5
  '''
  - Shows only the first 5 entries.  

          ls /bin output
               │
               ▼
           [ pipe operator ]
               │
               ▼
            less (interactive pager)
  


5. Splitting Long Commands Across Lines ('\')  
- Use a backslash ('\') at the end of a line to continue on the next line.  
- Improves readability for very long one-liners.  
- Example:
  '''bash
  mkdir hello; \
  cd hello;    \
  echo done
  '''
  This runs exactly like:
  '''bash
  mkdir hello; cd hello; echo done
  '''



6. Tips & Best Practices  
- Spaces around operators make sequences clearer but are optional.  
- Combine operators ('&&', '||') to build robust checks.  
- Use pipes to chain tools (e.g., filters, pagers, text processors).  
- Line continuations ('\') help avoid horizontal scrolling in your terminal.



Practice Exercises  
1. Create a directory, enter it, and list its contents in one line using '&&'.  
2. Attempt to create the same directory twice with '||' to handle the “already exists” message.  
3. Pipe a long 'ls -l /usr/bin' into 'grep bash' to find bash‐related executables.  
4. Write a multi-line compound command using '\' to set up a new project folder structure.





Codes and Other Notes in this Discussion: 

mkdir test;cd test;echo done

mkdir test && cd test && echo done  

mkdir test || echo "directory exists"


ls /bin | less 
ls /bin | head -n 5 

mkdir hello;\
cd hello;\
echo done; 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Environment Variables: 

Environment Variables in Linux
Environment variables store configuration settings and are accessible to applications at runtime. They behave like variables in programming languages but live in the shell environment.



1. Viewing All Environment Variables

Use either command to list every variable in the current session:

- 'printenv'  
- 'env'  

'''bash
printenv
'''

This displays key=value pairs, for example:
'''
HOSTNAME=2f7a1b3c
PWD=/root
HOME=/root
LS_COLORS=...
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:...
'''



2. Inspecting a Specific Variable

1. With 'printenv':

   '''bash
   printenv PATH
   '''
   Outputs the full 'PATH' string.

2. With 'echo':  
   Prefix the variable name with '$':
   '''bash
   echo $PATH
   '''



3. Understanding 'PATH'

- A colon-separated list of directories that the shell searches for executables.  
- Example:
  '''
  /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
  '''
- If a command isn’t found, it’s usually because its directory isn’t in 'PATH'.



4. Setting Environment Variables (Temporary)

Use 'export' to define a variable for the current session:

'''bash
export DB_USER=mosh
echo $DB_USER         # → mosh
printenv DB_USER      # → mosh
'''

- Scope: Only lasts until you close the terminal or exit the container.



5. Docker Containers and Session Scope

1. List all containers (including stopped):
   '''bash
   docker ps -a
   '''
2. Start and attach to a stopped container:
   '''bash
   docker start -i <container-id>
   '''
3. Check 'DB_USER':
   '''bash
   echo $DB_USER       # → (empty, because the session restarted)
   '''



6. Making Variables Persistent

Variables become permanent by adding them to the user’s startup file: '~/.bashrc'

1. Edit '.bashrc' manually:
   '''bash
   nano ~/.bashrc
   '''
   Add at the end:
   '''
   export DB_USER=mosh
   '''

2. Or append via redirection (avoids opening the editor):
   '''bash
   echo 'export DB_USER=mosh' >> ~/.bashrc
   '''

3. Verify:
   '''bash
   tail -n 1 ~/.bashrc   # Shows the newly added line
   '''



7. Applying Changes Immediately

- Open a new terminal/session, or  
- Reload the file in the current session:
  '''bash
  source ~/.bashrc
  echo $DB_USER       # → mosh
  '''



8. Example: Adding Another Variable

'''bash
echo 'export COLOR=blue' >> ~/.bashrc

# Immediately available?
echo $COLOR          # → (empty)

# After sourcing:
source ~/.bashrc
echo $COLOR          # → blue
'''



9. Security Consideration

Avoid storing sensitive data (passwords, secrets) in '~/.bashrc'. This file is plain text and readable by anyone with access to your account. Use secure vaults or encrypted storage for secrets.



With these steps, you can view, set, and persist environment variables reliably in any Linux-based system or Docker container.




Codes and Other Notes in this Discussion: 

printenv 

printenv PATH 

echo $PATH 

export DB_USER=mosh 
echo $DB_USER
prinenv DB_USER



docker ps -a 
docker start -i container-id



nano .bashrc 
or 
echo DB_USER=mosh >> .bashrc 


echo COLOR=blue >> .bashrc 
echo $COLOR 


source .bashrc 
source ~/ .bashrc 



--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Managing Processes:

Managing Processes in Linux
A process is a running instance of a program. You can view, background, and terminate processes using built-in commands.



1. Viewing Processes with 'ps'

'''bash
ps
'''

By default, 'ps' lists your processes on the current terminal:

'''
  PID TTY          TIME CMD
   35 pts/0    00:00:00 bash
   38 pts/0    00:00:00 ps
'''

- PID: Process ID (unique identifier).  
- TTY: Terminal (teletype) device, e.g. 'pts/0' is the first pseudo-terminal.  
- TIME: CPU time consumed so far.  
- CMD: Command name.  

> Note: 'ps' itself runs briefly to produce output, so you’ll always see a 'ps' entry when you call it.




2. Running a Command in the Background

A background process lets you run long-running commands without blocking the shell.

'''bash
sleep 100 &
'''

- 'sleep 100': Waits for 100 seconds.  
- '&': Runs it in the background, returning control to the prompt immediately.

Re-run 'ps' to see 'sleep':

'''
  PID TTY          TIME CMD
   35 pts/0    00:00:00 bash
   38 pts/0    00:00:00 sleep
   39 pts/0    00:00:00 ps
'''



3. Terminating a Process with 'kill'

When you identify an unwanted process by its PID, terminate it:

'''bash
kill 38
'''

- Sends the default SIGTERM signal to PID 38.  
- After killing, 'ps' no longer lists that process:

'''
  PID TTY          TIME CMD
   35 pts/0    00:00:00 bash
   40 pts/0    00:00:00 ps
'''



4. Quick Reference

| Action                   | Command                     |
|--------------------------|-----------------------------|
| List processes           | 'ps'                        |
| Run job in background    | 'sleep 100 &'               |
| List processes again     | 'ps'                        |
| Kill a process by PID    | 'kill <PID>'                |



Key Concepts

- Foreground vs. Background:  
  - Foreground jobs block the shell until completion.  
  - Background jobs ('&') free up the shell immediately.

- TTY Column:  
  - 'pts/0', 'pts/1', etc., indicate which terminal window a process is attached to.

- TIME Column:  
  - Shows CPU time consumed—high values can indicate CPU-heavy processes.



Practice Exercise:  
1. Run 'sleep 60 &' and note its PID from 'ps'.  
2. Use 'kill <that-PID>' to terminate it.  
3. Observe the change in 'ps' output.



Codes and Other Notes in this Discussion: 

ps 

sleep 100 &

kill 38 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Managing Users: 

Managing Users in Linux
This guide covers creating, modifying, deleting, and logging in as users—down to every flag, file, and behavior you need to know.



1. Key Commands

- 'useradd' – create a new user  
- 'usermod' – modify an existing user  
- 'userdel' – delete a user account  
- 'adduser' – interactive script wrapping 'useradd'  



2. Creating a User with 'useradd'

1. Create home directory  
   '''bash
   sudo useradd -m john
   '''
   - '-m' (or '--create-home') automatically makes '/home/john'.

2. Inspect '/etc/passwd'  
   '''bash
   cat /etc/passwd
   '''
   Sample '/etc/passwd' entry for John (fields separated by ':'):  
   '''
   john:x:1001:1001::/home/john:/bin/sh
   │    │    │   │      │           └─ default shell
   │    │    │   │      └─ home directory
   │    │    │   └─ GID (primary group ID)
   │    │    └─ UID (user ID)
   │    └─ password placeholder (“x” → in /etc/shadow)
   └─ username
   '''
   - Passwords are actually stored (encrypted) in '/etc/shadow', readable only by root.



3. Modifying a User with 'usermod'

- Change login shell  
  '''bash
  sudo usermod -s /bin/bash john
  '''
  - '-s' (or '--shell') sets the user’s login shell.  
  - Verify in '/etc/passwd': John’s entry now ends with '/bin/bash'.



4. Logging In as a Specific User in Docker

1. List containers  
   '''bash
   docker ps -a
   '''
2. Start an interactive shell as root  
   '''bash
   docker exec -it <container-id> bash
   '''
   - Prompt shows 'root@…#'.

3. Start an interactive shell as John  
   '''bash
   docker exec -it -u john <container-id> bash
   '''
   - Prompt shows 'john@…$'.  
   - Verify reduced privileges:
     '''bash
     cat /etc/shadow
     # → Permission denied
     '''
   - John’s home:  
     '''bash
     cd ~
     pwd
     # → /home/john
     '''



5. Deleting a User

- Remove John (including home directory if desired):
  '''bash
  sudo userdel john
  # add -r to delete /home/john and mail spool
  '''



6. 'adduser' vs. 'useradd'

- 'useradd' is a low-level, non-interactive utility.  
- 'adduser' is a Perl script that wraps 'useradd' and:
  - Prompts for password  
  - Creates and populates the home directory  
  - Creates a group matching the username  
  - Optionally collects user info (full name, phone, etc.)

# Example Interactive Flow
'''bash
sudo adduser bob
# – Enter a new UNIX password:
# – Re-enter new UNIX password:
# – Full Name []: Bob Smith
# – Room Number []: 123
# – Work Phone []:
# – Home Phone []:
# – Other []:
# Is the information correct? [Y/n]
'''

> In Docker: prefer 'useradd' (batch) over 'adduser' to avoid interactive prompts in automated scripts.



Recap of Commands

| Task                          | Command                                     |
|-------------------------------|---------------------------------------------|
| Create user with home dir     | 'sudo useradd -m john'                      |
| Modify user shell             | 'sudo usermod -s /bin/bash john'            |
| Delete user                   | 'sudo userdel john'                         |
| Interactive add user          | 'sudo adduser bob'                          |
| View account records          | 'cat /etc/passwd'                           |
| View encrypted passwords      | 'sudo cat /etc/shadow'                      |
| Login to container as user    | 'docker exec -it -u john <container> bash'  |



With these commands and file insights, you can fully manage Linux user accounts in any environment—including Docker containers.






Codes and Other Notes in this Discussion: 

useradd 
usermod
userdel 

useradd -m john 

cat /etc/passwd 


usermod -s /bin/bash john 

cat /etc/shadow 


docker exec -it container-id bash


docker exec -it -u john container-id bash 


adduser bob 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Managing Groups:

Managing Groups in Linux
Groups let you assign a set of users common permissions. Every user has one primary group and zero or more supplementary groups. Here’s how to create, modify, and inspect groups and group memberships.



1. Key Commands  
- 'groupadd'  – create a new group  
- 'groupmod'  – modify an existing group (less common)  
- 'groupdel'  – delete a group  
- 'usermod'   – add/remove a user to/from groups  
- 'groups'    – display the groups a user belongs to  



2. Create a New Group

'''bash
sudo groupadd developers
'''

- 'developers' is now listed in '/etc/group'.  
- Verify:
  '''bash
  cat /etc/group | grep developers
  '''
  Sample entry:
  '''
  developers:x:1003:
  '''
  - 1003 is the GID (group ID).  
  - No users listed after the final colon yet.



3. Primary vs. Supplementary Groups

- Primary group  
  - One per user  
  - Used as the default “owner group” when the user creates new files  
  - Automatically created with the same name as the user (e.g., 'john:x:1001:1001:…')

- Supplementary groups  
  - Zero or more per user  
  - Grant additional access rights (e.g., share files with a team)  



4. Add a User to a Group

Use 'usermod' with '-G' (capital G) for supplementary groups:

'''bash
sudo usermod -G developers john
'''

- This adds john to the 'developers' group without changing his primary group.



5. Inspect User’s Group Membership

1. View '/etc/passwd' record for John:
   '''bash
   cat /etc/passwd | grep john
   '''
   Sample line:
   '''
   john:x:1001:1001::/home/john:/bin/bash
   '''
   - 4th field ('1001') is John’s primary GID.

2. List groups John belongs to:
   '''bash
   groups john
   '''
   Output:
   '''
   john : john developers
   '''
   - First is John’s primary group ('john').  
   - Next are supplementary groups ('developers').



6. Removing a Group

'''bash
sudo groupdel developers
'''

- Fails if any user is still a member. Remove users first (via 'usermod -G "" john' to clear).



7. Exercise

1. Create a new group called 'artists':
   '''bash
   sudo groupadd artists
   '''
2. Add John to 'artists' as supplementary group:
   '''bash
   sudo usermod -G artists john
   '''
3. Verify:
   '''bash
   groups john
   # → john : john artists
   '''



Summary  
- Use 'groupadd', 'groupmod', 'groupdel' to manage groups.  
- Use 'usermod -G' to assign supplementary groups; '-g' sets the primary group.  
- Check membership with 'groups <user>' and inspect '/etc/group' and '/etc/passwd'.





Codes and Other Notes in this Discussion: 

groupaddd developers 
cat /etc/group 

usermod -G developers john 

cat /etc/passwd  | grep john 


groups john  

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

File Permissions:
File permissions control who can read, write, or execute a file. They are encoded as a 10-character string (shown by 'ls -l') and modified with the 'chmod' command.



1. Create a Shell Script

1. Write a line to 'deploy.sh':  
   '''bash
   echo 'echo hello' > deploy.sh
   '''
2. Verify content:  
   '''bash
   cat deploy.sh
   # → echo hello
   '''
3. List details:  
   '''bash
   ls -l
   '''
   Example output:
   '''
   -rw-r--r-- 1 root root   12 Apr 15 12:00 deploy.sh
   '''



2. Understanding 'ls -l' Output

'''
-rw-r--r-- 1 root root 12 Apr 15 12:00 deploy.sh
│││││││││└─ filename
││││││││└──── modification time
│││││││└──── size (bytes)
││││││└────── group owner
│││││└──────── user owner
││││└───────── hard link count
│││└────────── permissions for “others”
││└─────────── permissions for “group”
│└──────────── permissions for “owner”
└───────────── file type ('-'=file, 'd'=directory)
'''


2.1 Permission String Breakdown

| Position    | Symbol      | Meaning                                                  |
|-------------|-------------|----------------------------------------------------------|
| 1           | '-' or 'd'  | '-' = regular file; 'd' = directory                     |
| 2–4 (owner) | 'r'/'w'/'x' or '-' | Owner can read/write/execute                         |
| 5–7 (group) | same        | Group members’ permissions                              |
| 8–10 (other)| same        | Everyone else’s permissions                              |

Example: '-rw-r--r--'  
- Owner: 'rw-' (read, write)  
- Group: 'r--' (read)  
- Others: 'r--' (read)




3. Adding Execute Permission

By default, 'deploy.sh' lacks execute permission ('x'):

'''bash
ls -l deploy.sh
# -rw-r--r-- 1 root root ... deploy.sh
'''

1. Add owner ('u') execute ('+x'):  
   '''bash
   chmod u+x deploy.sh
   '''
2. Verify:  
   '''bash
   ls -l deploy.sh
   # -rwxr--r-- 1 root root ... deploy.sh
   '''
3. Run the script:  
   '''bash
   ./deploy.sh
   # → hello
   '''




4. Granting Execute to Others

To let all users run it:

'''bash
chmod o+x deploy.sh
'''

- Or combine owner and group:  
  '''bash
  chmod og+x deploy.sh
  '''
- Verify as non-root user:  
  '''bash
  # Switch to regular user (e.g., john)
  ./deploy.sh
  # → hello
  '''




5. Advanced Symbolic Syntax

- Combine actions:  
  '''bash
  chmod og+ x,w  -r deploy.sh
  '''
  Breakdown:
  - 'o' & 'g' → others & group  
  - '+x,w' → add execute & write  
  - '-r' → remove read  

- Apply to all '.sh' files:  
  '''bash
  chmod og+wx-r *.sh
  '''



6. Quick Reference

| Change Permission                              | Command                               |
|------------------------------------------------|---------------------------------------|
| Add execute for owner                          | 'chmod u+x file'                      |
| Add execute for group and others               | 'chmod og+x file'                     |
| Remove write for group                         | 'chmod g-w file'                      |
| Add read/write/execute for all ('a' = all)     | 'chmod a+rwx file'                    |
| Remove all permissions for others              | 'chmod o= file'                       |



*With these basics you can read, interpret, and modify any file’s permissions securely and flexibly.*




Codes and Other Notes in this Discussion: 

echo echo hello > deploy.sh 

cat deploy.sh 

ls -l 

chmod u+x deploy.sh 


chmod o+x deploy.sh 

chmod og+x+w-r deploy.sh 
chmod og+x+w-r *.sh 
   
--------------------------------------------------------------------------------------------------------------------------------------------------------------------   
  
   
Summary: 

The Linux Command Line

Managing Packages
apt update
apt list
apt install nano
apt remove nano


Navigating the file system

pwd # to print the working directory
ls # to list the files and directories
ls -l # to print a long list
cd / # to go to the root directory
cd bin # to go to the bin directory
cd .. # to go one level up
cd ~ # to go to the home directory   



Manipulating files and directories

mkdir test # to create the test directory
mv test docker # to rename a directory
touch file.txt # to create file.txt
mv file.txt hello.txt # to rename a file
rm hello.txt # to remove a file
rm -r docker # to recursively remove a directory




Editing and viewing files

nano file.txt # to edit file.txt
cat file.txt # to view file.txt
less file.txt # to view with scrolling capabilities
head file.txt # to view the first 10 lines
head -n 5 file.txt # to view the first 5 lines
tail file.txt # to view the last 10 lines
tail -n 5 file.txt # to view the last 5 lines 




Searching for text

grep hello file.txt # to search for hello in file.txt
grep -i hello file.txt # case-insensitive search
grep -i hello file*.txt # to search in files with a pattern
grep -i -r hello . # to search in the current directory




Finding files and directories

find # to list all files and directories
find -type d # to list directories only
find -type f # to list files only
find -name “f*” # to filter by name using a pattern




Managing environment variables

printenv # to list all variables and their value
printenv PATH # to view the value of PATH
echo $PATH # to view the value of PATH
export name=bob # to set a variable in the current session




Managing processes

ps # to list the running processes
kill 37 # to kill the process with ID 37





Managing users and groups

useradd -m john # to create a user with a home directory
adduser john # to add a user interactively
usermod # to modify a user
userdel # to delete a user

groupadd devs # to create a group
groups john # to view the groups for john
groupmod # to modify a group
groupdel # to delete a group





File permissions

chmod u+x deploy.sh # give the owning user execute permission
chmod g+x deploy.sh # give the owning group execute permission
chmod o+x deploy.sh # give everyone else execute permission
chmod ug+x deploy.sh # to give the owning user and group
					 # execute permission
chmod ug-x deploy.sh # to remove the execute permission from
					 # the owning user and group 
					 
--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Building Images: 

Introduction: 
The first step in using Docker to build and deploy applications is creating images. So having a solid understanding of Docker images is crucial for you, and that's what this section is all about. We'll be talking about creating Docker files, versioning images, sharing them, saving and loading them, and a few optimization techniques for reducing the image size and speeding up builds. I'm so excited about this section, I hope you are too, so let's jump in and get started.

What You’ll Learn in This Section  
- Creating Dockerfiles  
  – Writing step-by-step build recipes  
- Versioning Images  
  – Tagging strategies (semver, 'latest', CI-friendly)  
- Sharing Images  
  – Pushing to Docker Hub or private registries  
- Saving & Loading Images  
  – Exporting images as tarballs for offline transfer ('docker save'/'docker load')  
- Reducing Image Size  
  – Multi-stage builds, minimal base images, layer consolidation  
- Speeding Up Builds  
  – Leveraging cache, ordering instructions for efficient rebuilds  



High-Level Workflow:  

[Dockerfile]      --docker build-->      [Image:app:1.0]
      │                                     │
      │ docker push                         │ docker pull & run
      ▼                                     ▼
[Registry (Docker Hub)]             [Production / Test Host]
      │                                     │
      └─── docker save/load ────────────────┘


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Images and Containers: 

Understand the fundamental difference between Docker images and containers before diving into actual workflows.


1. What Is a Docker Image?

A Docker image is a read-only template that bundles everything your application needs:

- A cut-down operating system (e.g., Linux, Windows)  
- Third-party libraries and dependencies  
- Your application’s files and assets  
- Configuration settings and environment variables  

Once built, an image can be stored, versioned, and shared—just like a binary package.



2. What Is a Docker Container?

A Docker container is a running instance of an image, providing an isolated environment:

- Technically, it’s just a special OS process  
- Has its own filesystem (image layers + its own writable layer)  
- Can be stopped, started, and restarted independently  
- Changes inside one container do not affect other containers  



3. Key Differences

| Aspect             | Image                            | Container                                |
|--------------------|----------------------------------|------------------------------------------|
| State              | Immutable, read-only             | Mutable, writes go to a thin top layer   |
| Execution          | Not running                      | One or more processes                    |
| Isolation          | N/A                              | Namespaces & cgroups for CPU, network…   |
| Lifespan           | Persistent in registry/storage   | Ephemeral or long-lived runtime          |
| Sharing            | Pushed/pulled via registries     | Created/destroyed locally or remotely    |



4. Live Demo: Two Containers from the Same Image

1. Check running containers  
   '''bash
   docker ps
   '''
2. Start a new interactive Ubuntu container  
   '''bash
   docker run -it ubuntu
   '''
   - A new container ID appears (different from any existing one).
3. Compare file systems  
   - In Container A (first Ubuntu shell), create files in '~/':  
     '''bash
     mkdir demo && echo "A" > demo/a.txt
     '''
   - In Container B (second Ubuntu shell), inspect '~/demo':  
     '''bash
     ls ~/demo
     # → directory doesn’t exist—filesystem is isolated
     '''



5. Isolation Diagram (ASCII)

           Image: ubuntu:latest
          ┌─────────────────────┐
          │  Layers (RO)        │
          │ ─ base OS           │
          │ ─ utilities         │
          │ ─ (no app files)    │
          └─────────────────────┘
                   ▲
      ┌────────────┴─────────────┐
      │                          │
┌─────┴─────┐              ┌─────┴─────┐
│Container A│              │Container B│
│(Process)  │              │(Process)  │
│RO Layers  │              │RO Layers  │
│+ RW Layer │              │+ RW Layer │
│ Demo files│              │ (empty)   │
└───────────┘              └───────────┘


- RO Layers are shared across all containers from the same image.  
- RW Layer is unique—writes in one container stay invisible to others.



Summary

- Image = blueprint (everything your app needs, immutable).  
- Container = running instance (isolated with its own writable layer).  
- Multiple containers from one image share the same base layers but keep changes separate.





Code and Other Notes in this Discussion: 

Image:
A cut-down OS
Third-party libraries 
Application Files 
Environment Variables  


Container: 
Provides an isolated environment 
Can be stopped & restarted 
Is just a process!


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Sample Web Application: 
This example React front-end app will be dockerized in upcoming lessons. You do not need React or JavaScript knowledge—any language background (C#, Java, Python, Ruby) works; we use this project solely to demonstrate Docker concepts.



1. Download & Extraction

1. Download the ZIP file attached below the video.  
2. Extract the contents to a folder on your local machine (Windows, Linux, or macOS).



2. Project Structure

At the project root you’ll find:

- 'package.json'  
- (After installation) 'node_modules/'  
- React source files (e.g., 'src/', 'public/', etc.)  

Optional ASCII overview:

my-react-app/
├── package.json
├── node_modules/
├── public/
├── src/
└── README.md




3. Key File: 'package.json'

- Name & Version  
  Identifies the application:  
  '''json
  {
    "name": "my-react-app",
    "version": "0.1.0",
    "dependencies": { … }
  }
  '''
- Dependencies  
  Lists third-party libraries required at runtime.



4. Manual Startup Steps (Without Docker)

On a brand-new machine (only OS installed):

1. Install Node.js  
2. In the project folder, run:
   '''bash
   npm install
   '''
   - Reads 'package.json' and downloads all dependencies into 'node_modules/'.  
3. Start the development server:
   '''bash
   npm start
   '''
4. Open your browser at 'http://localhost:3000' to view the React app.

> After 'npm install', 'node_modules/' contains hundreds or thousands of sub-folders for each dependency.



5. Recap of Manual Workflow

1. Provision OS  
2. Install Node.js  
3. Install dependencies via 'npm install'  
4. Launch app with 'npm start'  
5. Access at 'localhost:3000'

This process repeats for every new environment—Docker will simplify and standardize these steps.



6. Next Steps

In the following lessons, you will:

- Write a Dockerfile to automate setup  
- Build a Docker image packaging Node, dependencies, and app files  
- Run containers that replicate the manual workflow anywhere  



Note: This React app is a stand-in example. Docker concepts apply equally to any back-end or multi-container stack._



Codes and Other Notes Discussion: 

react.js application contains:

package.json 


Steps:

Install Node 
npm install 
npm start 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Dockerfile Instructions: 
A Dockerfile is a plain-text recipe of instructions that tell Docker how to assemble an image. Below is the complete list of core instructions you’ll use to build and configure your images.



1. FROM

- Purpose: Specifies the base image to build upon (e.g., a cut-down Linux distro or language runtime).  
- Syntax:  
  '''
  FROM ubuntu:20.04
  '''
- Details:  
  - Must be the first non-comment line in your Dockerfile.  
  - You can use multiple 'FROM' lines for multi-stage builds (covered later).



2. WORKDIR

- Purpose: Sets the working directory for subsequent instructions.  
- Syntax:  
  '''
  WORKDIR /app
  '''
- Details:  
  - Creates the directory if it doesn’t exist.  
  - All 'COPY', 'ADD', 'RUN', etc., will execute relative to this path.



3. COPY & ADD

| Instruction | Purpose                                                                                  |
|-------------|------------------------------------------------------------------------------------------|
| 'COPY'      | Copies files/directories from build context into the image.                              |
| 'ADD'       | Same as 'COPY', plus:                                                                      |
|             | • Can pull from a URL                                                                      |
|             | • Auto-extracts local compressed archives ('.tar', '.tar.gz', etc.)                      |

- Syntax:  
  '''
  COPY src/ dest/
  ADD file.tar.gz /data/
  '''



4. RUN

- Purpose: Executes arbitrary commands in a new layer on top of the image.  
- Syntax:  
  '''dockerfile
  RUN apt update && apt install -y curl
  '''
  or (exec form):  
  '''dockerfile
  RUN ["npm", "install"]
  '''
- Details:  
  - Commands run during build time, affecting the image state.  
  - Use shell form for chaining ('&&'), exec form to avoid shell parsing.



5. ENV

- Purpose: Defines environment variables inside the image.  
- Syntax:  
  '''
  ENV NODE_ENV=production
  '''
- Details:  
  - Variables persist in the image and are available at runtime.  
  - Can set multiple variables at once:  
    '''
    ENV KEY1=value1 \
        KEY2=value2
    '''



6. EXPOSE

- Purpose: Documents the port(s) on which the container listens at runtime.  
- Syntax:  
  '''
  EXPOSE 3000
  '''
- Details:  
  - Does not publish the port on the host; use '-p' or '-P' with 'docker run'.  
  - Can specify protocol: 'EXPOSE 8080/tcp 53/udp'.



7. USER

- Purpose: Switches the user (and optionally group) for subsequent instructions and to run the container.  
- Syntax:  
  '''
  USER node
  '''
  or  
  '''
  USER appuser:appgroup
  '''
- Details:  
  - Improves security by avoiding root in containers.  
  - Affects file permissions and process privileges.



8. ENTRYPOINT & CMD

| Instruction  | Purpose                                                                                   |
|--------------|-------------------------------------------------------------------------------------------|
| 'ENTRYPOINT' | Sets the primary executable for the container, making the image behave like a binary. |
| 'CMD'        | Provides default arguments to 'ENTRYPOINT' (or specifies a command if no ENTRYPOINT).|

- Syntax examples:  
  '''dockerfile
  ENTRYPOINT ["nginx"]
  CMD ["-g", "daemon off;"]
  '''
  or single instruction:  
  '''dockerfile
  CMD ["node", "server.js"]
  '''
- Details:  
  - If both 'ENTRYPOINT' and 'CMD' are used, Docker appends 'CMD' arguments to 'ENTRYPOINT'.  
  - Overridden by arguments passed to 'docker run'.



9. Quick Reference Table

| Instruction  | Brief Description                                      |
|--------------|--------------------------------------------------------|
| FROM         | Base image                                            |
| WORKDIR      | Set working directory                                  |
| COPY, ADD    | Copy local files or extract archives                   |
| RUN          | Execute commands at build time                         |
| ENV          | Define environment variables                           |
| EXPOSE       | Document container ports                               |
| USER         | Switch to non-root user                                |
| ENTRYPOINT   | Define container’s executable                          |
| CMD          | Default command or arguments passed to ENTRYPOINT      |



*Next up: writing your first Dockerfile step by step.*




Codes and Other Notes in this Discussion: 

DockerFile: 
FROM
WORKDIR 
COPY 
ADD 
RUN 
ENV 
EXPOSE 
USER
CMD 
ENTRYPOINT 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Choosing the Right Base Image:
When you add a 'Dockerfile' to your project, the first instruction is always 'FROM', which specifies the base image. Picking the right base image affects image size, security, and build speed.



1. What Can a Base Image Be?

- Operating system (OS) only  
  e.g. 'ubuntu:20.04', 'alpine:3.13'  
- OS + runtime environment  
  e.g.  
  - '.NET' on Linux: 'mcr.microsoft.com/dotnet/aspnet:6.0'  
  - Python on Linux: 'python:3.9-slim'  
  - Node.js on Linux: 'node:14-alpine'

> Images may live in different registries. Docker’s default is Docker Hub.  
> For other registries (e.g., Microsoft Container Registry), you must specify the full URL:  
> 'FROM mcr.microsoft.com/dotnet/aspnet:6.0'



2. Exploring the Official Node Repository

1. Visit https://hub.docker.com/_/node  
2. Click Tags to view hundreds of Node.js images.

   Each tag indicates:  
   - Node version (major, minor, patch)  
   - Linux distro (Debian buster, Alpine, etc.)  
   - CPU architecture (amd64, arm64, …)

3. Avoid using the 'latest' tag—it's mutable and can break reproducible builds.



3. Debian vs. Alpine Variants

| Variant                      | Compressed Size | Uncompressed Size | Notes                             |
|------------------------------|-----------------|-------------------|-----------------------------------|
| 'node:14.16-buster' (Debian) | ~300 MB         | ~1 GB             | full-featured but large           |
| 'node:14.16-alpine3.13'      | ~40 MB          | ~100 MB           | minimal footprint, faster pulls   |

Tip: Alpine builds are much smaller and ideal for production containers.



4. Selecting & Pinning Your Tag

For this React example, we’ll use:

'''dockerfile
FROM node:14.16.0-alpine3.13
'''

- 14.16.0 = exact Node.js version  
- alpine3.13 = exact Alpine Linux version  

This guarantees consistent builds across environments.



5. Building Your Image

In your project root (where 'Dockerfile' lives), run:

'''bash
docker build -t react-app .
'''

- '-t react-app' tags the image as 'react-app:latest'.  
- '.' tells Docker to use the current directory as build context.

Verify:

'''bash
docker image ls
'''

Look for 'react-app' and note its size (≈ 40 MB from Alpine).



6. Inspecting the New Image

# 6.1 Default Container Command

'''bash
docker run -it react-app
'''

- Launches the default command defined by the base image ('node' REPL).  
- Exit with 'Ctrl+C' twice (first stops REPL, second stops container).

# 6.2 Starting a Shell

Alpine doesn’t include Bash. Use the built-in shell:

'''bash
docker run -it react-app sh
'''

Inside the container:

'''bash
ls /          # Linux filesystem layout
node --version  # Should output "v14.16.0"
'''

You’ll notice no application files yet—those come next with 'COPY' instructions.



Next Steps

- Add your application files into the image ('COPY' / 'WORKDIR').  
- Install dependencies ('RUN npm install').  
- Define startup command ('CMD ["npm","start"]').  

With the right base image selected, you’re ready to proceed and dockerize your React app reliably and efficiently.




Codes and Other Notes in this Discussion: 

https://docs.docker.com/reference/samples/


Dockerfile: 

FROM node:14.16.0-alpine3.13


docker build -t react-app .

docker images 

docker run -it react-app sh 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Copying Files and Directories:
Once you’ve chosen a base image and set your 'WORKDIR', the next step is to bring your application files into the image. Docker provides two instructions—'COPY' and 'ADD'—with almost identical syntax. In nearly all cases, use 'COPY'; reserve 'ADD' only for URL downloads or archive extraction.



1. Build Context

- When you run  
  '''bash
  docker build -t react-app .
  '''  
  the '.' at the end designates your build context: everything in the current directory.  
- Docker client packages up all files/folders under this directory and sends them to the Docker engine.  
- Files outside this context are inaccessible during the build.

'''
Local Directory (Build Context)
/my-project
├── Dockerfile
├── package.json
├── src/
└── .gitignore
'''



2. COPY Instruction

# Syntax

'''
COPY [--chown=<user>:<group>] <src>... <dest>
'''

- '<src>': one or more paths in the build context  
- '<dest>': path inside the image  

# Rules & Examples

1. Copy a single file  
   '''dockerfile
   COPY package.json /app/
   '''
   - Creates '/app/' if it doesn’t exist.

2. Copy multiple files  
   '''dockerfile
   COPY README.md LICENSE /app/docs/
   '''
   - Destination must be a directory and end with '/'.

3. Use wildcards (shell‐style)  
   '''dockerfile
   COPY package*.json /app/
   '''
   - Matches 'package.json' and 'package-lock.json'.

4. Copy entire context  
   '''dockerfile
   COPY . /app/
   '''
   - Copies everything (excluding patterns in '.dockerignore').

5. Relative paths with WORKDIR  
   '''dockerfile
   WORKDIR /app
   COPY . .
   '''
   - After 'WORKDIR', both '.' refer to project root (build context) and '/app/' inside image.

6. Filenames with spaces  
   When a source or dest contains spaces, use the JSON array form:  
   '''dockerfile
   COPY ["hello world.txt", "dest/hello world.txt"]
   '''



3. ADD Instruction

Identical to 'COPY', plus:

- URL support:  
  '''dockerfile
  ADD https://example.com/config.json /app/
  '''
- Archive extraction: auto‐unpacks '.tar', '.tar.gz', '.zip' into the destination.

> Best Practice: Prefer 'COPY' unless you need those extra features.




4. Building & Verifying

1. Dockerfile snippet  
   '''dockerfile
   FROM node:14.16.0-alpine3.13
   WORKDIR /app
   COPY . .
   '''
2. Build the image  
   '''bash
   docker build -t react-app .
   '''
   - Look for 'Transferring context' in the build logs—it shows Docker sending your files.
3. Run an interactive shell  
   '''bash
   docker run -it react-app sh
   '''
4. Inspect files  
   '''bash
   cd /app
   ls
   # All project files (package.json, src/, node_modules/) should be present
   '''



5. Next Up: Excluding Files
To avoid copying unnecessary files (e.g., local logs, 'node_modules/' during build), use a '.dockerignore' file. We’ll cover that in the next lesson.



Codes and Other Notes in this Discussion: 

Dockerfile:

FROM node:14.16.0-alpine3.13
WORKDIR /app
COPY . .
 

docker build -t react-app .


docker run -it react-app sh 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Excluding Files and Directories: 

Excluding Files and Directories with .dockerignore:
By default, Docker’s build context sends every file under your project directory to the Docker engine. Large directories (e.g., 'node_modules/') bloat the context and slow transfers. Use a '.dockerignore' file to exclude unnecessary files.



1. Why Exclude Files?

- Smaller build context → faster network transfers to remote Docker engines.  
- Faster builds → less overhead processing and archiving.  
- Clean images → avoid copying local dev artifacts.



2. Create '.dockerignore'

1. In your project root, create a file named:
   '''
   .dockerignore
   '''
2. List patterns (one per line) to exclude. Example:
   '''
   node_modules/
   .git
   *.log
   '''
   - Follows the same glob patterns as '.gitignore'.



3. Demonstration

- Before adding '.dockerignore', build context was ~150 MB (including 'node_modules/').
- After:
  '''bash
  docker build -t react-app .
  '''
  - You’ll see:
    '''
    Sending build context to Docker daemon  10.24kB
    '''
  - Context reduced to ~10 KB.



4. Impact on Container Filesystem

- Excluded files do not end up in the image.
  '''bash
  docker run -it react-app sh
  cd /app
  ls
  # → shows only project source, not node_modules/
  '''
- You must install dependencies inside the image:
  '''dockerfile
  RUN npm install
  '''



5. Best Practices

- Always ignore large, auto-generated, or sensitive files:  
  - Dependency directories ('node_modules/', 'vendor/')  
  - Source control metadata ('.git/', '.svn/')  
  - Build artifacts ('dist/', 'bin/', 'obj/')  
  - Logs and temp files ('*.log', 'tmp/')  
- Keep '.dockerignore' in sync with '.gitignore' for consistency.



*With '.dockerignore', you ensure lightweight, efficient Docker builds and deploy clean images.*




Codes and Other Notes in this Discussion: 

.dockerignore 
node_modules/

--------------------------------------------------------------------------------------------------------------------------------------------------------------------



Running Commands in Your Dockerfile
The Docker 'RUN' instruction lets you execute any command in the image at build time—installing dependencies, configuring tools, or running OS package managers.



1. Purpose of 'RUN'

- Executes commands in a new build-layer  
- Used for installing dependencies, compiling assets, or any setup steps  
- Runs during image build, not when the container starts



2. Syntax

Two forms:

1. Shell form (runs in '/bin/sh -c' by default):  
   '''dockerfile
   RUN command arg1 arg2
   '''
2. Exec form (avoids shell parsing):  
   '''dockerfile
   RUN ["npm", "install"]
   '''



3. Installing Project Dependencies

In a Node.js app, you typically install dependencies with:

'''dockerfile
WORKDIR /app
COPY . .
RUN npm install
'''

- 'npm install' reads 'package.json' and 'package-lock.json'  
- Downloads all modules into '/app/node_modules/' inside the image



4. Running Other OS Commands

You can run any command available in your base image:

- On Debian/Ubuntu images:  
  '''dockerfile
  RUN apt update && apt install -y python3
  '''
- On Alpine Linux images (no 'apt'):  
  '''dockerfile
  RUN apk add --no-cache python3
  '''

> Tip: Always use the correct package manager for your base image to avoid errors.




5. Building the Image

From your project root (where your 'Dockerfile' lives), run:

'''bash
docker build -t react-app .
'''

- Docker executes each 'RUN' in sequence.  
- Look for the 'npm install' step in the logs to confirm dependency installation.




6. Verifying in a Container

1. Start an interactive shell:
   '''bash
   docker run -it react-app sh
   '''
2. Inspect:
   '''bash
   cd /app
   ls
   # → you should see node_modules/ alongside your source files
   '''
3. Check a module:
   '''bash
   ls node_modules/react
   # → confirms React was installed
   '''



Next Up: Setting environment variables ('ENV') and defining your container’s startup command ('CMD').





Codes and Other Notes in this Discussion: 

Dockerfile:
FROM node:14.16.0-alpine3.13
WORKDIR /app
COPY . .
RUN npm install 


docker build -t react-app . 




--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Setting Environment Variables: 


Setting Environment Variables in Your Dockerfile

Containerized applications often rely on external services—APIs, databases, caches—whose connection details change by environment (development, staging, production). Using environment variables makes your image flexible and configurable at runtime.


1. 'ENV' Instruction

- Purpose: Define one or more environment variables at build time, baked into the image.  
- Syntax (preferred):  
  '''dockerfile
  ENV API_URL=http://api.myapp.com/
  '''
- Alternative syntax:  
  '''dockerfile
  ENV API_URL http://api.myapp.com/
  '''
  Both work; the equal-sign form clearly shows the key/value pair.



2. Dockerfile Example

'''dockerfile
FROM node:14.16.0-alpine3.13

WORKDIR /app
COPY . .
RUN npm install

# Set the API URL for the front-end to consume at runtime
ENV API_URL=http://api.myapp.com/
'''

- Place each 'ENV' near related instructions for clarity (e.g., before your 'CMD' or application start command).



3. Building & Running

1. Build the image (from your project root):  
   '''bash
   docker build -t react-app .
   '''
2. Run a container with an interactive shell:  
   '''bash
   docker run -it react-app sh
   '''



4. Verifying the Variable Inside the Container

- List all environment variables:  
  '''bash
  printenv
  '''
  Look for the 'API_URL' entry:

  '''
  API_URL=http://api.myapp.com/
  '''

- Print a specific variable:  
  '''bash
  printenv API_URL
  # → http://api.myapp.com/
  '''
  or
  '''bash
  echo $API_URL
  # → http://api.myapp.com/
  '''



5. Override at Runtime (Optional)

You can override built-in 'ENV' values when you start a container:

'''bash
docker run -e API_URL=http://staging.api.myapp.com/ -it react-app sh
'''

- '-e' or '--env' flags let you inject or override environment variables per container instance.



With 'ENV' in your Dockerfile—and the ability to override at runtime—you can configure your containerized application for any environment without changing your code.





Codes and Other Notes in this Discussion: 

Dockerfile:
FROM node:14.16.0-alpine3.13
WORKDIR /app
COPY . .
RUN npm install 
ENV API_URL=http://api.myapp.com/



docker build -t react-app . 

docker run -it react-app sh 

printenv
printenv API_URL 
echo $API_URL 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Exposing Ports in Your Dockerfile:
When you run your application in a container, you must tell Docker which container port will listen for traffic. The 'EXPOSE' instruction serves as documentation and a hint for later port mappings—it does not automatically make the port reachable on the host.



1. Local Development (Outside Docker)

- Start the React app with:
  '''bash
  npm start
  '''
- The development server listens on port 3000 of your host machine.
- Visit 'http://localhost:3000' in your browser to access the app.




2. Containers Listen Internally

- Inside a Docker container, your app still listens on port 3000, but only inside that container’s network namespace.
- Host port 3000 is not automatically connected to the container’s port 3000.




3. EXPOSE: Documenting the Listening Port

Add this line to your 'Dockerfile' to declare which port your container uses:

'''dockerfile
FROM node:14.16.0-alpine3.13
WORKDIR /app
COPY . .
RUN npm install
ENV API_URL=http://api.myapp.com/
EXPOSE 3000
'''

- Purpose:  
  - Makes your Dockerfile self-documenting.  
  - Enables tools (and future users) to know your container’s network requirements.  
- Does not:  
  - Open or publish the port on the host.



4. Mapping Host Port to Container Port

> (Covered in the next section)

When you start a container, you’ll explicitly map ports:

'''bash
docker run -p 3000:3000 react-app
'''

- Left side (host):  port you access on your machine  
- Right side (container): port inside the container  



5. Visualization

'''
┌─────────────────┐            ┌─────────────────┐
│ Host Machine    │            │ Docker Container│
│                 │            │ ┌─────────────┐ │
│  [Browser]      │  HTTP      │ │ Node App    │ │
│    ↓            │  traffic   │ │ listens on  │ │
│  localhost:3000 │ ─────────> │ │ port 3000   │ │
│                 │            │ └─────────────┘ │
└─────────────────┘            └─────────────────┘
'''

- EXPOSE 3000 in the Dockerfile tells Docker and readers: _“This container will listen on port 3000.”_



With 'EXPOSE', you make your containers’ network interfaces explicit and prepare for clear, consistent port bindings when running the container.




Codes and Other Notes in this Discussion: 

Dockerfile:

FROM node:14.16.0-alpine3.13
WORKDIR /app
COPY . .
RUN npm install 
ENV API_URL=http://api.myapp.com/
EXPOSE 3000


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Setting the User: 

Setting a Non-Root User in Your Dockerfile:
By default, Docker containers run as root, which can introduce security risks. Running your application under a restricted user reduces the attack surface.



1. Experimenting in an Alpine Shell

Start an interactive Alpine session to practice user/group commands:

'''bash
docker run -it alpine sh
'''

Inside the container:

1. Create a group called 'app':  
   '''sh
   addgroup app
   '''
2. Create a system user 'app' in the 'app' group:  
   '''sh
   adduser -S -G app app
   '''
   - '-S' → system user (no password, no home)  
   - '-G app' → primary group

3. Verify group membership:  
   '''sh
   groups app
   # → app : app
   '''

4. Combine into one line (create group and user):  
   '''sh
   addgroup app && adduser -S -G app app
   groups app
   '''

Exit the shell when you’re done:
'''sh
exit
'''



2. Integrating into Your Dockerfile

Add the same commands in a 'RUN' step, then switch to the new user:

'''dockerfile
FROM node:14.16.0-alpine3.13
WORKDIR /app

COPY . .
RUN npm install

ENV API_URL=http://api.myapp.com/
EXPOSE 3000

# Create group 'app' and system user 'app'
RUN addgroup app \
 && adduser -S -G app app

# Switch to the 'app' user for all following steps and at runtime
USER app
'''

- 'RUN addgroup … && adduser …'  
  Builds a restricted user in one layer.  
- 'USER app'  
  All subsequent instructions (and container start) run under app, not root.




3. Building and Verifying

1. Build the image:  
   '''bash
   docker build -t react-app .
   '''
2. Run as 'app' user:  
   '''bash
   docker run -it react-app sh
   '''
3. Confirm identity:  
   '''sh
   whoami
   # → app
   '''
4. Inspect file ownership:  
   '''sh
   ls -l /app
   # Files owned by root; app cannot write
   echo "touch newfile" > try.sh
   sh try.sh
   # → Permission denied
   '''



4. Why This Matters

- Least privilege: The 'app' user cannot modify application files owned by root.  
- Security: Even if an attacker exploits your app, they’re jailed under a non-privileged account.  
- Best Practice: Always switch from root to a lesser-privileged user in production containers.



With a dedicated system user and group defined in your Dockerfile, you ensure your containerized application runs with minimal privileges and improved security.




Codes and Other Notes in this Discussion: 

docker run -it alpine 

addgroup app 

adduser -S -G app app

groups app 

addgroup mosh && adduser -S -G mosh mosh 
groups mosh 



Dockerfile:

FROM node:14.16.0-alpine3.13
WORKDIR /app
COPY . .
RUN npm install 
ENV API_URL=http://api.myapp.com/
EXPOSE 3000
RUN addgroup app && adduser -S -G app app 
USER app  


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Defining Entrypoints: 

Defining the Startup Command in Your Dockerfile
Your Dockerfile should specify how the container starts your application. Use 'CMD' (default) or 'ENTRYPOINT' to set the runtime command; these instructions differ from build-time 'RUN'.



1. Why Define a Default Command?

- Avoid typing the command every time you run the container:  
  '''bash
  docker run react-app npm start
  '''
- Instead, configure the Dockerfile so simply:  
  '''bash
  docker run react-app
  '''



2. 'CMD' vs 'ENTRYPOINT'

| Instruction   | Purpose                                        | Overridable by 'docker run'?        |
|---------------|------------------------------------------------|-------------------------------------|
| 'CMD'         | Default arguments or command at runtime    | Yes—any arguments you pass replace it |
| 'ENTRYPOINT'  | Fixed executable, with optional default args | Harder—you must use '--entrypoint'  |

- Both have shell form and exec form.  
- Exec form (array) is preferred—avoids extra shell and handles signals properly.



3. Example Dockerfile with 'CMD'

'''dockerfile
FROM node:14.16.0-alpine3.13

# Create non-root user
RUN addgroup app && adduser -S -G app app
USER app

WORKDIR /app
COPY . .
RUN npm install

ENV API_URL=http://api.myapp.com/
EXPOSE 3000

# Default command to start the React app
CMD ["npm", "start"]
'''

- Exec form: 'CMD ["npm", "start"]'  
- Shell form (not recommended): 'CMD npm start'




4. Running Your Container

'''bash
docker build -t react-app .
docker run react-app
'''

- Container starts automatically with 'npm start', serving on port 3000.



5. Overriding 'CMD'

- Passing arguments replaces 'CMD' entirely:
  '''bash
  docker run react-app echo hello
  '''
  - Prints 'hello' instead of running 'npm start'.



6. Using 'ENTRYPOINT' (Optional)

If you want to lock in the main executable and only allow extra args:

'''dockerfile
ENTRYPOINT ["npm", "start"]
'''

- Shell form: 'ENTRYPOINT npm start'  
- Exec form: 'ENTRYPOINT ["npm", "start"]'

To override an 'ENTRYPOINT', you must use the '--entrypoint' flag:

'''bash
docker run --entrypoint echo react-app hello
# → hello
'''



7. Build-Time vs. Run-Time

- 'RUN' steps happen during build, creating image layers.  
- 'CMD'/'ENTRYPOINT' happen when you 'docker run', starting the container process.



Tip: Use:
- 'RUN' for installing, setting up layers  
- 'CMD' (exec form) for default startup command  
- 'ENTRYPOINT' when you want a fixed executable and only want to pass args   

This setup ensures a streamlined developer experience and container behavior.





Codes and Other Notes in this Discussion: 

docker run react-app npm start 


Dockerfile:
FROM node:14.16.0-alpine3.13
RUN addgroup app && adduser -S -G app app 
USER app  
WORKDIR /app
COPY . .
RUN npm install 
ENV API_URL=http://api.myapp.com/
EXPOSE 3000
CMD npm start 


docker run react-app 


#shell form 
CMD npm start 

#Exec form 
CMD ["npm", "start"]


ENTRYPOINT npm start 

ENTRYPOINT ["npm", "start"]

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Speeding Up Builds: 

Speeding Up Docker Builds with Layer Caching:
Docker images are built in layers, each corresponding to one instruction in your Dockerfile. By understanding how layers and the build cache work, you can dramatically reduce rebuild time.



1. Docker Image Layers

- Each Dockerfile instruction (e.g., 'FROM', 'RUN', 'COPY') creates a new layer.  
- A layer captures only the filesystem changes from that instruction.  
- The final image is the stack of all layers, from base to top.



2. Inspecting Layers

Use 'docker history' to view your image’s layers:

'''bash
docker history react-app
'''

Sample output (read bottom to top):

| CREATED BY                          | SIZE     |
|-------------------------------------|----------|
| 'npm start'                         | 0 B      |
| 'EXPOSE 3000'                       | 0 B      |
| 'ENV API_URL=http://api.myapp.com/' | 0 B      |
| 'COPY . .'                          | 1.6 MB   |
| 'RUN npm install'                   | 178 MB   |
| 'WORKDIR /app'                      | 0 B      |
| 'USER app'                          | 0 B      |
| 'RUN addgroup ... && adduser ...'   | 4 KB     |
| 'node:14.16-alpine3.13' (base)      | 40 MB+   |




3. How Layer Caching Works

1. First build: Docker executes each instruction and caches its resulting layer.  
2. Subsequent builds:  
   - Docker checks each instruction against the cache.  
   - If the instruction and its inputs (files, environment) are unchanged, Docker reuses the cached layer.  
   - If any input changed, that layer and all following layers must be rebuilt.



4. The Cache-Busting Problem

- 'COPY . .' copies all project files.  
- Even a tiny edit (e.g., a one-line change in 'README.md') changes the inputs for 'COPY . .'.  
- This invalidates the cache for 'COPY . .' and every layer after it, including the heavy 'RUN npm install' step.



5. Optimizing Your Dockerfile

Goal: Separate rarely changing steps from frequently changing code.

# Standard (Slow) Order

'''dockerfile
COPY . .
RUN npm install
... rest of Dockerfile ...
'''

- Issue: Any code change busts the cache before 'npm install'.

# Optimized (Fast) Order

'''dockerfile
# 1. Base image and user setup (rarely changes)
FROM node:14.16.0-alpine3.13
RUN addgroup app && adduser -S -G app app
USER app

# 2. Dependencies: only package files (rarely change)
WORKDIR /app
COPY package*.json ./
RUN npm install

# 3. Application source (changes frequently)
COPY . .

# 4. Runtime configuration
ENV API_URL=http://api.myapp.com/
EXPOSE 3000
CMD ["npm", "start"]
'''

- Step 2 ('COPY package*.json' + 'RUN npm install')  
  - Cache is reused unless you change dependencies.  
- Step 3 ('COPY . .')  
  - Always invalidated on code edits, but only affects layers after it.




6. Demonstration

1. First build (no cache yet):  
   '''bash
   docker build -t react-app .
   '''
2. Edit a source file (e.g., add a line in 'src/App.js').  
3. Rebuild:  
   '''bash
   docker build -t react-app .
   '''
   - You’ll see Docker caching all steps up to 'RUN npm install'.  
   - Only the 'COPY . .' and subsequent layers rebuild—no reinstallation of 'npm' packages.



7. Key Takeaways

- Order matters: Put stable instructions (install deps) before copying changing code.  
- Minimize invalidated layers: Copy only what’s needed for a step.  
- Leverage caching: Cuts minutes off rebuilds when editing code.



By reorganizing your Dockerfile into dependency steps and source steps, you harness Docker’s layer caching to speed up development and CI builds.




Codes and Other Notes in this Discussion: 

docker history react-app 


Dockerfile:

FROM node:14.16.0-alpine3.13
RUN addgroup app && adduser -S -G app app 
USER app  
WORKDIR /app
COPY package*.json .
RUN npm install 
COPY . .
ENV API_URL=http://api.myapp.com/
EXPOSE 3000
CMD npm start 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Removing Images: 



Removing Docker Images and Containers:
Efficiently clean up unused images and containers to free disk space and keep your Docker environment tidy.


1. List All Images

'''bash
docker images
'''

- Columns:  
  - REPOSITORY and TAG show image name/tag.  
  - '<none>:<none>' entries are dangling images (untagged layers).



2. Clean Up Dangling Images

'''bash
docker image prune
'''

- Prompts:  
  '''
  WARNING! This will remove all dangling images.
  Are you sure you want to continue? [y/N]
  '''
- Removes only layers that are not referenced by any tagged image.



3. List Containers

- Running containers:  
  '''bash
  docker ps
  '''
- All containers (including stopped):  
  '''bash
  docker ps -a
  '''



4. Remove Stopped Containers

'''bash
docker container prune
'''

- Prompts:  
  '''
  WARNING! This will remove all stopped containers.
  Are you sure you want to continue? [y/N]
  '''
- Frees space used by containers no longer running.



 5. Prune Images Again

After removing stopped containers, dangling image layers may become unreferenced:

'''bash
docker image prune
'''

- Now removes any remaining dangling images.



6. Remove Specific Images

Use 'docker image rm' (alias: 'docker rmi') to delete one or more images:

- By name/tag:
  '''bash
  docker image rm react-app
  '''
- By image ID:
  '''bash
  docker image rm df3
  '''
- Multiple images:
  '''bash
  docker image rm react-app old-image another-id
  '''



7. Docker Image Subcommands

'''bash
docker image
'''

- Common operations:  
  - 'build' – build an image from a Dockerfile  
  - 'ls' / 'images' – list images  
  - 'history' – show image layer history  
  - 'prune' – remove dangling images  
  - 'rm' / 'rmi' – remove specific images  
  - 'save' / 'load' – export/import images as tarballs  



Quick Reference

| Action                          | Command                          |
|---------------------------------|----------------------------------|
| List all images                 | 'docker images'                  |
| Prune dangling images           | 'docker image prune'             |
| List all containers             | 'docker ps -a'                   |
| Prune stopped containers        | 'docker container prune'         |
| Remove a specific image by name | 'docker image rm <name>'         |
| Remove a specific image by ID   | 'docker image rm <image-id>'     |
| Remove multiple images          | 'docker image rm <id1> <id2> ...'|



With these commands, you can regularly clean and manage your Docker environment, ensuring optimal performance and disk usage.



Codes and Other Notes in this Discussion: 

docker images 

docker image prune 


docker ps -a 

docker container prune 

docker image rm react-app 

docker image rm df3

docker image rm hello-docker 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------


Tagging Images: 


Tagging Docker Images
Proper tagging gives each image a clear, immutable identifier—critical for reproducible deployments, rollbacks, and troubleshooting.



1. The 'latest' Tag

- Default behavior:  
  When you run 'docker build -t myapp .' without specifying a tag, Docker assigns the 'latest' tag: 'myapp:latest'.

- Why not use 'latest' in production?  
  - It’s just a label—not a guarantee of recency.  
  - Overwrites can happen out of order, making it hard to know what’s deployed.  
  - Rollbacks become guesswork if you don’t know which version “latest” points to.

- Recommendation:  
  - Development: 'latest' is fine.  
  - Staging/Production: use explicit version tags ('v1.0.0', 'build-123', etc.).



2. Tagging During Build

Use '-t' with name:tag to apply a tag at build time:

'''bash
docker build -t react-app:1.0.0 .
'''

- Examples of tag schemes:  
  - Semantic versioning: '1.0.0', '2.1.5'  
  - Code names: 'buster', 'hydrogen'  
  - Build numbers: 'build-76', '20230708.3'



3. Listing Images & Tags

'''bash
docker images
'''

| REPOSITORY | TAG     | IMAGE ID   | SIZE  |
|------------|---------|------------|-------|
| react-app  | latest  | 'b06f...'  | 180MB |
| react-app  | 1.0.0   | 'b06f...'  | 180MB |
| react-app  | 2.0.0   | 'd4a3...'  | 183MB |

- Notice same IMAGE ID for 'latest' and '1.0.0': two tags point to the same image.



4. Removing a Tag vs. Removing an Image

- Remove a specific tag (does not delete the underlying image if another tag exists):
  '''bash
  docker image rm react-app:1.0.0
  '''
- Remove an image outright by its ID or last tag:
  '''bash
  docker image rm d4a3...
  '''



5. Tagging After Build

To add or retag an existing image:

'''bash
docker image tag SOURCE_IMAGE[:TAG] TARGET_REPO:TARGET_TAG
'''

- By name/tag:  
  '''bash
  docker image tag react-app:latest react-app:2.0.0
  '''
- By image ID:  
  '''bash
  docker image tag b06f... react-app:latest
  '''



6. Keeping 'latest' Accurate

1. Build v1.0.0:
   '''bash
   docker build -t react-app:1.0.0 .
   '''
2. Tag 'latest':
   '''bash
   docker image tag react-app:1.0.0 react-app:latest
   '''
3. Build v2.0.0 after code change:
   '''bash
   docker build -t react-app:2.0.0 .
   '''
4. Retag 'latest' to v2.0.0:
   '''bash
   docker image tag react-app:2.0.0 react-app:latest
   '''



7. Visualizing Tags vs. IDs

'''
           +--------------------------------+
           |        IMAGE ID: b06f...       |
           +--------------------------------+
            ↙               ↘
     react-app:1.0.0     react-app:latest
'''

- One ID, multiple tags pointing at it.



Key Commands Summary

| Task                                  | Command                                                |
|---------------------------------------|--------------------------------------------------------|
| Build with explicit tag               | 'docker build -t repo:tag .'                           |
| List all images & tags                | 'docker images'                                        |
| Remove a tag (leaving other tags)     | 'docker image rm repo:tag'                             |
| Remove an image by ID                 | 'docker image rm <image-id>'                           |
| Tag an existing image by name/tag     | 'docker image tag srcRepo:srcTag destRepo:destTag'     |
| Tag an existing image by ID           | 'docker image tag <image-id> destRepo:destTag'         |



Takeaway:  
Always use explicit, immutable tags—never rely on 'latest' in production. Regularly retag 'latest' yourself to maintain clarity on which version is actually “current.”



Codes and Other Notes in this Discussion:  

docker build -t react-app:buster 

docker build -t react-app:3.1.5

docker build -t react-app:76


docker image remove react-app:1


docker image tag react-app:latest react-app:1


docker image tag b06 react-app:latest 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Sharing Images: 


Sharing Docker Images
Distribute your custom images to collaborators or deploy them on any Docker host by pushing to a registry like Docker Hub.



1. Set Up a Docker Hub Repository

1. Sign up at https://hub.docker.com (free tier).  
2. Create a repository (similar to GitHub):  
   - Name: usually '<your-username>/<image-name>'  
   - Visibility: public (unlimited) or private (1 free private repo)  
   - Optional CI integration: connect to GitHub for automated builds



2. Tag Your Local Image for the Remote Repo

Docker identifies images by 'REPOSITORY:TAG'. To push to your Docker Hub repo, retag a local image:

'''bash
# Identify the image ID or local tag
docker images
# Example shows IMAGE ID b06f... for react-app:2

# Tag for Docker Hub (defaults to :latest if no tag)
docker image tag b06f... codewithmosh/react-app:2
'''

- 'b06f...' → image ID (first few characters suffice)  
- 'codewithmosh' → your Docker Hub username  
- 'react-app:2' → repo name and version tag  



3. Authenticate with Docker Hub

Before pushing, log in from the CLI:

'''bash
docker login
# Enter Docker Hub username & password when prompted
'''



4. Push Your Image

Send all layers of 'codewithmosh/react-app:2' to Docker Hub:

'''bash
docker push codewithmosh/react-app:2
'''

- First push may be slow (large layers).  
- Subsequent pushes upload only changed layers—much faster.



5. Verify on Docker Hub

- Refresh your repository page on Docker Hub.  
- You should see Tag: 2 listed under your repository.



6. Update & Push New Versions

1. Modify code (e.g., 'README.md').  
2. Rebuild locally with new tag:
   '''bash
   docker build -t react-app:3 .
   '''
3. Retag for Docker Hub:
   '''bash
   docker image tag react-app:3 codewithmosh/react-app:3
   '''
4. Push new version:
   '''bash
   docker push codewithmosh/react-app:3
   '''
5. Verify both 2 and 3 appear under your repo.



7. Pull & Run Anywhere

On any Docker-enabled host:

'''bash
docker pull codewithmosh/react-app:3
docker run -d -p 3000:3000 codewithmosh/react-app:3
'''

Now your image runs identically across environments.



8. Workflow Diagram

'''
[Local Image]                           [Docker Hub]
┌──────────────┐      docker push      ┌──────────────────┐
│ react-app:2  │ ───────────────────▶  │ codewithmosh/    │
└──────────────┘                       │ react-app:2      │
                                       └──────────────────┘

[Local Image]                           [Docker Hub]
┌──────────────┐      docker push      ┌──────────────────┐
│ react-app:3  │ ───────────────────▶  │ codewithmosh/    │
└──────────────┘                       │ react-app:3      │
                                       └──────────────────┘
'''



Key Commands Summary

| Action                    | Command                                                                    |
|---------------------------|----------------------------------------------------------------------------|
| Tag local image           | 'docker image tag <ID> username/repo:tag'                                  |
| Log in to Docker Hub      | 'docker login'                                                             |
| Push image to registry    | 'docker push username/repo:tag'                                            |
| Pull image from registry  | 'docker pull username/repo:tag'                                            |
| Run pulled image          | 'docker run -d -p hostPort:containerPort username/repo:tag'                |

Use this workflow to share, deploy, and version your Dockerized applications across teams and environments.




Codes and Other Notes in this Discussion: 

docker image tag b06 codewithmosh/react-app:2 


docker login 

docker push codewithmosh/react-app:2 


docker build -t react-app:3 . 

docker image tag react-app:3 codewithmosh/react-app:3

docker push codewithmosh/react-app:3


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Saving and Loading Images: 

Saving and Loading Docker Images:
Use 'docker image save' and 'docker image load' to transfer images between hosts without a registry. Images are exported as tar archives containing all layers, metadata, and configuration.



1. Saving an Image to a Tarball

# Command Syntax

'''bash
docker image save -o <output-file.tar> <repository:tag>
# or using long form:
docker image save --output react-app.tar react-app:3
'''

- '-o' / '--output': Path to write the tar archive.  
- '<repository:tag>': The image you want to export (e.g., 'react-app:3').

# Example

'''bash
docker image save -o react-app.tar react-app:3
'''

- Creates 'react-app.tar' in the current directory.  
- Export may take time proportional to image size (especially large dependency layers).

# Exploring the Archive

After extraction ('tar -tf react-app.tar'), you’ll see a structure like:

'''
react-app.tar
├── manifest.json
├── config.json
├── sha256_<layer1>/
│   ├── layer.tar
│   └── json
├── sha256_<layer2>/
│   ├── layer.tar
│   └── json
└── …more layers…
'''

- 'layer.tar' holds the filesystem changes for that layer.  
- '.json' files store layer metadata.  



2. Removing the Local Image

To test loading, first delete the image (and any remaining tags):

'''bash
docker image rm react-app:3
# If still present under another tag, remove by image ID:
docker image rm <image-id>
'''

Confirm with 'docker images'—the image should no longer appear.



3. Loading an Image from a Tarball

# Command Syntax

'''bash
docker image load -i <input-file.tar>
# or long form:
docker image load --input react-app.tar
'''

- '-i' / '--input': Path to the tar archive to read.

# Example

'''bash
docker image load -i react-app.tar
'''

- Reads the tarball and re-creates the image (with all original tags).  
- Output confirms loaded repository and tag.

Verify with:

'''bash
docker images | grep react-app
# Should list react-app:3 again
'''



4. Quick Command Reference

| Action                                    | Command                                                             |
|-------------------------------------------|---------------------------------------------------------------------|
| View save options                         | 'docker image save --help'                                          |
| Save image to tar                         | 'docker image save -o react-app.tar react-app:3'                    |
| View load options                         | 'docker image load --help'                                          |
| Load image from tar                       | 'docker image load -i react-app.tar'                                |
| Remove image by name and tag              | 'docker image rm react-app:3'                                       |
| Remove image by ID                        | 'docker image rm <image-id>'                                        |



Benefit: Quickly move images between air-gapped networks or machines without relying on Docker Hub or other registries.




Codes and Other Notes in this Discussion: 

docker image save --help 

docker image save -o react-app.tar react-app:3 

docker image load --help

docker image load -i react-app.tar 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Summary: 
Images

Dockerfile instructions

FROM # to specify the base image
WORKDIR # to set the working directory
COPY # to copy files/directories
ADD # to copy files/directories
RUN # to run commands
ENV # to set environment variables
EXPOSE # to document the port the container is listening on
USER # to set the user running the app
CMD # to set the default command/program
ENTRYPOINT # to set the default command/program



Image commands:
docker build -t <name> .
docker images
docker image ls
docker run -it <image> sh



Starting and stopping containers:
docker stop <containerID>
docker start <containerID>


Removing containers:
docker container rm <containerID>
docker rm <containerID>
docker rm -f <containerID> # to force the removal
docker container prune # to remove stopped containers


Volumes:
docker volume ls
docker volume create app-data
docker volume inspect app-data
docker run -v app-data:/app/data <image>



Copying files between the host and containers:
docker cp <containerID>:/app/log.txt .
docker cp secret.txt <containerID>:/app



Sharing source code with containers:

docker run -v $(pwd):/app <image>


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Working With Containers: 

Introduction: 

Welcome back to another section of the Ultimate Docker course. In this section we're going to explore containers in more detail. We'll talk about starting and stopping containers, publishing ports, viewing container logs, executing commands in running containers, removing containers, persisting data using volumes, and sharing source code with containers so we don't have to rebuild our image every time we make a change in our code. This is a short and sweet section, so let's jump in and get started.

In this Section: 

- Starting & Stopping Containers  
  How to launch containers in the foreground or background, and how to cleanly stop them.

- Publishing Ports  
  Mapping container ports to host ports so external clients (browsers, APIs) can reach your services.

- Viewing Container Logs  
  Using 'docker logs' to inspect stdout/stderr output from running containers.

- Executing Commands in Containers  
  Attaching to a running container with 'docker exec' for debugging or one-off tasks.

- Removing Containers  
  Cleaning up stopped containers ('docker rm' and 'docker container prune') to reclaim disk space.

- Persisting Data Using Volumes  
  Storing and sharing data between containers and the host filesystem so state survives restarts.

- Sharing Source Code  
  Mounting your project directory into a container for live code updates—no rebuild required.




--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Starting Containers: 
Review core commands for launching containers, with tips for detached mode and naming.



1. Verify Available Images

List images on your machine:

'''bash
docker images
'''

Typical output:
'''
REPOSITORY   TAG       IMAGE ID       SIZE
react-app    latest    b06f4e2a3d5c   180MB
ubuntu       20.04     f643c72bc252   72MB
alpine       3.13      a24bb4013296   5.6MB
'''

You only need react-app:latest for this section.



2. Build Your React-App Image (If Needed)

If you haven’t built it yet:

1. Download and extract the provided ZIP.  
2. In the project root (where 'Dockerfile' lives), run:
   '''bash
   docker build -t react-app .
   '''



3. Check Running Containers

'''bash
docker ps
'''

- Shows only running containers.  
- Empty output means no containers are active.



4. Running a Container in the Foreground

'''bash
docker run react-app
'''

- Starts the React development server (listening on port 3000).  
- Blocks your terminal session—typing commands isn’t possible.  
- Press 'Ctrl+C' stops the container.



5. Detached Mode ('-d')

Run the container in the background:

'''bash
docker run -d react-app
'''

- Returns a Container ID immediately.  
- Frees your terminal for other commands.  
- Container continues running the web server.

# Example

'''
$ docker run -d react-app
9f1c2b3a4d5e
$ docker ps
CONTAINER ID   IMAGE      COMMAND       CREATED          STATUS          PORTS     NAMES
9f1c2b3a4d5e   react-app  "npm start"   10 seconds ago   Up 8 seconds    3000/tcp  affectionate_roentgen
'''



6. Assigning a Custom Name:
Docker auto-generates whimsical names (e.g., 'affectionate_roentgen'). For clarity, use '--name':

'''bash
docker run -d --name blue-sky react-app
'''

Verify:

'''bash
docker ps
'''

| CONTAINER ID | IMAGE     | COMMAND       | STATUS       | PORTS     | NAMES    |
|--------------|-----------|---------------|--------------|-----------|----------|
| f6g7h8i9j0   | react-app | "npm start"   | Up 5 seconds | 3000/tcp  | blue-sky |



7. Quick Tips:
- Stop a container:  
  '''bash
  docker stop <container-id_or_name>
  '''
- View logs:  
  '''bash
  docker logs blue-sky
  '''
- List all containers (running + stopped):  
  '''bash
  docker ps -a
  '''



8. Visual Workflow

'''
[ Foreground Run ]
$ docker run react-app
←─ blocks until you press Ctrl+C

[ Detached Run ]
$ docker run -d --name blue-sky react-app
9f1c2b3a4d5e  ← Container ID returned immediately
$  ← Terminal prompt available for other commands
'''

With these commands, you can efficiently start containers in the mode and naming scheme that best suits your workflow.



Codes and Other Notes in this Discussion: 

docker ps 

docker run -d react-app 

docker run -d --name blue-sky react-app


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Viewing the Logs:

Viewing Container Logs:
When running containers in detached mode, your terminal is free—but you still need visibility into what’s happening inside. The 'docker logs' command retrieves a container’s STDOUT and STDERR streams.



1. Identify Your Container:
First, list running containers to get the ID or name:

'''bash
docker ps
'''

Example output:

'''
CONTAINER ID   IMAGE      COMMAND     STATUS     PORTS     NAMES
9f1c2b3a4d5e   react-app  "npm start" Up 2 minutes 3000/tcp affectionate_roentgen
f6g7h8i9j0a1   react-app  "npm start" Up 1 minute  3000/tcp blue-sky
'''

You can reference either '9f1c2b3a4d5e' or its name 'affectionate_roentgen', and similarly for 'blue-sky'.



2. Basic Log Retrieval

'''bash
docker logs <container>
'''

Retrieve all log output since the container started:

'''bash
docker logs 9f1c2b3a4d5e
'''



3. Follow Real-Time Logs

Use '-f' (or '--follow') to stream logs as they arrive:

'''bash
docker logs -f blue-sky
'''

Press 'Ctrl+C' to stop following.



4. Tail Specific Lines

Use '-n' (or '--tail') to view only the last *N* lines:

'''bash
docker logs -n 5 9f1c2b3a4d5e
'''

Shows the five most recent log entries.



5. Show Timestamps

Add '-t' (or '--timestamps') to prefix each line with its timestamp:

'''bash
docker logs -n 5 -t blue-sky
'''

Example output:

'''
2023-07-10T14:02:01.123456789Z Server listening on port 3000
2023-07-10T14:02:03.987654321Z GET /api/users 200
...
'''



6. Quick Help

For all options, use:

'''bash
docker logs --help
'''



7. ASCII Diagram of Log Flow

'''
+-------------+         docker logs          +--------------+
|             | ────────────────────────────▶|              |
|  Container  |         -f, -n, -t           |  Terminal    |
| (background)|                              |  Window      |
+-------------+                              +--------------+
'''



Keep these commands handy to diagnose errors, monitor runtime behavior, and ensure your applications inside Docker containers run smoothly.



Codes and Other Notes in this Discussion: 

docker logs 655

docker logs --help

docker logs -f 123

docker logs -n 5 655 

docker logs -n 5 -t 655 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Publishing Ports: 



Publishing Ports
Docker containers listen on ports inside their own network namespace. To make those ports accessible on the host machine, you must publish them when running the container.



1. Why Publishing Is Needed

- Container port 3000 is open inside the container.  
- Host port 3000 is closed by default—browsers and external clients can’t reach it.  
- Each container can independently listen on port 3000 internally without conflict.



2. Check Current Port Mappings

Use 'docker ps' to view port mappings ('PORTS' column):

'''bash
docker ps
'''

Example:

'''
CONTAINER ID  IMAGE       COMMAND      STATUS        PORTS       NAMES
9f1c2b...     react-app   "npm start"  Up 5 mins    3000/tcp    blue-sky
f6g7h...     react-app   "npm start"  Up 2 mins    3000/tcp    affectionate_roentgen
'''

- '3000/tcp' indicates internal listening but no host mapping.



3. Publish a Host Port

When running a container, use '-p hostPort:containerPort':

'''bash
docker run -d \
  -p 80:3000 \
  --name c1 \
  react-app
'''

- '-d': detached mode  
- '-p 80:3000': map host port 80 → container port 3000  
- '--name c1': assign container name



4. Verify Access

- Open your browser to 'http://localhost:80'.  
- You should see the React application running.



5. Confirm Port Mapping

Run 'docker ps' again:

'''
CONTAINER ID  IMAGE       COMMAND      STATUS        PORTS                NAMES
a1b2c3d4e5f6  react-app   "npm start"  Up 10 sec     0.0.0.0:80->3000/tcp c1
9f1c2b3a4d5e  react-app   "npm start"  Up 15 mins    3000/tcp             blue-sky
'''

- '0.0.0.0:80->3000/tcp' shows host port 80 forwarded to container port 3000.



6. Multiple Host Mappings

You can map multiple host ports:

'''bash
docker run -d -p 8080:3000 -p 8443:3443 myservice
'''

- Maps host 8080 → container 3000  
- Maps host 8443 → container 3443  



7. IPv6 & Binding

- By default, Docker binds to '0.0.0.0' (all interfaces).  
- To bind only to localhost:
  '''bash
  docker run -p 127.0.0.1:80:3000 react-app
  '''



With port publishing configured, your containerized services become reachable from your host and beyond.



Codes and Other Notes in this Discussion: 

docker run -d -p 80:3000 --name c1 react-app 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Executing Commands in Running Containers: 
You can interact with a live container at any time using 'docker exec'. This allows troubleshooting, inspecting files, or running one-off commands without stopping or restarting the container.


1. 'docker run' vs. 'docker exec'

- 'docker run'  
  - Creates and starts a new container  
  - Executes the specified command as the container’s main process  
- 'docker exec'  
  - Executes a command inside an existing, running container  
  - Does not start a new container



2. Identify Your Container

List running containers to get the ID or name:

'''bash
docker ps
'''

Example output:

'''
CONTAINER ID  IMAGE      NAMES      STATUS
a1b2c3d4e5f   react-app  c1         Up 5 minutes
'''

- Here we’ll use the container name 'c1'.



3. Run a One-Off Command

Execute a non-interactive command (e.g., list files):

'''bash
docker exec c1 ls
'''

- Why '/app'?  
  Because the Dockerfile set 'WORKDIR /app', so commands run there by default.



4. Start an Interactive Shell

Open a shell session inside the container:

'''bash
docker exec -it c1 sh
'''

- '-i': Keep STDIN open  
- '-t': Allocate a pseudo-TTY  
- 'sh': Alpine’s default shell ('bash' may not exist)

Once inside, you can run:

'''sh
ls
pwd
cat somefile.txt
'''

Exit the shell with:

'''sh
exit
'''



5. Container State Remains Running

After exiting the shell:

'''bash
docker ps
'''

You’ll see 'c1' is still Up—your 'exec' session did not stop the container.




6. ASCII Workflow Diagram

'''
┌──────────────────────────────┐
│ Host Shell                   │
│ ┌──────────────────────────┐ │
│ │ docker exec -it c1 sh    │ │
│ └─────────────┬────────────┘ │
└───────────────│──────────────┘
                │
┌───────────────▼──────────────┐
│ Container “c1”               │
│  WORKDIR /app                │
│  ┌────────────────────────┐  │
│  │ Interactive Shell      │  │
│  └────────────────────────┘  │
└──────────────────────────────┘
'''

With 'docker exec', you gain live, interactive control over running containers—ideal for debugging and ad-hoc maintenance.




Codes and Other Notes in this Discussion: 

docker exec c1 ls 

docker exec -it c1 sh 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Stopping and Starting Containers: 
Just like a lightweight VM, Docker containers can be cleanly stopped and later restarted without recreating them.



1. Stopping a Container

- Command:  
  ```bash
  docker stop c1
  ```
- What happens:  
  - Sends a SIGTERM (then SIGKILL after a timeout) to the main process.  
  - Container transitions from Running → Exited.

- Verify:  
  ```bash
  docker ps
  ```
  The stopped container no longer appears in the running list.

- Impact on Service:  
  - Your application becomes unavailable.  
  - Refreshing `http://localhost:…` will fail until you restart.



2. Starting a Stopped Container

- Command:  
  ```bash
  docker start c1
  ```
- What happens:  
  - Restarts the existing container (`c1`) and its original command/process.  
  - Container transitions from Exited → Running.

- Difference vs. `docker run`:  
  | Command      | Action                                         |
  |--------------|------------------------------------------------|
  | `docker run` | Creates and starts a new container     |
  | `docker start` | Starts an already-created (stopped) container |



3. Lifecycle Diagram

```
[ Running ] ── docker stop ──▶ [ Exited ]
    ▲                             │
    │                             └─ docker start ──▶ [ Running ]
    │
docker run
    │
[ New Container in Running ]
```



With `docker stop` and `docker start`, you can pause and resume containers quickly—ideal for maintenance, testing, or resource management.




Codes and Other Notes in this Discussion: 

docker stop c1 

docker start c1 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Removing Containers:
You can delete containers by name or ID, either individually or in bulk. Stopped containers must be removed before cleaning up disk space.



1. Basic Removal Commands

- By full command:  
  '''bash
  docker container rm c1
  '''
- Shortcut:  
  '''bash
  docker rm c1
  '''

Attempting to remove a running container without stopping it first produces an error:
'''
Error response from daemon: You cannot remove a running container c1
'''



2. Stopping & Forcing Removal

# 2.1 Stop then Remove

1. Stop the container:
   '''bash
   docker stop c1
   '''
2. Remove it:
   '''bash
   docker rm c1
   '''

2.2 Force Removal

- Combines stop and remove in one step:
  '''bash
  docker rm -f c1
  '''



3. Viewing Stopped Containers

- List all containers (running + stopped):
  '''bash
  docker ps -a
  '''
- Filter by name (Linux/macOS):
  '''bash
  docker ps -a | grep c1
  '''
  Returns only entries matching 'c1'.



4. Bulk Removal of Stopped Containers

- Use the prune command to delete all stopped containers:
  '''bash
  docker container prune
  '''
  You’ll be prompted for confirmation:
  '''
  WARNING! This will remove all stopped containers.
  Are you sure you want to continue? [y/N]
  '''



5. Workflow Diagram

'''
[ Running Container ]
        │
   docker stop c1
        ↓
[ Exited Container ]
        │
   docker rm c1   ← or docker rm -f c1
        ↓
[ Container Removed ]
'''

With these commands, you can remove individual containers safely or clean up all stopped containers in one action.



Codes and Other Notes in this Discussion: 

docker container rm c1 

docker rm c1 

docker rm -f c1 

docker ps -a 

docker ps -a | grep c1 

docker container prune 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Containers File System: 


Containers’ File System Isolation:
Each Docker container gets its own writable layer on top of shared image layers. Changes inside one container are invisible to others and are lost when the container is removed.



1. Experiment: Writing in One Container, Checking Another

1. Ensure two containers are running from the same image ('react-app'):  
   '''bash
   docker run -d --name c1 react-app
   docker run -d --name c2 react-app
   '''
2. Shell into Container 1 and create a file:  
   '''bash
   docker exec -it c1 sh
   cd /app
   echo "hello from c1" > data.txt
   exit
   '''
3. Shell into Container 2 and look for 'data.txt':  
   '''bash
   docker exec -it c2 sh
   cd /app
   ls | grep data.txt
   # → no output: file doesn’t exist here
   exit
   '''



2. Observations

- The file created in Container 1 ('data.txt') is not visible in Container 2.  
- Each container’s writable layer is isolated, providing an “ephemeral filesystem.”  
- Removing a container also deletes its writable layer and any data stored there.



3. Why This Matters

- Containers are ideal for stateless workloads.  
- Persistent data should never live in the container’s filesystem; it’ll be lost on deletion.  
- Use Volumes (covered next) to store data outside the container’s ephemeral layer.




4. Isolation Diagram

'''
        Shared Image Layers (read-only)
       ┌─────────────────────────────┐
       │  /bin, /lib, /usr, /app/... │
       └─────────────────────────────┘
            ▲                 ▲
            │                 │
┌───────────┴───────────┐ ┌───┴────────────┐
│ Container “c1”        │ │ Container “c2” │
│ ┌───────────────────┐ │ │ ┌─────────────┐│
│ │ Writable Layer    │ │ │ │ Writable    ││
│ │ – data.txt exists │ │ │ │ Layer       ││
│ │ – any changes     │ │ │ │ – no data.tx││
│ └───────────────────┘ │ │ └─────────────┘│
└───────────────────────┘ └────────────────┘
'''

Each container’s writable layer is separate, ensuring true filesystem isolation.




Codes and Other Notes in this Discussion: 

docker exec -it 655 sh 

echo data > data.txt 


docker exec -it 6eb sh 

ls | grep data

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Persisting Data Using Volumes: 
Volumes are the preferred way to store and share persistent data outside a container’s writable layer. They live on the host (or remote driver), survive container restarts, and can be shared among multiple containers.



1. Volume Basics

- A volume is storage outside any single container  
- Can be:  
  - A directory on the Docker host (default)  
  - A remote/store driver (cloud storage, NFS, etc.)  
- Volumes persist beyond container lifecycles—deleting a container doesn’t delete its volumes



2. Volume Management Commands

'''bash
# List Docker volume subcommands
docker volume

# Create a new volume named "app-data"
docker volume create app-data

# List all volumes
docker volume ls

# Inspect a volume’s details (JSON output)
docker volume inspect app-data

# Remove unused volumes
docker volume prune

# Remove a specific volume (must be unused)
docker volume rm app-data
'''

Inspect output highlights:
- 'Driver': "local" (host directory) or custom driver  
- 'Mountpoint': path inside the Docker host or VM where data is stored  

> On macOS, Docker runs in a Linux VM—inspect paths exist inside that VM, not directly on macOS’s filesystem.



3. Mounting a Volume into a Container

Use the '-v' or '--volume' flag with 'docker run':

'''bash
docker run -d \
  -p 4000:3000 \
  -v app-data:/app/data \
  react-app
'''

- 'app-data': volume name  
- '/app/data': absolute path in the container’s filesystem  
- Docker auto-creates the volume and target directory if needed



4. Permission Pitfall & Solution

1. Issue: Docker mounts a fresh volume owned by 'root'. Your non-root 'app' user may have no write permission in '/app/data'.
2. Symptom:  
   '''sh
   echo "hello" > /app/data/data.txt
   # → Permission denied
   '''
3. Fix in Dockerfile:  
   '''dockerfile
   FROM node:14.16.0-alpine3.13
   # Create non-root user...
   WORKDIR /app

   # Ensure data directory exists and is owned by 'app'
   RUN mkdir data

   COPY package*.json ./
   RUN npm install
   COPY . .
   ENV API_URL=http://api.myapp.com/
   EXPOSE 3000
   CMD ["npm","start"]
   '''
   - 'RUN mkdir data' executes as the current user ('app'), so '/app/data' is writable.

4. Rebuild your image:  
   '''bash
   docker build -t react-app .
   '''



5. Verifying Persistence

1. Start container with volume mounted (uses updated image):
   '''bash
   docker run -d -p 5000:3000 -v app-data:/app/data --name c-data react-app
   '''
2. Write data inside:
   '''bash
   docker exec -it c-data sh
   cd /app/data
   echo "some data" > data.txt
   exit
   '''
3. Remove container (volume remains):
   '''bash
   docker rm -f c-data
   '''
4. Run a new container with same volume:
   '''bash
   docker run -d -p 5001:3000 -v app-data:/app/data --name c-data2 react-app
   docker exec -it c-data2 sh
   ls /app/data
   # → data.txt still present
   '''



6. Sharing a Volume Across Containers

Multiple containers can mount the same volume:

'''bash
docker run -d --name web1 -v app-data:/app/data react-app
docker run -d --name web2 -v app-data:/app/data react-app
'''

Both 'web1' and 'web2' read/write to the same persisted data in 'app-data'.



7. Volume vs. Bind Mounts Comparison

| Feature            | Named Volume ('-v name:path')      | Bind Mount ('-v hostDir:contDir')      |
|--------------------|-------------------------------------|----------------------------------------|
| Managed by Docker  | Yes                                 | No                                     |
| Host path visible  | No (managed in Docker’s area)       | Yes (any host directory)               |
| Backup/Restore     | 'docker volume' commands            | Host fs tools (tar, rsync)             |
| Permissions        | Root-owned by default (adjustable)  | Inherits host directory permissions    |



Volume Lifecycle Diagram

'''
     ┌───────────────┐
     │   Volume:     │
     │   app-data    │
     └─────┬─────────┘
           │
           ▼
  ┌───────────────────┐    mount   ┌─────────────┐
  │                   │  ─────────▶│             │
  │ Container “c1”    │            │  /app/data  │ (writable layer)
  │  /app/data  ←─────┤            │             │
  │                   │    mount   │             │
  └───────────────────┘  ─────────▶└─────────────┘

Deleting "c1" ──▶ Volume "app-data" remains intact
'''

Volumes decouple data persistence from container lifecycles, ensuring your application state survives upgrades, restarts, and container deletion.





Codes and Other Notes in this Discussion: 

docker volume 

docker volume create app-data 

docker volume inspect app-data 

docker run -d -p 4000:3000 -v app-data:/app/data react-app



Dockerfile:

FROM node:14.16.0-alpine3.13
RUN addgroup app && adduser -S -G app app 
USER app  
WORKDIR /app
RUN mkdir data 
COPY package*.json .
RUN npm install 
COPY . .
ENV API_URL=http://api.myapp.com/
EXPOSE 3000
CMD ["npm", "start"] 



docker run -d -p 5000:3000 -v app-data:/app/data react-app


docker exec -it 007 sh 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Copying Files between the Host and Containers:
Sometimes you need to transfer files into or out of a running container—for example, export a log for analysis or inject a secret at runtime. Docker’s 'docker cp' command handles this.



1. Prepare a File Inside the Container

1. Identify the container  
   '''bash
   docker ps
   '''
   Note the CONTAINER ID or NAME (e.g., 'c1').

2. Create a file in the container’s filesystem:  
   '''bash
   # Open a shell in container c1
   docker exec -it c1 sh

   # Inside the container, write to /app/log.txt
   cd /app
   echo "hello from container" > log.txt

   exit
   '''



2. Copy from Container → Host

Use 'docker cp' with '<container>:<path>' as the source, and a host path as the destination.

'''bash
# Copy /app/log.txt from container c1 into current host directory
docker cp c1:/app/log.txt .
'''

- Result:  
  './log.txt' now exists in your host’s working directory.



3. Copy from Host → Container

You can also push files from your host into a container:

1. Create a file on the host (e.g., 'secret.txt'):  
   '''bash
   echo "my secret" > secret.txt
   '''
2. Copy it into the container:  
   '''bash
   # Copy host’s secret.txt into /app/secret.txt inside c1
   docker cp secret.txt c1:/app/secret.txt
   '''
3. Verify inside the container:  
   '''bash
   docker exec -it c1 sh
   cd /app
   ls
   # → you’ll see secret.txt and log.txt
   exit
   '''

> Note: On Windows, replace 'echo' commands with your preferred editor or PowerShell commands to create files.




4. Workflow Diagram

'''
Host                                      Container c1
┌──────────────┐                          ┌───────────────┐
│ secret.txt   │                          │               │
│ log.txt      │                          │               │
└─────┬────────┘                          │               │
      │ docker cp host→container          │               │
      └───────────────>┌────────────────┐ │               │
                       │ /app/secret.txt│ │               │
                       │ /app/log.txt   │ │               │
                       └────────────────┘ │               │
                                     ▲    │               │
            docker cp container→host │    │               │
      ┌────────────┐<────────────────┘    │               │
      │ log.txt    │                      │               │
      └────────────┘                      └───────────────┘
'''



5. Key Commands

| Action                                    | Command                                            |
|-------------------------------------------|----------------------------------------------------|
| List running containers                   | 'docker ps'                                        |
| Open shell in container                   | 'docker exec -it <id|name> sh'                     |
| Copy file from container to host          | 'docker cp <id|name>:/path/to/file /host/dest'     |
| Copy file from host to container          | 'docker cp /host/srcfile <id|name>:/path/in/cont'  |

Use these commands whenever you need to inspect container state, extract artifacts, or inject configuration without rebuilding images.





Codes and Other Notes in this Discussion: 

docker cp e1c9043ea8ce:/app/log.txt . 

docker cp secret.txt e1c9043ea8ce:/app


--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Sharing the Source Code with a Container: 


Sharing Source Code with a Container (Bind Mounts):
During development, rebuilding your Docker image for every code change is too slow. Instead, you can bind mount your local project directory into the container so edits are reflected immediately.



1. The Problem

- Your React app runs at 'localhost:5000' inside a container.  
- You edit 'public/index.html' (e.g., change the '<title>'), save, then refresh—no change, because the container still uses the old files.  
- Rebuilding the image for each change or manually copying files ('docker cp') is inefficient.



2. Bind Mount Solution

Use the '-v' (volume) flag with a host path to map your code into the container:

'''bash
docker run -d \
  -p 5001:3000 \
  -v $(pwd):/app \
  --name react-dev \
  react-app
'''

- '-d'  
  Runs the container detached (in the background).  
- '-p 5001:3000'  
  Maps host port 5001 → container port 3000.  
- '-v $(pwd):/app'  
  Bind mounts your current directory into '/app' in the container.  
  - '$(pwd)' is shell‐evaluated to the full host path (e.g., '/home/user/project').  
  - Without '$( )', Docker would treat it as a named volume.  



3. How It Works

1. Shell expansion  
   - The shell replaces '$(pwd)' with the absolute path before Docker runs.  
2. Bind mount  
   - Docker mounts that host directory directly into the container’s '/app'.  
3. Live updates  
   - React’s development server inside the container watches '/app' for changes.  
   - On save, hot reloading pushes updates to the browser—no manual refresh required.



4. Bind Mount vs. Named Volume

| Feature                     | Named Volume ('-v name:/app/data') | Bind Mount ('-v /host/path:/app') |
|-----------------------------|-------------------------------------|-----------------------------------|
| Managed by Docker?          | Yes                                 | No                                |
| Path visible on host?       | No                                  | Yes                               |
| Ideal for code sharing      | No                                  | Yes                               |
| Ideal for persistent data   | Yes                                 | Depends                          |

In this scenario, a bind mount lets you share source code for rapid development.



5. Live Editing Demo

1. Start container with bind mount:  
   '''bash
   docker run -d -p 5001:3000 -v $(pwd):/app --name react-dev react-app
   '''
2. Edit 'public/index.html' title to “Dockerized React App!” in your host editor.  
3. Watch your browser at 'http://localhost:5001' update instantly via React’s hot reload.  




6. Visual Diagram

'''
   Host Machine                          Docker Container
┌─────────────────────────┐            ┌─────────────────────────┐
│ /home/user/project      │ ──bind──▶ │ /app (mounted)          │
│   public/index.html     │            │   public/index.html     │
│   src/                  │            │   src/                  │
└─────────────────────────┘            └─────────────────────────┘
       ^                                     │
       |  File edits                         │ React dev server
       └─────────────────────────────────────┘ hot-reloads browser
'''




7. Next Up: Docker Compose

Manually typing long 'docker run' commands is tedious. In the next section, you’ll learn Docker Compose to define these settings in a 'docker-compose.yml' and launch with 'docker-compose up'.



Codes and Other Notes in this Discussion: 

docker run -d -p 5001:3000 -v $(pwd):/app react-app
--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Summary:

Containers

Running containers:
docker run <image>
docker run -d <image> # run in the background
docker run —name <name> <image> # to give a custom name
docker run —p 3000:3000 <image> # to publish a port HOST:CONTAINER



Listing containers:
docker ps # to list running containers
docker ps -a # to list all containers



Viewing the logs:
docker logs <containerID>
docker logs -f <containerID> # to follow the log
docker logs —t <containerID> # to add timestamps
docker logs —n 10 <containerID> # to view the last 10 lines



Executing commands in running containers:
docker exec <containerID> <cmd>
docker exec -it <containerID> sh # to start a shell 




Starting and stopping containers:
docker stop <containerID>
docker start <containerID>





Removing containers:
docker container rm <containerID>
docker rm <containerID>
docker rm -f <containerID> # to force the removal
docker container prune # to remove stopped containers




Volumes:
docker volume ls
docker volume create app-data
docker volume inspect app-data
docker run -v app-data:/app/data <image>




Copying files between the host and containers:
docker cp <containerID>:/app/log.txt .
docker cp secret.txt <containerID>:/app



Sharing source code with containers:
docker run -v $(pwd):/app <image>


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Running Multi-Container Applications: 

Introduction: 

Welcome back to another section of the ultimate Docker course. In this section we're going to talk about running multi container applications. So I'm going to give you a real world application with three building blocks. A front end built with React, a back end built with Node and a MongoDB database. Once again, you don't need to be familiar or use any of these tools.

Application Overview:
Our sample app has three components:
Front End A React-based UI served by a Node.js web server.
Back End A Node.js API handling business logic and data access.
Database A MongoDB instance for persistent storage.


Our focus here is on Docker and not on development tools. I think this is the most exciting part of this course where you can see everything coming together. We'll talk about Docker Compose for building and running multi container applications. We'll also talk about Docker networking, database migration and running automated tests. So, let's jump in and get started.

Note: You don’t need prior React, Node, or MongoDB experience—our focus is Docker orchestration.



In this Section: 
Docker Compose: Define and run multi-container apps with a single YAML file.
Docker Networking: Enable secure, automatic communication between containers on custom networks.
Database Migration: Automate schema setup or updates for MongoDB when containers start.
Automated Testing: Spin up test environments, run integration tests, then tear down services effortlessly.




Multi-Container Architecture Diagram:

                        ┌───────────┐
 ┌───────────────┐      │           │      ┌───────────────┐
 │   React App   │◀────▶│  Node API │◀───▶ │   MongoDB     │
 │  (Service 1)  │      │(Service 2)│      │ (Service 3)   │
 └───────────────┘      └───────────┘      └───────────────┘
        │                    │                   │
        │   docker-compose   │   docker-compose  │
        └────────────────────────────────────────┘                   


All services are defined and linked via Docker Compose
Each runs in its own container but communicates over a private network




Why It Matters
Consistency: Dev, test, and prod environments use identical container configurations.
Simplicity: One command (docker-compose up) builds, starts, and connects every service.
Scalability: Easily add replicas or new services (e.g., Redis cache) to the same network.
Isolation: Each service runs in its own container, reducing dependency conflicts.


With this foundation, you’ll be ready to define complex, multi-service stacks and manage them as cohesive applications. Let’s dive into Docker Compose next!


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Installing Docker Compose:
Docker Compose is a CLI tool built on top of Docker Engine that simplifies the management of multi-container applications. Follow these steps to install or verify your Compose setup.


1. Locate the Official Docs

- Google “Docker Compose install” or visit:  
  https://docs.docker.com/compose/install/



2. macOS & Windows (Docker Desktop)

- Docker Desktop for Mac and Windows includes Docker Compose out of the box.  
- No additional install steps required.

To verify:

'''bash
docker compose version
# or, if using the v1 standalone binary:
docker-compose --version
'''

You should see output like:

'''
Docker Compose version v2.28.5
# or
docker-compose version 1.29.2, build 5becea4c
'''



3. Linux & Windows Server

If you’re running on Linux or Windows Server (non-Desktop), you must install Compose manually:

1. Follow platform-specific instructions on the Docker docs page.  
2. Choose either the plugin (recommended) or standalone binary.  
3. Grant executable permission and move to your '$PATH'.

Example for standalone Linux binary:

'''bash
sudo curl -L "https://github.com/docker/compose/releases/download/v2.28.5/docker-compose-$(uname -s)-$(uname -m)" \
  -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
'''



4. Verify Your Installation

Run:

'''bash
docker compose version
'''

or (for v1):

'''bash
docker-compose --version
'''

- Confirm you see v2. (or ≥ 1.29 for the standalone v1).  
- If outdated, upgrade Docker or download the latest Compose release.



5. Next Steps

With Compose installed, you’re ready to define services in a 'docker-compose.yml' and orchestrate your multi-container stack with:

'''bash
docker compose up
docker compose down
'''

In the next lesson, we’ll build our first Compose file and spin up a three-tier application in one command.



Codes and Other Notes in this Discussion: 

docker compose install --> https://docs.docker.com/compose/install/

docker-compose --version 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Cleaning Up our Workspace: 



Cleaning Up Your Docker Workspace:
As you experiment with Docker, unused containers, images, and volumes can clutter your environment. Below are two methods—CLI commands and the Docker Desktop GUI—to quickly clean up everything.



1. Removing All Containers and Images via CLI

# Why Containers First?
Images in use by containers cannot be removed. Always delete containers before removing images.

# Step-by-Step Commands

1. List all containers (including stopped):  
   '''bash
   docker container ls -aq
   '''
   - '-a' shows all containers  
   - '-q' returns only IDs  

2. Force-remove all containers:  
   '''bash
   docker container rm -f $(docker container ls -aq)
   '''
   - '-f' stops and removes running containers  

3. List all images:  
   '''bash
   docker image ls -q
   '''
   - '-q' returns only image IDs  

4. Remove all images:  
   '''bash
   docker image rm $(docker image ls -q)
   '''

# One-Liner Cleanup

Combine both steps in a single shell session:

'''bash
docker container rm -f $(docker container ls -aq)
docker image rm $(docker image ls -q)
'''

After running these, 'docker ps -a' and 'docker images' should show no entries.




2. Purging via Docker Desktop GUI

If you use Docker Desktop on macOS or Windows, you can wipe everything graphically:

1. Click the Docker icon in the menu bar (macOS) or notification tray (Windows).  
2. Select Preferences (or Settings).  
3. Go to the Troubleshoot (or Maintenance) tab.  
4. Click Clean / Purge Data.  
   - Removes all containers, images, volumes, and caches.  
   - Restarts the Docker engine—wait ~30 seconds for it to come back online.

> Caution: This action is irreversible. Only use it when you truly want a fresh Docker environment.





3. Visual Comparison

'''
╔══════════════════════╗   ╔══════════════════════════════╗
║   Command-Line CLI   ║   ║    Docker Desktop GUI        ║
╠══════════════════════╣   ╠══════════════════════════════╣
║ docker container rm… ║   ║ Docker icon → Preferences    ║
║ docker image rm…     ║   ║ Troubleshoot → Clean / Purge ║
╚══════════════════════╝   ╚══════════════════════════════╝
'''

With these techniques, you can swiftly reset your Docker workspace and avoid disk bloat from unintended leftovers.




Codes and Other Notes in this Discussion: 

docker container rm -f $(docker container ls -aq)

docker image rm $(docker image ls -q) 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
The Sample Web Application: 

Sample Multi-Container Web Application:
In this section, we explore a real-world, three-tier application—comprising a React front end, a Node.js back end, and a MongoDB database—deployed with a single Docker Compose command.



1. Project Structure

After downloading and extracting the provided ZIP, you’ll see:

'''
project-root/
├─ backend/            # Node.js API project (port 3001)
│  ├─ package.json
│  └─ index.js
│
├─ frontend/           # React app project (port 3000)
│  ├─ package.json
│  └─ src/
│     └─ ...
│
└─ docker-compose.yml  # Orchestrates all three services
'''



2. Manual Setup (Without Docker)

To run locally without containers, you’d need to:

1. Backend (Node API)  
   - 'cd backend'  
   - 'npm install'  
   - 'npm start'  (listens on port 3001)  

2. Frontend (React UI)  
   - In a new terminal: 'cd frontend'  
   - 'npm install'  
   - 'npm start'  (listens on port 3000)  

3. Database & Data  
   - Install MongoDB on your host machine  
   - Manually create and seed the 'movies' collection  
   - Open additional shells to run unit/integration tests  

This workflow requires four+ terminals and manual setup for dependencies, ports, and migrations.



3. One-Command Docker Compose Launch

With 'docker-compose.yml' in the project root, simply run:

'''bash
docker-compose up
'''

What happens:

- MongoDB image is pulled and started.  
- Backend container:  
  - Installs dependencies  
  - Runs database migration scripts to seed initial movie data  
  - Exposes port 3001  
  - Runs automated API tests.  
- Frontend container:  
  - Installs React dependencies  
  - Starts development server on port 3000  
  - Runs front-end tests.  
- All logs (build steps, migrations, test results, server startup) stream to your console.

Once complete, your browser can open:

'''
http://localhost:3000
'''

to view a movie management UI with pre-populated data. You can add or delete movies; backend API and MongoDB work seamlessly behind the scenes.



4. Architecture Diagram

'''
┌──────────────────┐     ┌────────────────────┐     ┌────────────────┐
│   React Front    │◀───▶│   Node.js Back     │◀───▶│   MongoDB DB   │
│   (Service:3000) │     │   (Service:3001)   │     │   (Service:27017) │
└──────────────────┘     └────────────────────┘     └────────────────┘
                ▲                ▲                     ▲
                │                │                     │
            docker-compose.yml orchestrates network & volumes
'''

- All services run on a private Docker network.  
- Ports are published to the host for local access.  
- Volumes or Init scripts handle data seeding (migrations).



5. Key Benefits

- Simplicity: One command builds, wires, and runs all services.  
- Consistency: Same environment across team members and CI/CD.  
- Automation: Migrations and tests run automatically on startup.  
- Isolation: Each component lives in its own container—no global installs.



6. Next Steps

Before inspecting the Compose file, ensure you understand YAML syntax. The next lesson covers JSON vs. YAML so you can read 'docker-compose.yml' with confidence.




Codes and Other Notes in this Discussion: 

docker-compose up 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

JSON and YAML Formats: 



JSON vs. YAML Formats:
Below is an overview of JSON and YAML, how to write data in each format, and when to use them.



JSON Format

- JSON is a human‐readable data‐exchange language using  
  - Curly braces '{ }' for objects  
  - Square brackets '[ ]' for arrays  
  - Double quotes '"' around keys and string values  
  - Commas ',' between items  

Example file 'data.json':

'''json
{
  "name": "The Ultimate Docker Course",
  "price": 149,
  "is_published": true,
  "tags": ["software", "devops"],
  "author": {
    "first_name": "Mosh",
    "last_name": "Hamedani"
  }
}
'''

- Key/value pairs:  
  - Strings must be in double quotes  
  - Numbers and booleans unquoted  
- Arrays: comma-separated values in '[ ]'  
- Nesting: objects inside objects via '{ }'





YAML Format

- YAML emphasizes minimal syntax using  
  - Indentation (spaces) for hierarchy  
  - Hyphens '-' for list items  
  - No quotes required for simple strings  
  - No commas between entries  

Example file 'data.yml':

'''yaml

name: The Ultimate Docker Course
price: 149
is_published: true
tags:
  - software
  - devops
author:
  first_name: Mosh
  last_name: Hamedani
'''

- Start with '---' to mark the document beginning  
- Lists: each item prefixed with '-' at the same indent level  
- Maps/objects: key followed by ':' then indented children



Key Differences

| Feature              | JSON                     | YAML                        |
|----------------------|--------------------------|-----------------------------|
| Syntax clutter       | Braces, brackets, commas | Indentation, hyphens        |
| String quoting       | Required ('"')           | Optional                    |
| Hierarchy            | '{ }', '[ ]'             | Indentation only            |
| Parsing performance  | Faster (explicit types)  | Slightly slower (type inference) |



When to Use

- JSON  
  - Exchanging data between client & server  
  - APIs, microservices communication  
- YAML  
  - Configuration files ('docker-compose.yml', CI/CD)  
  - Human editing—easier to read and write



Visual Comparison

'''
JSON                  YAML
─────                 ─────
{                     ---
  "tags": [           tags:
    "a",              - a
    "b"               - b
  ]                   author:
}                       first: John
                        last: Doe
'''

Choose the format that fits your use case: JSON for strict data interchange, YAML for readable configuration.




Codes and Other Notes in this Discussion: 

data.json: 

{
	"name": "The Ultimate Docker Course", 
	"price": 149, 
	"is_published": true, 
	"tags": ["software", "devops"],
	"author": {
		"first_name": "Mosh",
		"last_name": "Hamedani"
	}
}



data.yaml: data.yml 

name: The Ultimate Docker Course
price: 149
is_published: true
tags: 
  - software 
  - devops
author: 
  first_name: Mosh
  last_name: Hamedani
  
  
--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Creating a Compose File: 

Creating a Docker Compose File:
A Docker Compose file ('docker-compose.yml') lets you define and run multiple containers as services with one command. Below is a step-by-step guide to building a Compose file for a three-tier app (Front End, API, Database).



1. File Naming & Location

- Filename: 'docker-compose.yml' (all lowercase, hyphens)  
- Default path: Project root  
- Docker Compose will auto-detect this file when you run 'docker compose up'.



2. Specify the Compose File Version

At the very top, declare the format version. Use a string—Compose expects it as text, not a number.

'''yaml
version: "3.8"
'''

- Why 3.8?  
  - Latest format with broad Docker Engine compatibility  
  - Access to modern features (networks, secrets, healthchecks)



3. Define Your Services

Under the 'services:' key, list each application component by name. These names become hostnames on the Compose network.

'''yaml
services:
  web:       # Front End
    build: ./frontend
    ports:
      - "3000:3000"

  api:       # Node.js Back End
    build: ./backend
    ports:
      - "3001:3001"
    environment:
      DB_URL: "mongodb://db/vidly"

  db:        # MongoDB
    image: mongo:4.0-xenial
    ports:
      - "27017:27017"
    volumes:
      - vidly:/data/db
'''

3.1 web (Front End)

- 'build: ./frontend'  
  Context folder containing a 'Dockerfile' for the React app.  
- 'ports:'  
  - '"3000:3000"' maps host port 3000 → container port 3000.

3.2 api (Back End)

- 'build: ./backend'  
  Context folder containing a 'Dockerfile' for the Node API.  
- 'ports:'  
  - '"3001:3001"' maps host port 3001 → container port 3001.  
- 'environment:'  
  - 'DB_URL': Connection string to MongoDB—  
    'mongodb://db/vidly'  
    - 'db': hostname equals the service name  
    - 'vidly': database name  

3.3 db (Database)

- 'image: mongo:4.0-xenial'  
  Pulls the official MongoDB image version 4.0 on Ubuntu Xenial.  
- 'ports:'  
  - '"27017:27017"' exposes MongoDB default port for external clients.  
- 'volumes:'  
  - 'vidly:/data/db' mounts a named volume 'vidly' into '/data/db' to persist data.



4. Declare Named Volumes

At the bottom, define any named volumes used by your services:

'''yaml
volumes:
  vidly: {}
'''

- An empty object ('{}') tells Compose to create and manage 'vidly' on the host.



5. Full Example: 'docker-compose.yml'

'''yaml
version: "3.8"

services:
  web:
    build: ./frontend
    ports:
      - "3000:3000"

  api:
    build: ./backend
    ports:
      - "3001:3001"
    environment:
      DB_URL: "mongodb://db/vidly"

  db:
    image: mongo:4.0-xenial
    ports:
      - "27017:27017"
    volumes:
      - vidly:/data/db

volumes:
  vidly: {}
'''



6. How It Works

1. 'docker compose up':  
   - Builds 'frontend' and 'backend' images from their Dockerfiles.  
   - Pulls the 'mongo:4.0-xenial' image.  
   - Creates an isolated network where services resolve each other by name ('db', 'api', 'web').  
   - Sets up port mappings and mounts the 'vidly' volume.  
2. Service Startup:  
   - 'db' (MongoDB) starts first.  
   - 'api' connects to 'db' using 'DB_URL'.  
   - 'web' connects to 'api' at 'http://api:3001'.  



7. Next Steps

With this Compose file in place, bring up your full stack in one go:

'''bash
docker compose up
'''

You’ll have a React UI at 'http://localhost:3000', a Node API at 'http://localhost:3001', and a persistent MongoDB at 'mongodb://localhost:27017/vidly'.




Codes and Other Notes in this Discussion:   

docker-compose.yml 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 3000:3000
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db

volumes:
  vidly: 

	  
For references: https://docs.docker.com/reference/compose-file/


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Building Images: 

Building Images with Docker Compose:
Docker Compose extends the Docker CLI to manage multi-service applications. All familiar Docker commands—build, push, pull, run, rm—are available under the 'docker compose' umbrella, but they apply across all services defined in your 'docker-compose.yml'.



1. Exploring Compose Subcommands

Run 'docker compose' without arguments to see the full list:

'''bash
docker compose
'''

Common subcommands include:

- 'build'    Build (or rebuild) service images  
- 'up'       Create and start containers  
- 'down'     Stop and remove containers, networks, volumes  
- 'run'      Run a one-off command in a service container  
- 'push'     Push service images to a registry  
- 'pull'     Pull service images from a registry  
- 'rm'       Remove stopped service containers  



2. The 'build' Command

Use 'docker compose build' to build images for all build-configured services:

'''bash
docker compose build
'''

Helpful Options:
- '--no-cache'  
  Instructs Compose to ignore cached layers and rebuild from scratch. Useful if you suspect stale layers.  
- '--pull'  
  Forces pulling newer base images before building, ensuring you start from the latest upstream.

View all options:
'''bash
docker compose build --help
'''



3. Interpreting the Build Output

After 'docker compose build', run:

'''bash
docker images
'''

Sample output:

| REPOSITORY             | TAG       | IMAGE ID   | CREATED       | SIZE    |
|------------------------|-----------|------------|---------------|---------|
| vidly_web          | latest    | abc1234    | 2 hours ago   | 120MB   |
| vidly_api          | latest    | def5678    | 2 hours ago   | 130MB   |
| vidly_frontend     | latest    | ghi9012    | 1 day ago     | 115MB   |
| vidly_backend      | latest    | jkl3456    | 1 day ago     | 125MB   |
| mongo              | 4.0-xenial| mno7890    | 7 days ago    | 450MB   |

- Prefix: Images are named '<directory>_<service>' by default (e.g., 'vidly_web')  
- Cache Hits: If files and Dockerfile instructions didn’t change, cached layers are reused, making rebuilds very fast  



4. Cache Behavior & Forcing a Full Rebuild

By default, Compose caches each build step:

1. Docker checks each Dockerfile instruction  
2. If unchanged, it reuses the layer from cache  
3. Subsequent services or layers skip rebuild likewise  

To invalidate the cache and ensure all layers rebuild:

'''bash
docker compose build --no-cache
'''

You’ll see longer build times and fresh timestamps in 'docker images':

'''
vidly_web      latest   uvw1122   Less than a minute ago    120MB
vidly_api      latest   xyz3344   Less than a minute ago    130MB
'''



5. What’s Happening Under the Hood

'''text
Docker Compose Build Flow:
────────────────────────────────────────
services:
  web   ──┐
  api   ──┼──▶ docker compose build ──▶ builds each Dockerfile
frontend ──┘
backend ──┐
db      ──┘ (pulled, not built)
────────────────────────────────────────
'''

- build entries in 'docker-compose.yml' trigger local image builds  
- image entries trigger pulls from Docker Hub or other registries  
- The Compose CLI sequences builds, preserving service dependency order



6. Key Takeaways

- Compose build replaces multiple 'docker build' calls with one command.  
- Naming convention: Compose prefixes images with the project directory name.  
- Cache reuse speeds up iterative development—use '--no-cache' to reset.  
- Versioning: Always check your Compose and Docker Engine compatibility (e.g., file format v3.8).



Armed with these commands, you can efficiently build, rebuild, and manage images for your entire multi-service stack.




Codes and Other Notes in this Discussion: 

docker-compose 

docker-compose build --help 

docker-compose build 

docker-compose build --no-cache 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Starting and Stopping the Application: 



Starting and Stopping the Application with Docker Compose:
Use Docker Compose to build, start, and stop your multi-container application with a single command sequence.



1. Starting the Application

1.1 Basic Command

- 'docker-compose up'  
  - If images exist, Compose starts containers.  
  - If images missing, Compose builds them first, then starts.  

1.2 Useful Options

- '--build'  
  Force a rebuild of all service images before starting.  
- '-d' (detached)  
  Run containers in the background, freeing your terminal.

1.3 Example

'''bash
# Rebuild images and start containers in detached mode
docker-compose up --build -d
'''



2. Viewing Running Services

2.1 Compose-Scoped List

'''bash
docker-compose ps
'''

Shows containers defined in your compose file, with columns for:

- Name: '<project>_<service>_<index>' (e.g., 'vidly_api_1')  
- Command: the process run inside each container ('npm start', 'mongod')  
- State: “Up X seconds”  
- Ports: host ↔ container mappings (e.g., '0.0.0.0:3000->3000/tcp')

2.2 Global List

'''bash
docker ps
'''

Lists all running containers on the Docker host, regardless of project.



3. Container Scaling Insight

- The suffix '_1', '_2', etc., indicates multiple replicas of the same service.  
- Scaling services for high availability uses the same Compose file with 'docker-compose up --scale <service>=<count>' (covered later).



4. Accessing the Application

- Once up, your React front end is available at:
  '''
  http://localhost:3000
  '''
- API listens on 'localhost:3001', MongoDB on 'localhost:27017'.



5. Stopping and Tearing Down

5.1 Stop & Remove Containers

'''bash
docker-compose down
'''

- Stops all containers for this project.  
- Removes containers and networks.  
- Does not remove images or named volumes by default.

5.2 Fast Teardown Workflow

'''bash
docker-compose down
docker-compose ps      # Should show no containers for this project
docker ps              # Other unrelated containers remain running
'''



6. Workflow Diagram

'''
docker-compose up --build -d
      │
      ├─ Builds images for 'web', 'api' services
      ├─ Pulls 'db' image from registry
      └─ Starts three containers:

    ┌──────────┐   ┌──────────┐   ┌──────────┐
    │ vidly_web│   │ vidly_api│   │ vidly_db │
    └──────────┘   └──────────┘   └──────────┘
       3000↔3000      3001↔3001      27017↔27017

docker-compose down
      │
      └─ Stops & removes *these* containers and networks
'''

With 'up' and 'down', Docker Compose manages your entire application lifecycle in a consistent, reproducible way.




Codes and Other Notes in this Discussion: 

docker-compose up --build 

docker-compose up -d 

docker-compose ps 

docker ps 

docker-compose down 



--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Docker Networking:



Docker Networking
Docker Compose automatically provisions an isolated network for your application’s containers, enabling seamless inter-service communication using DNS-based service discovery.



1. Default Docker Networks

On any Docker host, you’ll see three predefined networks:

'''bash
docker network ls
'''
| NETWORK ID | NAME    | DRIVER | SCOPE |
|------------|---------|--------|-------|
| xxxx       | bridge  | bridge | local |
| yyyy       | host    | host   | local |
| zzzz       | none    | null   | local |

- bridge: Default user-defined network  
- host: Containers share the host’s network namespace  
- none: Containers have no networking



2. Compose Project Network

Running 'docker compose up' creates a project-specific network named '<directory>_default':

'''bash
docker compose up -d
# Output: Creating network “vidly_default” with the default driver
'''

Check it:

'''bash
docker network ls | grep vidly_default
'''

| NAME             | DRIVER | SCOPE |
|------------------|--------|-------|
| vidly_default    | bridge | local |

All services (containers) defined in your 'docker-compose.yml' join this network by default.



3. Container-to-Container Communication

Containers on the same network resolve each other by service name. Internally, Docker runs an embedded DNS server that maps service names → IP addresses.

# Ping Demo

1. List running containers  
   '''bash
   docker compose ps
   #   Name            Command       State   Ports
   #   vidly_web_1     "npm start"   Up …    0.0.0.0:3000->3000/tcp
   #   vidly_api_1     "npm start"   Up …    0.0.0.0:3001->3001/tcp
   #   vidly_db_1      "mongod"      Up …    0.0.0.0:27017->27017/tcp
   '''
2. Shell into “web” as root (needed because the default non‐root user can’t run 'ping'):  
   '''bash
   docker exec -it -u root vidly_web_1 sh
   '''
3. Ping the API container by name:  
   '''sh
   ping api    # or ping vidly_api_1
   PING api (172.21.0.3): 56 data bytes
   64 bytes from 172.21.0.3: seq=0 ttl=64 time=0.085 ms
   '''
4. Exit shell:  
   '''sh
   exit
   '''



4. Inspect Container IP Address

Inside a container, list its network interfaces to see its IP on the bridge:

'''bash
docker exec -it -u root vidly_web_1 sh
# Inside container:
ifconfig
#   eth0: inet 172.21.0.2  netmask 255.255.0.0  broadcast 172.21.255.255
exit
'''

- 172.21.0.2 is the container’s IP on 'vidly_default'.



5. Service Discovery via Environment Variables

In 'docker-compose.yml', using:

'''yaml
services:
  api:
    environment:
      DB_URL: "mongodb://db/vidly"
'''

- 'db' refers to the MongoDB service by name.  
- The API container’s DNS resolver maps 'db' → IP of 'vidly_db_1'.  
- No manual IP configuration required.



6. External Access: Port Publishing

To reach services from the host (or other clients):

'''yaml
services:
  db:
    ports:
      - "27017:27017"
'''

- Maps host port 27017 → container port 27017  
- Allows external tools (e.g., MongoDB Compass) to connect to 'localhost:27017'.

Verify with MongoDB Compass:  
- Hostname: 'localhost'  
- Port: '27017'  
- Auth: none (default)  
- See databases (e.g., 'vidly') and its collections.



7. Network Architecture Diagram

'''
           ┌───────────────────────────┐
           │       vidly_default       │ (bridge network)
           └────────────┬──────────────┘
                        │
      ┌───────────┐     │    ┌───────────┐          ┌───────────┐
      │ vidly_web │◀───DNS──▶│ vidly_api │◀──DNS──▶ │ vidly_db  │
      │ 172.21.0.2│          │172.21.0.3 │          │172.21.0.4│
      └───────────┘          └───────────┘          └───────────┘
           │                      │                     │
           │Port 3000             │Port 3001            │Port 27017
           ▼                      ▼                     ▼
       localhost:3000         localhost:3001        localhost:27017
'''

- DNS inside the bridge network resolves service names to container IPs.  
- Port mappings expose chosen ports on the Docker host.



Takeaway: Docker’s networking model and embedded DNS let containers locate and communicate with each other by name, while port publishing exposes services to external clients.




Codes and Other Notes in this Discussion: 

docker-compose up -d 

docker network ls 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Viewing Logs: 

Viewing Logs in a Multi-Container Setup:
When running an application with Docker Compose, you have two main ways to inspect logs across all services or per container. Below are the commands, options, and techniques to get real-time visibility into your app.



1. Aggregate Logs with Docker Compose

'''bash
docker-compose logs
'''
- Streams logs from all containers defined in 'docker-compose.yml'  
- Color-coded by service name for easier reading  

Common Options:  
'''bash
docker-compose logs --help
'''
- '-f', '--follow'  
  Continuously stream new log entries (press 'Ctrl+C' to exit)  
- '-t', '--timestamps'  
  Prefix each line with its timestamp  
- '--tail="N"'  
  Show only the last *N* lines per service  

Examples:
- Follow logs in real time  
  '''bash
  docker-compose logs -f
  '''
- Show timestamps and tail last 10 lines  
  '''bash
  docker-compose logs -t --tail=10
  '''



2. Per-Container Logs with Docker CLI

If you prefer separate terminals or monitors for each service, use the 'docker logs' command on an individual container.

1. Identify the container  
   '''bash
   docker ps
   '''
   Find the CONTAINER ID or NAME for your target service (e.g., 'vidly_web_1').

2. View non-streaming logs  
   '''bash
   docker logs <container-id>
   '''
3. Follow logs in real time  
   '''bash
   docker logs -f <container-id>
   '''
4. Add timestamps  
   '''bash
   docker logs -t <container-id>
   '''
5. Tail only last N lines  
   '''bash
   docker logs --tail 5 <container-id>
   '''



3. Workflow Diagram

'''text
                 ┌───────────────┐
                 │ docker-compose│
                 │    logs       │
                 └──────┬────────┘
                        │
          ┌─────────────┼──────────────────┐
          │             │                  │
    ┌─────▼─────┐ ┌─────▼────┐      ┌──────▼─────┐
    │  web_1    │ │  api_1   │  …   │  db_1      │
    └───────────┘ └──────────┘      └────────────┘
'''
- Aggregate: One command shows logs from *all* services simultaneously.

'''text
    ┌───────────────────┐      ┌───────────────┐
    │ docker logs -f    │─────▶│  web_1        │
    │   <web-container> │      └───────────────┘
    └───────────────────┘
'''
- Individual: Pinpoint one container’s logs in a separate window.



4. Tips & Best Practices

- Use aggregate logs during development to get an overview of all services.  
- Switch to per-container logs when debugging a specific service.  
- Combine '-f', '-t', and '--tail' to narrow down noisy logs and focus on recent events.  
- Long-running services often produce verbose output—tailing the last *N* lines avoids scrolling through old entries.  

With these commands, you can efficiently monitor, debug, and trace issues in your multi-container applications.




Codes and Other Notes in this Discussion: 

docker-compose logs 

docker-compose logs --help 

docker logs 8c6 -f 



--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Publishing Changes: 

Publishing Source Code Changes in Development:
In development, you don’t want to rebuild Docker images on every code edit. Instead, use bind mounts in your 'docker-compose.yml' so that your local source directories are mounted directly into the containers. Changes you save locally appear instantly inside the container.



1. Modify 'docker-compose.yml' to Add Bind Mounts

'''yaml
version: "3.8"

services:
  web:
    build: ./frontend
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app           # Bind-mount frontend source

  api:
    build: ./backend
    ports:
      - "3001:3001"
    environment:
      DB_URL: "mongodb://db/vidly"
    volumes:
      - ./backend:/app            # Bind-mount backend source

  db:
    image: mongo:4.0-xenial
    ports:
      - "27017:27017"
    volumes:
      - vidly:/data/db

volumes:
  vidly: {}
'''

- './frontend:/app'  
  Mounts your host’s 'frontend' folder over '/app' in the 'web' container.  
- './backend:/app'  
  Mounts your host’s 'backend' folder over '/app' in the 'api' container.



2. The Unexpected “nodemon: not found” Error

When you start with 'docker-compose up', you may see:

'''
sh: nodemon: not found
'''

# Why It Happens

1. Image build previously installed 'node_modules' inside the container’s '/app'.  
2. Bind mount now overlays your host’s './backend' folder onto '/app'.  
3. Your host folder does not contain 'node_modules', so the container filesystem no longer has them—hence 'nodemon' (a devDependency) is missing.



3. Solution: Install Dependencies Locally

Before starting the containers:

'''bash
# Stop the compose run if it’s still running
# (Press Ctrl+C in the compose terminal)

cd backend
npm install          # or 'npm i'
cd ../frontend
npm install          # to set up frontend dependencies if you bind-mount it too
'''

Now you have a 'node_modules' folder in each directory. Rerun:

'''bash
docker-compose up
'''

- The containers see your local 'node_modules' and can start successfully.
- Nodemon (from 'devDependencies') runs and watches for file changes.



4. Live-Reloading Your Code

With 'nodemon' in the API container and React’s hot-reload in the web container, any saved code edit automatically:

1. Triggers a restart or rebuild inside the container  
2. Updates the running server or browser view  

Demo:  
- Edit 'backend/routes/index.js' (change the welcome message).  
- Save the file.  
- In the compose terminal, observe 'nodemon' detecting the change:  
  '''
  [nodemon] restarting due to changes...
  '''
- Refresh 'http://localhost:3001/' to see your updated output instantly.



5. Behind the Scenes: Bind Mount Overlay

'''
   Host directory: ./backend           Container /app
   ┌──────────────────────┐            ┌──────────────────┐
   │ backend/             │            │ /app             │
   │ ├─ index.js          │            │ ├─ index.js      │ ← Live file edits
   │ ├─ package.json      │ ▶ bind ────│ ├─ package.json  │
   │ └─ node_modules/     │            │ └─ node_modules/ │ ← Installed locally
   └──────────────────────┘            └──────────────────┘
'''

- A bind mount completely masks the container’s '/app' content with your host folder.  
- Ensure all required files ('node_modules', dev tools) live in the host directory when binding.



6. Front End Live-Mount (Exercise)

Repeat the same steps for the 'frontend' service:

1. Bind mount './frontend:/app' (already configured).  
2. Run 'npm install' inside './frontend' locally.  
3. 'docker-compose up' to start with hot reload.  
4. Edit React source and watch the browser update live via Create React App’s hot reload.



Takeaway:  
Use bind mounts in Compose for rapid development, but remember that mounts replace container directories. Install dependencies locally (or adjust mounts) so that containers retain required files like 'node_modules'.




Codes and Other Notes in this Discussion: 

docker-compose.yml 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 3000:3000
	volumes: 
	  - ./frontend:/app
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	volumes: 
	  - ./backend:/app
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db

volumes:
  vidly: 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Migrating the Database: 

Database Migration in Docker Compose:
Automating database migrations ensures your MongoDB is in the correct state (schema and seed data) every time you bring up your application. Below is a detailed guide to integrating migrate-mongo and a wait-for-it script into your Docker Compose workflow.



1. Migrations Tool & Scripts

- Tool: 'migrate-mongo' (installed as a devDependency in 'backend/package.json')  
- Scripts location: 'backend/migrations/'  
- Each migration file  
  - Exports two functions:  
    - 'up(db)' — applies changes (e.g., inserts documents)  
    - 'down(db)' — reverts changes (e.g., removes documents)  
  - filename prefixed with a timestamp, e.g., '20230708120000-populate-movies.js'



2. Change Log & Idempotence

- 'migrate-mongo up' runs all pending migrations.  
- Already-applied scripts are recorded in the 'migrations.changelog' collection.  
- Re-running 'migrate-mongo up' skips scripts that have already been applied.



3. npm Script Alias

In 'backend/package.json':

'''jsonc
"scripts": {
  "db:up": "migrate-mongo up",
  "start": "node index.js"
}
'''

- Run migrations via 'npm run db:up'.



4. Integrating Migrations into Docker Compose

4.1 Race Condition: Waiting for MongoDB

- The DB container may be running before Mongo is ready to accept connections.  
- Use a wait-for-it tool to block until port '27017' is listening.

Options: 'wait-for-it.sh', 'dockerize', 'wait-for', etc.  
We’ll use 'wait-for-it.sh' (github.com/vishnubob/wait-for-it).

4.2 Entry Point Script

Create 'backend/docker-entrypoint.sh':

'''sh
#!/bin/sh
echo "Waiting for MongoDB to start..."
./wait-for-it.sh db:27017

echo "Migrating the database..."
npm run db:up

echo "Starting the server..."
npm start
'''

- Place 'wait-for-it.sh' alongside this script in 'backend/'.  
- Make both executable:  
  '''bash
  chmod +x backend/wait-for-it.sh backend/docker-entrypoint.sh
  '''

4.3 Update 'docker-compose.yml'

Override the API service’s command to use the entry point:

'''yaml
version: "3.8"
services:
  api:
    build: ./backend
    ports:
      - "3001:3001"
    environment:
      DB_URL: "mongodb://db/vidly"
    volumes:
      - ./backend:/app
    command: ./docker-entrypoint.sh

  db:
    image: mongo:4.0-xenial
    ports:
      - "27017:27017"
    volumes:
      - vidly:/data/db

  web:
    build: ./frontend
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app

volumes:
  vidly: {}
'''

- 'command:' replaces the 'CMD' in the Dockerfile with your entry point script.



5. Migration Flow Diagram

'''
docker compose up
      │
      ├─ db container starts (MongoDB binds 27017)
      │
      ├─ api container runs entrypoint:
      │     ├─ wait-for-it.sh db:27017
      │     ├─ npm run db:up      ← seed ‘movies’ collection
      │     └─ npm start          ← starts Node API on port 3001
      │
      └─ web container builds/runs React app on port 3000
'''



6. Verifying Migration

1. Tear down existing containers & network (volumes not removed):
   '''bash
   docker-compose down
   '''
2. List volumes:
   '''bash
   docker volume ls
   # e.g., vidly_vidly
   '''
3. Remove the MongoDB volume to reset data:
   '''bash
   docker volume rm vidly_vidly
   '''
4. Bring up the stack again:
   '''bash
   docker-compose up
   '''
5. Check the API endpoint:
   '''
   http://localhost:3001/api/movies
   '''
   The pre-seeded “movies” should appear.  
6. Open the front end:
   '''
   http://localhost:3000
   '''



7. Commands Summary

| Purpose                                     | Command                                                           |
|---------------------------------------------|-------------------------------------------------------------------|
| Start all services with migration & build   | 'docker-compose up'                                               |
| Stop & remove containers & network          | 'docker-compose down'                                             |
| Remove MongoDB volume (reset data)          | 'docker volume rm vidly_vidly'                                    |
| Run migrations interactively (outside Docker)| 'cd backend && npm run db:up'                                     |
| Make entry scripts executable               | 'chmod +x backend/*.sh'                                           |

By embedding migration and readiness checks into your Compose workflow, you ensure a reliable, repeatable startup process that always seeds your database before your API serves requests.




Codes and Other Notes in this Discussion: 

docker-compose.yml 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 3000:3000
	volumes: 
	  - ./frontend:/app	
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	volumes: 
	  - ./backend:/app
	command: ./wait-for db:27017 migrate-mongo up  && npm start 
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db

volumes:
  vidly: 
  
  
  
docker wait for container --> https://docs.docker.com/reference/cli/docker/container/wait/



docker-entrypoint.sh:

#!/bin/sh

echo "Waiting for MongoDB to start..."
./wait-for db:27017

echo "Migrating the database..."
npm run db:up

echo "starting the server..."
npm start 




docker-compose.yml: 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 3000:3000
	volumes: 
	  - ./frontend:/app 
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	volumes: 
	  - ./backend:/app
	command: ./docker-entrypoint.sh
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db

volumes:
  vidly: 
  
  

docker volume rm vidly_vidly 

docker compose up 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Running Tests:

Running Automated Tests:
Automate your front-end and back-end test suites within the same Docker Compose setup. Below are two approaches: manual via local 'npm test', and containerized via dedicated Compose services.



1. Manual Test Execution

Run tests quickly outside Docker to get rapid feedback:

1. Front End  
   '''bash
   cd frontend
   npm test
   '''
   - Runs Create React App’s test runner.  
   - Nine passing tests, then press q to exit.

2. Back End  
   '''bash
   cd backend
   npm test
   '''
   - Executes your Node.js test suite (e.g., Mocha, Jest).  
   - Press q (or Ctrl+C) to exit.

Pros: Fast feedback, minimal overhead  
Cons: Requires local Node/npm setup



2. Containerized Tests with Docker Compose

If you prefer encapsulating tests in containers—maintaining environment parity—add two test services to your Compose file.

# 2.1 Update 'docker-compose.yml'

'''yaml
version: "3.8"

services:
  web:
    build: ./frontend
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app

  api:
    build: ./backend
    ports:
      - "3001:3001"
    environment:
      DB_URL: "mongodb://db/vidly"
    volumes:
      - ./backend:/app
    command: ./docker-entrypoint.sh

  db:
    image: mongo:4.0-xenial
    ports:
      - "27017:27017"
    volumes:
      - vidly:/data/db

  # --------------------------------
  web-tests:
    image: vidly_web         # reuse the built frontend image
    volumes:
      - ./frontend:/app
    command: npm test        # run front-end tests
    depends_on:
      - db                   # ensure Mongo exists, if needed

  api-tests:
    image: vidly_api         # reuse the built backend image
    volumes:
      - ./backend:/app
    command: npm test        # run back-end tests
    environment:
      DB_URL: "mongodb://db/vidly"
    depends_on:
      - db                   # ensure DB is ready

volumes:
  vidly: {}
'''

- 'image:' points to your project images ('<directory>_<service>').  
- 'volumes:' bind-mount source so tests see your code & dependencies.  
- 'command:' overrides the container’s default to run 'npm test'.  
- 'depends_on:' guarantees test services start after the database container (optional, but often needed).



3. Running Tests in Containers

'''bash
docker-compose up web-tests api-tests
'''

- Logs stream both test suites in one terminal.  
- Automatic re-run on file changes is possible but slower than manual—each change restarts the container.

# ASCII Workflow

'''
docker-compose up
├─ web         (React dev server)
│
├─ api         (Node API server)
│
├─ db          (MongoDB)
│
├─ web-tests   (npm test in web image)
└─ api-tests   (npm test in api image)
'''



4. Performance Considerations

- Manual ('npm test' locally):  
  - Fast startup, immediate results.  
  - Ideal during development.

- Containerized ('docker-compose up'):  
  - Ensures consistency with CI/CD environments.  
  - Adds container startup overhead—tests may feel slower.



5. Choosing Your Workflow

- Local tests for rapid TDD:  
  Keep your code editor and test window side by side.

- Compose tests for CI parity:  
  Use the same Docker environment as production.

With both methods available, you can balance speed and consistency as you develop and validate your multi-service application.






Codes and Other Notes in this Discussion: 


docker-compose.yml: 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 3000:3000
	volumes: 
	  - ./frontend:/app 
  web-tests: 
    image: vidly_web 
	volumes: 
	  - ./frontend:/app 
	command: npm test 
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	volumes: 
	  - ./backend:/app
	command: ./docker-entrypoint.sh
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db

volumes:
  vidly: 
  
  

docker-compose up 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------



Summary:

Multi-containers apps

Docker Compose commands

docker-compose build
docker-compose build --no-cache
docker-compose up
docker-compose up -d
docker-compose up —build
docker-compose down
docker-compose ps
docker-compose logs


--------------------------------------------------------------------------------------------------------------------------------------------------------------------



Deploying Applications: 

Welcome back to the last section of the ultimate Docker course. In this section, you're going to learn how to deploy your dockerized applications. So we're going to take the same application we have been working with and put it in the cloud. We'll be talking about various deployment options, getting a virtual private server or VPS, using Docker machine to provision hosts and connect with them, creating optimized production images, and of course, deploying the application and its updates. I think this is the most exciting part of this course because by the end of this section, our application will be live on the Internet with its front end, back end, and database.

So let's jump in and get started.


Introduction: 
Deployment Options
Getting a Virtual Private Server (VPS)
Using Docker Machine 
Creating optimized production images 
Deploying the application 


in detail:

Section: Deploying Applications
In this final section, we’ll take the same three-tier Dockerized app (React front end, Node API back end, MongoDB) and launch it to the cloud. You’ll learn how to:

- Evaluate and choose deployment options  
- Provision a Virtual Private Server (VPS)  
- Use Docker Machine to create and manage remote Docker hosts  
- Build optimized production images  
- Deploy your stack—and push updates—to a live environment  

By the end, your full application will be running live on the Internet.




1. Deployment Options:

1. Self-Managed VPS  
   - Rent an inexpensive VM (DigitalOcean, Linode, AWS Lightsail)  
   - Full control over OS, security, and Docker install  
2. Managed Container Services  
   - AWS ECS/EKS, Google GKE, Azure AKS  
   - Infrastructure managed for you, may incur higher cost  
3. Platform as a Service (PaaS)  
   - Heroku, Platform.sh, Render  
   - Simplified deployment workflows with Git integrations  
4. Hybrid & Edge Deployments  
   - On-premise + cloud, or multi-region edge nodes  
   - Useful for latency-sensitive or regulated environments  



2. Getting a Virtual Private Server (VPS):

- Sign up at your provider of choice.  
- Choose CPU/RAM/SSD based on expected load (e.g., 1 vCPU, 2 GB RAM for small apps).  
- Select OS: Ubuntu LTS is a common Docker host.  
- Configure SSH key for secure, password-less login.  
- Open firewall ports:  
  - HTTP/HTTPS (80/443)  
  - Your app’s ports (3000, 3001, 27017) if needed  



3. Provisioning with Docker Machine:

'''bash
docker machine create \
  --driver digitalocean \
  --digitalocean-access-token $DO_TOKEN \
  --digitalocean-region nyc3 \
  --digitalocean-size s-1vcpu-2gb \
  my-vps
'''

- 'docker-machine ls': Lists remote hosts  
- 'eval $(docker-machine env my-vps)': Point your local Docker client to the VPS  
- 'docker info': Verify you’re now talking to the remote Docker daemon  



4. Creating Optimized Production Images

1. Multi-stage Dockerfiles  
   - Build & test in one stage, copy only final artifacts into a slimmer runtime image  
2. Environment variables & secrets  
   - Use 'ARG' for build-time variables  
   - Use Docker secrets or external vaults for sensitive data  
3. Minimize layers & unused tools  
   - Remove package caches ('npm cache clean --force')  
   - Use 'alpine'-based base images when possible  



5. Deploying the Application

1. Push images to a registry (Docker Hub, AWS ECR):  
   '''bash
   docker-compose build --pull
   docker tag myapp_web:latest yourrepo/myapp_web:prod
   docker push yourrepo/myapp_web:prod
   '''
2. On the VPS (via Docker Machine):  
   '''bash
   git clone https://github.com/you/your-app.git
   cd your-app
   docker-compose -f docker-compose.prod.yml pull
   docker-compose -f docker-compose.prod.yml up -d
   '''
3. Update workflow:  
   - Rebuild & push new images  
   - 'docker-compose pull && docker-compose up -d' on the server  



6. Deployment Architecture Diagram

'''text
        Internet
           │
        Firewall
           │
       ┌───────────┐
       │   VPS     │  ← Docker Machine host
       └───────────┘
           │
 ┌─────────┼─────────┐
 │         │         │
 │   [web] │ [api]   │ [db]
 │  React  │  Node   │ MongoDB
 │ 3000→80 │3001→8080│27017→27017
 └─────────┴─────────┴────────┘
'''

- Port mappings expose services to the public  
- Private bridge network connects containers internally  
- Volumes persist database data beyond container lifecycles  

With these steps, your Dockerized application moves from local development to a secure, production-ready environment. Next, we’ll dive into monitoring and scaling techniques.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Deployment Options: 



Deployment Options:

When turning Dockerized applications into production services, you can choose between:

- Single-Host Deployment  
- Cluster Deployment  

Each has distinct trade-offs in terms of simplicity, availability, and scalability.



1. Single-Host Deployment

- Definition: All containers run on a single server (VPS or dedicated machine).  
- Pros:  
  - Very easy to set up  
  - Low operational overhead  
  - Ideal for small-scale or development environments  
- Cons:  
  - Single point of failure—if the host dies, the app goes down  
  - Limited resources—one machine can’t handle huge traffic spikes  

Use Case: Small teams or early-stage projects that need quick, simple deployments.



2. Cluster Deployment

- Definition: Containers are distributed across multiple servers (nodes) in a coordinated group.  
- Benefits:  
  - High Availability: If one node fails, others continue serving traffic  
  - Scalability: Add more nodes to handle more load  
- Complexity: Requires an orchestration layer to manage scheduling, health checks, service discovery, and scaling  



3. Orchestration Tools

To operate a cluster, you need a container orchestrator. The two main options are:

| Tool           | Description                                                 | Popularity                     |
|----------------|-------------------------------------------------------------|-------------------------------|
| Docker Swarm | Built-in Docker clustering & orchestration                 | Niche (≈5% usage¹)             |
| Kubernetes | CNCF-incubated, Google-originated, feature-rich orchestration | De facto industry standard     |

¹ Based on course survey data

# Why Not Cover Kubernetes?

- Steep Learning Curve: Kubernetes alone warrants a dedicated course or book.  
- Scope: This course focuses on Docker fundamentals and single-host deployments. You can always evolve into Kubernetes later as needs grow.



4. Recommendation

Start with a single-host deployment to keep things simple. Once you outgrow one machine or need high availability, plan a migration to a cluster with an orchestration tool like Kubernetes.



5. Visual Comparison

'''
 Single-Host                       Cluster
 ┌───────────┐                     ┌───────────┐ ┌───────────┐
 │ Container │                     │ Container │ │ Container │
 │ Container │                     │ Container │ │ Container │
 │ Container │                     │ Container │ │ Container │
 └──┬──┬──┬──┘                     └──┬──┬──┬──┘ └──┬──┬──┬──┘
    Host Server                         Node A       Node B
'''

- Single-Host: All containers co-located  
- Cluster: Containers spread across multiple nodes with an orchestrator managing them




Codes and Other Notes in this Discussion: 

Deployment Options: 

Single-host deployment 
Cluster deployment 


Cluster Solutions:

Docker Swarm 
Kubernetes 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Getting a Virtual Private Server:


Getting a Virtual Private Server (VPS):
A Virtual Private Server (VPS) is a remote machine you control—ideal for hosting Dockerized applications in production. Below are popular providers, selection guidance, and initial setup notes.



1. Why Use a VPS?

- Isolation: Dedicated resources (CPU, RAM, storage) for your app  
- Full Control: Configure OS, firewall, Docker install  
- Public Access: Expose your services on the Internet via public IP or domain  



2. Popular VPS Providers

| Provider                  | Strengths                                | Considerations                       |
|---------------------------|------------------------------------------|--------------------------------------|
| DigitalOcean              | Simple UI, beginner-friendly             | Fewer “enterprise” features          |
| Google Cloud Platform     | Broad services, global footprint         | Steeper learning curve               |
| Microsoft Azure           | Integrated with Microsoft ecosystem      | Complex pricing                      |
| Amazon Web Services (AWS) | Extensive features, scalability          | Can be overwhelming for beginners    |
| Others (Linode, Vultr…)   | Competitive pricing, niche features      | Varies by provider                   |



3. Choosing the Right Provider

- Simplicity vs. Features:  
  - DigitalOcean: Quick setup, straightforward billing  
  - GCP/Azure/AWS: Enterprise-grade, but more complex dashboards  
- Pricing:  
  - Entry‐level droplets often start at \$5–\$10/month.  
  - Billing requires a credit/debit card (unless you already have a sponsored account or free credits).  
- Region & Latency:  
  - Select a data center close to your users for lower latency.  



4. Creating a Droplet on DigitalOcean

1. Sign in or create an account at https://digitalocean.com.  
2. Add a credit/debit card for billing.  
3. Click Create → Droplets.  
4. Choose an OS image (e.g., Ubuntu 22.04 LTS).  
5. Select a plan (e.g., Basic \$6/mo, 1 vCPU, 2 GB RAM).  
6. Pick a data center region close to your audience.  
7. Add SSH keys for secure login (recommended) or set a root password.  
8. Give your droplet a hostname (e.g., 'myapp-vps').  
9. Click Create Droplet and wait for provisioning.



5. Initial Server Setup

After your droplet is ready:

'''bash
# Point your shell to the new server
ssh root@YOUR_DROPLET_IP

# Update & upgrade packages
apt update && apt upgrade -y

# Install Docker
apt install -y docker.io docker-compose

# Add your user to the docker group (optional)
usermod -aG docker YOUR_USERNAME
'''

- Firewall:  
  '''bash
  ufw allow OpenSSH
  ufw allow 80/tcp   # HTTP
  ufw allow 443/tcp  # HTTPS
  ufw enable
  '''
- Verify Docker:  
  '''bash
  docker run hello-world
  '''



6. Next Steps

- Push your Docker images to a registry (Docker Hub, AWS ECR).  
- Clone your application repository on the VPS.  
- Use 'docker-compose up -d' to launch your stack.  

Having a VPS lets you deploy, manage, and scale your Docker applications in a controlled, publicly accessible environment.




Codes and Other Notes in this Discussion: 

Digital Ocean 
Google Cloud Platform (GCP)
Microsoft Azure 
Amazon Web Services (AWS)


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Installing Docker Machine: 




Installing Docker Machine:
Docker Machine lets you manage remote Docker hosts from your local terminal. Once configured, any 'docker' or 'docker-compose' command you run locally will execute on your remote server.



1. Why Docker Machine?

- Remote Control: Connect your local CLI to Docker Engine running on a VPS.  
- Unified Workflow: Use the same Docker commands ('docker ps', 'docker run', 'docker-compose up') against both local and remote hosts.  
- Multiple Hosts: Easily switch between several Docker Machines (e.g., development, staging, production).



2. Get the Latest Release

1. Open the Docker Machine releases page:  
   https://github.com/docker/machine/releases

2. Note the latest version.  
   - Example at recording time: v0.16.2



3. Install Docker Machine

Choose the installation snippet for your platform from the releases page. Here’s an example for Linux/macOS:

'''bash
# Download and install Docker Machine binary
base=https://github.com/docker/machine/releases/download/v0.16.2
curl -L $base/docker-machine-$(uname -s)-$(uname -m) \
  -o /usr/local/bin/docker-machine

# Make it executable
chmod +x /usr/local/bin/docker-machine
'''

> On Windows, follow the equivalent PowerShell or MSI instructions listed on the releases page.



4. Verify the Installation

Run:

'''bash
docker-machine --version
'''

Expected output:

'''
docker-machine version 0.16.2, build 7b28e17
'''



5. Docker Machine Workflow Diagram

'''
[ Local CLI ]         ── SSH ──▶  [ Remote Docker Host ]
docker-machine env my-vps           Docker Engine
docker ps                           containers on VPS
docker run ...                      your services
'''

1. Create a machine:  
   'docker-machine create --driver <provider> my-vps'  
2. Point your shell at it:  
   'eval $(docker-machine env my-vps)'  
3. Run Docker commands locally, targeting the VPS.  



Once Docker Machine is installed and configured, you’re ready to provision and manage your remote Docker environments. In the next lesson, we’ll provision a VPS and connect to it with Docker Machine.




Codes and Other Notes in this Discussion: 

https://github.com/docker/machine/releases

docker-machine --version 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Provisioning a Host:



Provisioning a Remote Host with Docker Machine:
Use Docker Machine to spin up a remote Docker host (VPS) on DigitalOcean via your local CLI. This process creates a Droplet, installs Docker Engine, and makes it accessible for 'docker' and 'docker-compose' commands.



1. Prerequisites

- You have Docker Machine installed locally ('docker-machine --version').  
- You have a DigitalOcean account (requires a credit/debit card).  
- You’ve generated a Personal Access Token in DigitalOcean.



2. Docker Machine Drivers

Docker Machine supports multiple cloud providers via drivers:

- 'digitalocean'  
- 'amazonec2'  
- 'azure'  
- 'google'  
- 'none' (for existing hosts)

Find driver-specific options at:  
https://github.com/docker/machine#drivers



3. Generate a DigitalOcean API Token

1. Log in at digitalocean.com → Account → API.  
2. Under “Personal access tokens”, click Generate New Token.  
3. Name it (e.g., 'vidly') → Generate Token.  
4. Copy the token immediately (you won’t see it again).



4. Create the Droplet with 'docker-machine'

Use a multi-line command. On Linux/macOS, end lines with '\'. On Windows PowerShell, end with '' ' ''.

'''bash
docker-machine create \
  --driver digitalocean \
  --digitalocean-access-token YOUR_TOKEN_HERE \
  --engine-install-url \
    https://releases.rancher.com/install-docker/19.03.sh \
  vidly
'''

Explanation of flags:

- '--driver digitalocean'  
  Use the DigitalOcean driver.  
- '--digitalocean-access-token'  
  Your API token to authenticate with DO.  
- '--engine-install-url'  
  (Workaround) Custom install script URL to get Docker 19.03 on the Droplet.  
- 'vidly'  
  Name of your new machine (Droplet).

Note:  
DigitalOcean’s default may install an older Docker version—this flag points to a Rancher script for Docker 19.03. Remove it once DO supports newer Docker natively.



5. What Happens Under the Hood

1. DigitalOcean API call → provisions a new Droplet with Ubuntu LTS.  
2. SSH connection → runs the Docker Engine install script.  
3. Docker Machine stores client certs & config in '~/.docker/machine/machines/vidly'.  

Once complete, 'docker-machine ls' shows:

| NAME   | ACTIVE | DRIVER      | STATE   | URL                       |
|--------|--------|-------------|---------|---------------------------|
| vidly  | *      | digitalocean| Running | tcp://203.0.113.25:2376   |



6. Verify in the DigitalOcean Control Panel

- Go to your DigitalOcean dashboard → Droplets.  
- You should see a Droplet named vidly.  
- Note its public IP (e.g., '203.0.113.25').  

Visit the IP in your browser—nothing is served yet, but the server is online.



7. Next Steps: Connect Your CLI to the Droplet

'''bash
eval $(docker-machine env vidly)
# Your docker commands now target the remote host
docker info          # Verify Docker Engine on vidly
docker ps            # List containers on the Droplet
'''



8. Architecture Diagram

'''
[ Local Dev Machine ]
         │
docker-machine create
         │
  ┌──────▼──────┐    SSH & API    ┌────────────────────┐
  │ Docker CLI  │ ─────────────▶  │ DigitalOcean API   │
  │  (docker)   │                 │                    │
  └─────────────┘                 └────────┬───────────┘
                                           │ provision
                                           ▼
                                   ┌──────────────┐
                                   │  Droplet     │
                                   │ Name: vidly  │
                                   │ Ubuntu +     │
                                   │ Docker 19.03 │
                                   └──────────────┘
'''

With the Droplet provisioned and your CLI pointed at it, you’re ready to deploy your Docker Compose stack remotely.




Codes and Other Notes in this Discussion: 

docker-machine create \
--driver digitalocean \
--digitalocean-access-token b3f0.......... \
--engine-install-url \
vidly 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Connecting to the Host: 




Connecting to the Remote Host:
After provisioning a VPS with Docker Machine, you can inspect it and open an SSH session—all with simple Docker Machine commands.



1. List Your Docker Machines

Command:  
'''bash
docker-machine ls
'''

Sample output:

| NAME  | ACTIVE | DRIVER        | STATE   | URL                          | SWARM | DOCKER        | ERRORS |
|-------|:------:|---------------|---------|------------------------------|:-----:|---------------|--------|
| vidly |        | digitalocean  | Running | tcp://203.0.113.25:2376      |       | 19.03.8       |        |

- NAME: Your machine’s name ('vidly')  
- ACTIVE: “\*\*" marks the current target for your Docker CLI (empty until you 'env')  
- DRIVER: Cloud provider or 'none'  
- STATE: 'Running', 'Stopped', etc.  
- URL: Docker daemon endpoint (for TLS-based remote API)  
- SWARM: If it’s part of a Swarm cluster (blank = single-host)  
- DOCKER: Engine version on the host  
- ERRORS: Any setup errors  



2. SSH Into the Host

Instead of manually juggling SSH keys, Docker Machine wires it up for you:

'''bash
docker-machine ssh vidly
'''

- Automatically uses the SSH key and IP from 'docker-machine ls'.  
- Logs you in as root on the remote Ubuntu VM.

Example session:
'''
$ docker-machine ssh vidly
Welcome to Ubuntu 20.04 LTS (GNU/Linux 5.4.0-xx-generic x86_64)

root@vidly:~# pwd
/root
root@vidly:~# ls
(root directory is empty until you deploy your app)
root@vidly:~# exit
logout
Connection to 203.0.113.25 closed.
'''



3. How It Works (ASCII Diagram)

'''
 Local Dev Machine
 ┌────────────────────────────────┐
 │  docker-machine ssh vidly      │
 └───────────┬────────────────────┘
             │ SSH (auto-configured)
             ▼
   ┌───────────────────────────┐
   │ Remote Droplet “vidly”    │
   │  – OS: Ubuntu             │
   │  – Docker Engine: 19.03   │
   │  – Root’s HOME (/root)    │
   └───────────────────────────┘
'''

- Docker Machine sets up the SSH keys and user for you.  
- No manual 'ssh-add' or key copying required.



4. Next Steps

1. Point your Docker CLI to the remote host:  
   '''bash
   eval "$(docker-machine env vidly)"
   '''
2. Deploy your Compose stack:  
   '''bash
   git clone <repo-url>
   cd your-app
   docker-compose up -d
   '''
3. Verify running containers on the VPS:  
   '''bash
   docker ps
   '''

You’re now ready to manage Docker containers and services directly on your remote host.




Codes and Other Notes in this Discussion: 

docker-machine ls 

docker-machine ssh vidly 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------

Defining the Production Configuration: 

Production Compose Configuration:
Use a separate Compose file for production—stripped of development-only settings (bind mounts, test services), with proper port mappings and restart policies.



1. Create 'docker-compose.prod.yml'

1. Copy your existing 'docker-compose.yml' to a new file named 'docker-compose.prod.yml'.  
2. Remove development conveniences:  
   - Volume mounts for code (no hot-reload)  
   - Test services ('web-tests', 'api-tests')  



2. Service Definitions

docker-compose.prod.yml

'''yaml
version: "3.8"

services:
  web:
    build: ./frontend
    ports:
      - "80:3000"
    restart: unless-stopped

  api:
    build: ./backend
    ports:
      - "3001:3001"
    environment:
      DB_URL: "mongodb://db/vidly"
    command: ./docker-entrypoint.sh
    restart: unless-stopped

  db:
    image: mongo:4.0-xenial
    ports:
      - "27017:27017"
    volumes:
      - vidly:/data/db
    restart: unless-stopped

volumes:
  vidly: {}
'''

- web  
  - Maps public port 80 → container 3000 for HTTP.  
  - No code volume mount—serves built assets only.  
- api  
  - Maps host 3001 → API port 3001.  
  - Overrides startup with 'docker-entrypoint.sh' (runs migrations & server).  
- db  
  - Exposes MongoDB on 27017, persists data via named volume 'vidly'.  



3. Restart Policies

Define how Docker restarts failed or stopped containers:

| Policy         | Behavior                                                                 |
|----------------|--------------------------------------------------------------------------|
| no (default)   | Never restart automatically                                              |
| always         | Always restart container, even if manually stopped                       |
| on-failure[:N] | Restart only if exit code ≠ 0, up to *N* retries                         |
| unless-stopped | Like always, but does not restart after a manual 'docker stop'   |

Use 'restart: unless-stopped' in production to ensure uptime while allowing controlled shutdowns.



4. Service Lifecycle Diagram

'''
   ┌───────────────┐
   │  docker       │ Starts
   │docker-compose │─ up -d ─▶ web, api, db
   └───────────────┘          │
                              ▼
                       ┌───────────┐
                       │ Containers│
                       │ Running   │
                       └───────────┘
                              │
                   ┌──────────┴─────────┐
                   │Container Crash ↓   │
                   └──────────┬─────────┘
                              │
                    ┌─────────▼────────┐
                    │ Docker Restart   │
                    │ “unless-stopped” │
                    └──────────────────┘
'''



5. Environment-Specific Overrides

- Staging: 'docker-compose.staging.yml'—mirrors prod, adds debug logging or different DB URL.  
- Testing: 'docker-compose.test.yml'—uses ephemeral in-memory databases, no port publishing.  

Invoke production file explicitly:

'''bash
docker-compose -f docker-compose.prod.yml up -d
'''




Codes and Other Notes in this Discussion: 

docker-compose.prod.yml: 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 80:3000
	restart: unless-stopped
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	command: ./docker-entrypoint.sh
	restart: unless-stopped
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db
	restart: unless-stopped
	
volumes:
  vidly: 
  
  
  
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
Reducing the Image Size:   




Reducing Front-End Image Size with Multi-Stage Docker Builds:
By default, your React app image (built on a full Node base) can be hundreds of megabytes. With a multi-stage Dockerfile, you can trim it down to just the static assets served by a lightweight web server.



1. Baseline: Large Dev Image

'''text
REPOSITORY     TAG       SIZE
vidly_web      latest    300 MB    ← Node + source + build tools
'''

Our original 'Dockerfile' (in 'frontend/'):

'''dockerfile
FROM node:14.16.0-alpine3.13
RUN addgroup app && adduser -S -G app app
USER app
WORKDIR /app

COPY package*.json ./
RUN npm install
COPY . .

EXPOSE 3000
CMD ["npm", "start"]
'''

- Installs dev dependencies, Node, npm, source files  
- Runs React dev server on port 3000  
- Development-focused, not optimized for production



2. Generate Production Assets

React’s 'npm run build' outputs static files into 'build/':

'''bash
cd frontend
npm run build
'''

- Creates optimized, minified JS/CSS in 'frontend/build/'  
- Framework-agnostic: most modern front-end stacks have a “build” step



3. Multi-Stage Production Dockerfile

Create 'frontend/Dockerfile.prod':

'''dockerfile
# ── Step 1: Build Stage ───────────────────────────────────
FROM node:14.16.0-alpine3.13 AS build-stage
WORKDIR /app

# Copy manifest & install dependencies
COPY package*.json ./
RUN npm install

# Copy source and build static assets
COPY . .
RUN npm run build


# ── Step 2: Production Stage ────────────────────────────
FROM nginx:1.12-alpine
# (Optional) Create non-root user if needed:
# RUN addgroup app && adduser -S -G app app
# USER app

# Copy built files from the build-stage
COPY --from=build-stage /app/build /usr/share/nginx/html

EXPOSE 80
ENTRYPOINT ["nginx", "-g", "daemon off;"]
'''

Key Points:

- 'AS build-stage' labels the first stage.  
- 'COPY --from=build-stage' grabs only static assets.  
- Base image switches from Node (300 MB) to nginx:alpine (~16 MB).



4. Build the Optimized Image

From the 'frontend/' directory:

'''bash
docker build \
  -t vidly_web_opt \
  -f Dockerfile.prod \
  .
'''

After build:

'''text
REPOSITORY        TAG       SIZE
vidly_web_opt     latest    16 MB    ← Optimized static-only image
'''



5. Update Production Compose Configuration

In 'docker-compose.prod.yml', adjust the 'web' service:

'''yaml
version: "3.8"

services:
  web:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    ports:
      - "80:80"
    restart: unless-stopped

  api:
    # unchanged
  db:
    # unchanged
'''

- 'context:' points to 'frontend/'  
- 'dockerfile:' specifies the production Dockerfile  
- Port mapping now 80:80



6. Workflow Diagram

'''
┌─────────────────────────┐           ┌───────────────────────────┐
│  Stage 1: build-stage   │           │ Stage 2: production      │
│ FROM node:alpine        │           │ FROM nginx:alpine         │
│ RUN npm install         │           │                           │
│ RUN npm run build       │           │ COPY --from=build-stage   │
│   ↓ /app/build →        │ ────────▶ │   /usr/share/nginx/html   │
└─────────────────────────┘           └───────────────────────────┘
'''



7. Benefits

- Bandwidth & Speed: Push/pull 16 MB vs. 300 MB  
- Security & Stability: No unnecessary build tools in production image  
- Clarity: Separation of build vs. runtime concerns in Dockerfile stages  

With this multi-stage approach, your front-end container is lean, fast to deploy, and production-ready.





Codes and Other Notes in this Discussion: 


Dockerfile: (Initial Setup)

FROM node:14.16.0-alpine3.13 
RUN addgroup app && adduser -S -G app app
USER app
WORKDIR /app
COPY package*.json ./
RUN npm install 
COPY . . 
EXPOSE 3000
CMD ["npm", "start"]



Dockerfile.prod: 
#Step 1: Build Stage 
FROM node:14.16.0-alpine3.13 AS build-stage 
WORKDIR /app
COPY package*.json ./
RUN npm install 
COPY . . 
RUN npm run build 

#Step 2: Production
FROM nginx:1.12-alpine AS 
RUN addgroup app && adduser -S -G app app
USER app
COPY --from=build-stage /app/build /usr/share/nginx/html  
EXPOSE 80 
ENTRYPOINT [ "nginx", "-g", "daemon off;" ]



docker build -t vidly_web_opt -f Dockerfile.prod . 



docker-compose.prod.yml: 

version: "3.8"

services: 
  web: 
    build: 
	  context: ./frontend 
	  dockerfile: Dockerfile.prod 
	ports: 
	  - 80:80
	restart: unless-stopped
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	command: ./docker-entrypoint.sh
	restart: unless-stopped
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db
	restart: unless-stopped
	
volumes:
  vidly: 
  
  
  
docker-compose -f docker-compose.prod.yml build 




-------------------------------------------------------------------------------------------------------------------------------------------------------------------

Deploying the Application: 



Deploying Your Application to the Remote Host:
This guide walks through connecting your local Docker client to a remote VPS, running your production Compose stack, troubleshooting a permissions issue, and applying a Dockerfile fix.



1. Point Your CLI at the Remote Docker Host

1. List machines  
   '''bash
   docker-machine ls
   '''
   Sample output:
   '''
   NAME   ACTIVE   DRIVER       STATE    URL                     SWARM   DOCKER   ERRORS
   vidly           digitalocean Running  tcp://203.0.113.25:2376          19.03    
   '''

2. View connection env vars  
   '''bash
   docker-machine env vidly
   '''
   Outputs several 'export DOCKER_*' lines.

3. Apply them with 'eval'  
   '''bash
   eval "$(docker-machine env vidly)"
   '''
   Now all 'docker' and 'docker-compose' commands run against the remote VPS.



2. Deploy with Docker Compose

From your project root:

'''bash
docker-compose -f docker-compose.prod.yml up -d --build
'''

- '-f docker-compose.prod.yml' uses your production Compose file  
- '-d' runs in detached mode  
- '--build' forces rebuilding images before starting  

Docker will:

1. Build (or rebuild) your 'web', 'api' services using prod Dockerfiles  
2. Pull and start the 'db' service  
3. Create the network and named volume  
4. Launch containers on the remote host  



3. Permission Error: “missing write access to /app”

# Symptoms
During 'up --build', you see:

'''
Error: EACCES: permission denied, mkdir '/app'
'''

# Cause
- Older Docker Engine on the VPS uses the legacy builder (not BuildKit).  
- The 'WORKDIR /app' instruction creates '/app' as root-owned, then 'USER app' has no write permissions.

# BuildKit vs. Legacy
- BuildKit (newer Docker): Respects 'USER' for 'WORKDIR' creation.  
- Legacy builder: Always creates 'WORKDIR' as root, regardless of 'USER'.



4. Fix: Pre-create & Chown '/app' in the Dockerfile

# Original 'backend/Dockerfile'
'''dockerfile
FROM node:14.16.0-alpine3.13
RUN addgroup app && adduser -S -G app app
USER app

WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .

EXPOSE 3001
CMD ["npm","start"]
'''

# Updated 'backend/Dockerfile'
'''dockerfile
FROM node:14.16.0-alpine3.13

# 1. Create app user/group
RUN addgroup app && adduser -S -G app app

# 2. Pre-create /app and give ownership to 'app'
RUN mkdir /app && chown app:app /app

USER app
WORKDIR /app

# 3. Copy files & install dependencies
COPY package*.json ./
RUN npm install
COPY . .

EXPOSE 3001
CMD ["npm","start"]
'''

- Step 2 ensures '/app' is owned by 'app' before 'USER app' switches context.  
- This change works with both legacy and BuildKit builders.



5. Redeploy After the Fix

1. Commit your updated Dockerfile to version control.  
2. Rerun the deployment command:
   '''bash
   docker-compose -f docker-compose.prod.yml up -d --build
   '''
3. Confirm all containers start without errors:
   '''bash
   docker ps
   '''



6. End-to-End Workflow Diagram

'''
Local CLI
  │
  │ eval $(docker-machine env vidly)
  ▼
Remote VPS (vidly) Docker Engine
  ├─ Builds 'api' image with fixed Dockerfile
  ├─ Pulls 'db' image
  ├─ Builds 'web' image
  ├─ Creates network & volume
  └─ Runs containers in detached mode
'''

With these steps, your production stack should run smoothly on the remote host—complete with migrations, optimized images, and resilience to permission quirks.






Codes and Other Notes in this Discussion: 

docker-machine ls 

docker-machine env vidly 

eval $(docker-machine env vidly)

docker-compose -f docker-compose.prod.yml up -d 



backend/Dockerfile: 

Dockerfile: (Initial Setup)

FROM node:14.16.0-alpine3.13 

RUN addgroup app && adduser -S -G app app
USER app

WORKDIR /app
COPY package*.json ./
RUN npm install 
COPY . . 

EXPOSE 3001

CMD ["npm", "start"]



Dockerfile:
FROM node:14.16.0-alpine3.13 

RUN addgroup app && adduser -S -G app app
RUN mkdir /app && chown app:app /app  
USER app

WORKDIR /app
COPY package*.json ./
RUN npm install 
COPY . . 

EXPOSE 3001
CMD ["npm", "start"]



docker-compose -f docker-compose.prod.yml up -d --build 



-------------------------------------------------------------------------------------------------------------------------------------------------------------------

Troubleshooting Deployment Issues: 
When deploying your Docker Compose stack to a remote host, various runtime errors can occur. Below is a structured checklist and step-by-step guide to diagnose and resolve common problems.



1. Accessing the Application

1. Get the VPS IP  
   '''bash
   docker-machine ls
   '''
   Note the URL (e.g. 'tcp://203.0.113.25:2376') or check your provider’s dashboard.

2. Open in Browser  
   - Front end: 'http://<VPS_IP>/'  
   - Back end API: 'http://<VPS_IP>:3001/api'

3. Observation  
   - API responds correctly (e.g., '/api/movies').  
   - Front end shows blank page or fails to load.



2. Diagnose Container Failures

1. List containers on the remote host  
   '''bash
   docker-compose -f docker-compose.prod.yml ps
   '''
   Look for containers in 'Restarting' state.

2. Inspect container logs  
   '''bash
   docker-compose -f docker-compose.prod.yml logs web
   '''
   or
   '''bash
   docker logs <web_container_id>
   '''

3. Common log output  
   '''
   [warn] the 'user' directive makes sense only if the master process runs with super-user privileges
   Permission denied: cannot write to /usr/share/nginx/html
   '''
   Indicates NGINX was started as a non-root user but lacks file permissions.



3. Fixing NGINX Permission Errors

1. Problem Root Cause  
   - Production Dockerfile created a non-root 'app' user.  
   - NGINX default document root ('/usr/share/nginx/html') is owned by root.  
   - Non-root user cannot bind or write logs.

2. Temporary Workaround  
   In 'frontend/Dockerfile.prod', remove the non-root user steps:
   '''diff
   - RUN addgroup app && adduser -S -G app app
   - USER app
   '''
   This forces NGINX to run as root, restoring write access.

3. Rebuild & Redeploy  
   '''bash
   docker-compose -f docker-compose.prod.yml up -d --build
   '''
   Confirm the web container remains in 'Up' state.



4. API Connectivity from Front End

1. Symptom  
   - Front end loads but fetching data fails.  
   - DevTools Network tab shows requests to 'http://localhost:3001/api/...', which doesn’t resolve from the user’s browser.

2. Root Cause  
   - React code defaults to 'API_URL = process.env.REACT_APP_API_URL || 'http://localhost:3001/api''.  
   - No 'REACT_APP_API_URL' was set during the build, so it fell back to 'localhost'.

3. Verify in Browser  
   - Open Chrome DevTools → Network → Refresh.  
   - Look for API calls pointing to 'localhost:3001' instead of the VPS IP.



5. Injecting the Correct API URL

1. Build-Time Environment Variables  
   React’s build tool inlines env vars at build time—browser code cannot read runtime variables.

2. Modify 'Dockerfile.prod' (Front End)  
   '''dockerfile
   # Before RUN npm run build:
   ENV REACT_APP_API_URL=http://203.0.113.25:3001/api
   RUN npm run build
   '''
   Replace '203.0.113.25' with your VPS_IP.

3. Rebuild & Redeploy  
   '''bash
   docker-compose -f docker-compose.prod.yml up -d --build
   '''
   Now the static bundle references the correct external API host.



6. Final Verification

- Front end: 'http://<VPS_IP>/'  
  - Static page loads, movie list is populated.
- Back end API: 'http://<VPS_IP>:3001/api/movies'  
  - Returns JSON array as expected.



7. Deployment Workflow Recap

'''text
1. Choose target machine (e.g., 'vidly'):
   $ docker-machine ls

2. Point local Docker client to remote:
   $ eval "$(docker-machine env vidly)"

3. Build & launch production stack:
   $ docker-compose -f docker-compose.prod.yml up -d --build

4. Troubleshoot:
   - docker-compose ps    → find restarting containers
   - docker-compose logs   → inspect errors
   - Adjust Dockerfiles    → fix permissions, env vars
   - rebuild & redeploy    → verify fixes

5. Optional: Automate
   Create 'deploy.sh':
   '''bash
   #!/bin/sh
   eval "$(docker-machine env vidly)"
   docker-compose -f docker-compose.prod.yml up -d --build
   '''
   Run: './deploy.sh'
'''

By following this structured approach—checking container status, inspecting logs, applying targeted fixes, and rebuilding—you can efficiently troubleshoot and stabilize your production deployment.





Codes and Other Notes in this Discussion: 

docker-machine ls 

docker-machine env vidly 

eval $(docker-machine env vidly) 

docker-compose up 


-------------------------------------------------------------------------------------------------------------------------------------------------------------------

Publishing Changes: 

Publishing Changes with Versioned Image Tags:
When deploying to multiple environments, you need clarity on which version of your application is running. Properly tagging Docker images prevents confusion and enables traceability.



1. The Versioning Problem

- Running on your production host:  
  '''bash
  docker ps
  '''
  shows containers built from images like 'vidly_web' without any version suffix.  
- Without tags, you cannot tell whether 'vidly_web' corresponds to v1.0, v1.1, or a hotfix build.  
- Inconsistencies arise when staging has different code than production.



2. Add 'image:' Tags in Your Production Compose File

Edit 'docker-compose.prod.yml' to include explicit tags alongside 'build:':

'''yaml
version: "3.8"

services:
  web:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    image: vidly_web:1                # Tag this image “1”
    ports:
      - "80:80"
    restart: unless-stopped

  api:
    build: ./backend
    image: vidly_api:1                # Tag API image “1”
    ports:
      - "3001:3001"
    environment:
      DB_URL: "mongodb://db/vidly"
    command: ./docker-entrypoint.sh
    restart: unless-stopped

  db:
    image: mongo:4.0-xenial
    ports:
      - "27017:27017"
    volumes:
      - vidly:/data/db
    restart: unless-stopped

volumes:
  vidly: {}
'''

- 'image: vidly_web:1' tells Compose to tag the built image as 'vidly_web:1'.  
- Similarly for 'vidly_api:1'.  



3. Redeploy with New Tags

Run:

'''bash
docker-compose -f docker-compose.prod.yml up -d --build
'''

- '--build' rebuilds and tags new images.  
- '-d' runs containers in detached mode.

Verify with:

'''bash
docker ps
'''

You should now see:

'''
CONTAINER ID  IMAGE            COMMAND   ...   STATUS   PORTS
abcd1234      vidly_web:1      "nginx…"      Up       80/tcp
efgh5678      vidly_api:1      "node…"       Up       3001/tcp
...
'''



4. Manual Updates vs. CI/CD Automation

- Manual Tag Bump:  
  You edit 'docker-compose.prod.yml' to change ':1' → ':2', commit, and redeploy.  
- Automated CI/CD:  
  - Your pipeline builds the image after each Git push.  
  - It tags images using the Git SHA (e.g., 'vidly_web:abc1234') or a build number ('vidly_web:42').  
  - Deployment scripts pull and run the exact tagged version in each environment (staging, production).

'''text
Git Commit ──▶ CI Build ──▶ Docker Build & Tag ──▶ Push to Registry
                                         │
                                         ▼
                               Deploy to Production via Compose
'''

Benefit: You always know exactly which code is live, and you can rollback by redeploying an earlier tag.



5. Summary

1. Tag images in your Compose file to reflect versions.  
2. Redeploy with 'docker-compose up -d --build' to apply new tags.  
3. Confirm with 'docker ps' that the correct versions are running.  
4. Automate tagging and deployment via CI/CD for reliable, traceable releases.






Codes and Other Notes in this Discussion: 

docker-compose.prod.yml: 

version: "3.8"

services: 
  web: 
    build: 
	  context: ./frontend 
	  dockerfile: Dockerfile.prod 
	image: vidly_web:1
	ports: 
	  - 80:80
	restart: unless-stopped
  api: 
    build: ./backend 
	image: vidly_api:1 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	command: ./docker-entrypoint.sh
	restart: unless-stopped
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db
	restart: unless-stopped
	
volumes:
  vidly: 
  
  
    
docker-compose -f docker-compose.prod.yml up -d --build 

-------------------------------------------------------------------------------------------------------------------------------------------------------------------