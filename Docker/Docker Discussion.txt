

Remember to Use Think Deeper in Copilot

Docker 


Introduction: 
Welcome to the ultimate Docker course. In this course, I'm going to take you on a journey and teach you everything you need to know about Docker from the basics to more advanced concepts. So by the end of this course, you'll be able to use it like a pro as part of your software development workflow. 

If you're looking for a comprehensive and highly practical course that takes you from zero to hero, this is the Docker course for you. We're going to start off with a really simple project. So you understand the basics, then we'll use Docker run and deploy a full stack application with a front end, back end and a database. 

So you learn all the necessary techniques and apply them to your own projects. I'm Mosh Hamadani and I've taught millions of people how to advance their software engineering skills through my YouTube channel and online school CodeWithMosh.com. Now let's jump in and get started.



Prerequisites:
Let's quickly talk about what you need to know to take this course. To take this course, you don't need any prior knowledge of Docker, because I'm going to teach you everything from scratch, but you should have at least 3 months of programming experience. Ideally, you should have built at least one application, so you should know concepts like frontend, backend, API, and database. 

It doesn't matter what languages and frameworks or what database engines you're familiar with, but you should know what these concepts are all about. Also, you should be familiar with basic git commands, like cloning a GitHub repository, committing code, pushing and pulling. Just the basics, nothing more. 

With all that, let's move on to the next lesson.



How to Take This Course:
Now, we all have different ways of learning things. But let me tell you what I think is the best way to take this course. This course is highly practical so you should be active while watching this course.
In my opinion, it's best if you watch each lesson, take some notes, you can either take notes on a piece of paper or using your favorite note taking tool, just write some keywords, some keywords that help you remember what we talked about. Then, after each lesson, go through your notes and repeat the same steps I've taken in that lesson. So if I talked about a bunch of commands, play with those commands, make sure you understand how they work.
If you follow that, I promise you, by the end of this course, you're going to master docker and use it like a pro.











Introduction to Docker:


Introduction: 
Alright. Our journey to master Docker begins here. In every section, you're going to discover something new about Docker. In this section, we're going to talk about what Docker is and why it's so popular. Then we're going to talk about virtual machines and containers.

Next, we're going to talk about the architecture of Docker so you understand how it works. Then we're going to install Docker and get our hands dirty. I'm going to give you an overview of your development workflow with Docker and then we're going to see that in action using a really simple example. So by the end of this section, you will have an idea of what Docker is all about. I'm so excited about this section, I hope you are too, so let's jump in and get started.



Codes and Other Notes in this Discussion: 

In this Section: 
What is Docker?
Virtual Machines vs Containers
Architecture of Docker
Installing Docker
Development Workflow
Docker in Action 









What is Docker?

So what is docker and why is everyone using it these days? Well, docker is a platform for building, running and shipping applications in a consistent manner, so so if your application works on your development machine, it can run and function the same way on other machines. If you have been developing software for a while, you've probably come across this situation where your application works on your development machine but doesn't somewhere else. Can you think of three reasons why this happens? Well, this can happen if one or more files are not included as part of your deployment, so your application is not completely deployed, it's missing something.

This can also happen if the target machine is running a different version of some software that your application needs. Let's say your application needs node version 14, but the target machine is running node version nine. This can also happen if the configuration settings like environment variables are different across these machines. And this is where Docker comes to the rescue. With Docker, we can easily package up our application with everything it needs and run it anywhere on any machine with Docker.

So if your application needs a given version of node and mongo db, all of these will be included in your applications package. Now you can take this package and run it on any machine that runs docker. So if it works on your development machine, it's definitely going to work on your test and production machines. Now there's more, if someone joins your team, they don't have to spend half a day or so setting up a new machine to run your application. They don't have to install and configure all these dependencies, they simply tell docker to bring up your application and docker itself will automatically download and run these dependencies inside an isolated environment called a container.

And this is the beauty of docker, this isolated environment allows multiple applications use different versions of some software side by side. So one application may use note version 14, another application may use note version nine. Both these applications can run side by side on the same machine without messing with each other. So this is how docker allows us to consistently run an application on different machines. Now there is one more benefit here, when we're done with this application and don't want to work on it anymore, we can remove the application and all its dependencies in one go.

Without Docker, as we work on different projects, our development machine gets cluttered with so many libraries and tools that are used by different applications and then after a while, we don't know if we can remove one or more of these tools because we're always afraid that we would mess up with some application. With docker, we don't have to worry about this. Because each application runs with its dependencies inside an isolated environment, we can safely remove an application with all its dependencies to clean up our machine. Isn't that great? So in a nutshell, Docker helps us consistently build, run and ship our applications, and that's why a lot of employers are looking for people with Docker skills these days.

So if you're pursuing a job as a software or dev ops engineer, I highly encourage you to learn docker and learn it well. And that's exactly what this course is all about. I'm going to take you on a journey and teach you everything you need to know about docker so you can use it like a pro. No copy pasting commands here.




Codes and Other Notes in this Discussion: 

Reasons:

One or more files missing
Software version mismatch  
Different Configuration settings 



docker-compose up

docker-compose down --rmi all 













Virtual Machines vs Containers: 

So in the last lesson, I briefly talked about containers. A container is an isolated environment for running an application. Now one of the questions that often comes up is how are containers different from virtual machines or VM's? Do you know the differences? Well, a virtual machine as the name implies is an abstraction of a machine or physical hardware.

So we can run several virtual machines on a real physical machine. For example, we can have a Mac and on this Mac we can run two virtual machines, one running Windows, the other running Linux. How do we do that? Using a tool called hypervisor. I know, it's one of those computer science names.

In simple terms, a hypervisor is software we use to create and manage virtual machines. There are many hypervisors available out there like VirtualBox and VMware, which are cross platform so they can run on Windows, macOS and Linux, and hyper v which is only for windows. So with a hypervisor, we can manage virtual machines. Now what is the benefit of building virtual machines? Well, for us software developers, we can run an application in isolation inside a virtual machine.

So on the same physical machine, we can have two different virtual machines, each running a completely different application and each application has the exact dependencies it needs. So application one may use node version 14 and mongo db version four, while application two may use node version nine and mongo db version three. All these are running on the same machine, but in different isolated environments. That's one of the benefits of virtual machines. But there are a number of problems with this model.

Each virtual machine needs a full copy of an operating system that needs to be licensed, patched and monitored. And that's why these virtual machines are slow to start because the entire operating system has to be loaded just like starting your computer. Another problem is that these virtual machines are resource intensive because each virtual machine takes a slice of the actual physical hardware resources, like CPU, memory and disk space. So if you have eight gigabytes of memory, that memory has to be divided between different virtual machines. Of course, we can decide how much memory to allocate to each virtual machine, but at the end of the day, we have a limit in terms of the number of v m's we can run on a machine.

Usually a handful, otherwise we're going to run out of hardware resources. Now let's talk about containers. Containers give us the same kind of isolation, so we can run multiple applications in isolation, but they are more lightweight. They don't need a full operating system. In fact, all containers on a single machine share the operating system of the host, so that means we need to license, patch and monitor a single operating system.

Also, because the operating system has already started on the host, a container can start up pretty quickly, usually in a second, sometimes less. And also, these containers don't need a slice of the hardware resources on the host, So we don't need to give them a specific number of CPU cores or a slice of memory or disk space. So on a single host, we can run tens or even hundreds of containers side by side. So these are the differences between containers and virtual machines.



Codes and Other Notes in this Discussion: 

Problems with VMs:
Each VM needs a full-blown OS
Slow to start 
Resource intensive


Containers: 
Allow running multiple apps in isolation
Are lightweight 
Use OS of the host
Start quickly 
Need less hardware resources 













Docker Architecture:

Let's talk about the architecture of Docker so you understand how it works. Docker uses a client server architecture, so it has a client component that talks to a server component using a restful API. The server also called the docker engine sits on the background and takes care of building and running docker containers. Now technically a container is just a process, like other processes running on your computer, but it's a special kind of process which we're going to talk about soon. Now as I told you, unlike virtual machines, containers don't contain a full blown operating system.

Instead, all containers on a host share the operating system of the host. Now more accurately, all these containers share the kernel of the host. What's a kernel? A kernel is the core of an operating system. It's like the engine of a car, it's the part that manages all applications as well as hardware resources like memory and CPU.

Every operating system has it's own kernel or engine and these kernels have different API's, that's why we cannot run a windows application on Linux because under the hood this application needs to talk to the kernel of the underlying operating system, okay? So, that means on a Linux machine we can only run Linux containers because these containers need Linux. On a Windows machine however, we can run both windows and Linux containers, because Windows 10 is now shipped with a custom built Linux kernel. This is in addition to the Windows kernel that's always been in Windows, it's not a replacement. So with this Linux kernel, now we can run Linux applications natively on Windows.

So on Windows, we can run both Linux and Windows containers. Our Windows containers share the Windows kernel and our Linux containers share the Linux kernel, okay. Now what about macOS? Well, macOS has it's own kernel which is different from Linux and Windows kernels and this kernel does not have native support for containerized applications. So docker on mac uses a lightweight Linux virtual machine to run Linux containers.

Alright, enough about the architecture, next we're going to install Docker, and that's where the fun begins.


Codes and Other Notes in this Discussion: 




















Installing Docker: 


All right, now let's install the latest version of Docker. If you have an existing version of Docker on your machine, I highly encourage you to upgrade to the latest version because your version might be old and not compatible with the version I'm using in this course. So here I'm using Docker version twenty point ten point five, okay? So to get Docker, go to this page, docs.docker.com/getdocker. Or you can just Google get Docker or install Docker.

Now on this page you can see instructions for downloading and installing Docker desktop for Mac and Windows as well as Docker engine for Linux. So on Mac and Windows we have Docker desktop which is the combination of Docker engine plus a bunch of other tools. At the time of recording this, we don't have Docker desktop for Linux, we only have the Docker engine. But of course that might change in the future. So a couple of notes for my Mac and Windows users, let's go to this page, so over here you can download the latest version from Docker Hub, but before doing this, I highly encourage you to go through system requirements and make sure your computer satisfies these requirements.

Otherwise, you might encounter weird issues. So the installation is pretty straight forward, when you go to this page, you're going to download a dm g file, just drag and drop this onto the applications folder, and then start it. This is very important, a lot of people miss that step. So when you start Docker engine, by double clicking on this application, you're going to see the Docker icon on the top status bar. If you don't see this, Docker engine is not running and there is nothing you can do, so you have to wait for this to start.

The same applies to my Windows users. So, back to this page, let's look at the instructions for Windows, so once again you can download the latest version from Docker Hub, but once again make sure to read system requirements. One of the things that is really important is enabling hyper v and containers windows features. So I don't have a windows machine to show you here, but just go to the page where you can turn on or turn off windows features, there make sure you have enabled hyper v and containers. It's pretty straightforward, but if you can't find it, just Google it, I'm pretty sure there are tons of articles out there.

Also, at the end of the installation, you might get an error saying ws two installation is incomplete. Basically, what this error is saying is that you need to upgrade the Linux kernel that is shipped with your Windows. So just click on this link, this is going to take you to this page on microsoft.com, where you can download the latest Linux kernel. So just click on this link to get an MSI file, run it and then you have to restart your computer. Now, once your computer is restarted, you need to wait a little while until Docker engine is started.

Depending on your machine, this might take several seconds or up to a minute. So wait a little while, then open up a terminal window and run docker version. So over here, you can see the version of the client and the server. If the server, which is docker engine is not running, you are not going to see this information. And this applies to all operating systems, Windows, Mac OS, and Linux.

So make sure Docker engine is running before going forward. If you encounter any errors, you can post your question on our forum at forum.codewithmosh.com, or if you want a faster response, just Google the error message, I'm pretty sure hundreds of people have encountered the same problem as you.



Codes and Other Notes in this Discussion: 

docker version

https://docs.docker.com/get-started/get-docker/























Development Workflow: 

Now let's talk about your development workflow when using Docker. So to start off, we take an application, it doesn't matter what kind of application it is or how it's built, we take that application and dockerize it, which means we make a small change so that it can be run by docker. How? We just add a docker file to it. A docker file is a plain text file that includes instructions that docker uses to package up this application into an image.

This image contains everything our application needs to run, everything. Typically, a cut down operating system, a run time environment like node or python, it also contains application files, third party libraries, environment variables and so on. So we create a docker file and give it to docker for packaging our application into an image. Once we have an image, we tell docker to start a container using that image. So a container as I told you is just a process, but it's a special kind of process because it has its own file system which is provided by the image.

So our application gets loaded inside a container or a process and this is how we run our application locally on our development machine. So instead of directly launching the application and running it inside a typical process, we tell docker to run it inside a container, an isolated environment. Now here's the beauty of docker, once we have this image, we can push it to a docker registry like docker hub. Docker hub to docker is like github to git, it's a storage for docker images that anyone can use. So once our application image is on docker hub, then we can put it on any machines running docker.

This machine has the same image we have on our development machine which contains a specific version of our application with everything it needs. So we can start our application the same way we started it on our development machine, we just tell docker to start a container using this image. So with docker, we no longer need to maintain long complex release documents that have to be precisely followed. All the instructions for building an image of an application are written in a Docker file, with that we can package our application into an image and run it virtually anywhere, this is the beauty of Docker. Next we're going to see this workflow in action.


Codes and Other Notes in this Discussion: 


Image: 

A cut-down OS
A runtime environment (eg. Node)
Application Files 
Third Party Libraries 
Environment Variables 

















Docker in Action: 

In this lesson, I'm going to walk you through a typical development workflow. Now don't try to memorize anything in this lesson, because I just want you to see the big picture. So everything I show you in this lesson, we're going to cover in-depth later in the course. So, here in this terminal window, I'm currently on my desktop, I'm going to create a new directory called hello docker, then go inside this directory and open it in visual studio code. So I'm going to use Versus code as my editor, but you can use any editor that you prefer, okay?

Now in this directory, I'm going to add a new file called app dot js. You're going to write one line of JavaScript code, you don't have to be a JavaScript developer, you don't even need to learn JavaScript. So just follow along with me. So here we're going to write console in lower case dot log hello docker. So with this we're going to print a message on the terminal.

Let's say this is an application, and we want to dockerize this application. So we want to build, run, and ship it using docker. So typically without docker, if you want to ship this application or more accurately this program to a different computer, on that computer we need to install node, and then we can go to the terminal and type node app dot js. So we get the output. So here are the instructions for deploying this program.

We need to start with an operating system, then we need to install node which is an execution environment for javascript code, next we need to copy our application files, and finally we need to run node app dot js. So we have to follow four steps just for a simple program. What if we were working with a really complex application? You would end up with a complex release document that had to be precisely followed. Now this is where Docker comes to the rescue.

We can write these instructions inside a Docker file, and let docker package up our application. So back to Versus code, we're going to add another file to this project called docker file, so capital d and all the other letters are lower case, and this file doesn't have any extensions. Okay, now, yes code is asking if you want to install the recommended extensions for Docker. We can go ahead with that, good, so back to this Docker file, here we write instructions for packaging our application. So typically we start from a base image.

This base image has a bunch of files, we're going to take those files and add additional files to it. This is kind of like inheritance in programming, okay? So what is the base image? Well, we can start from a linux image and then install node on top of it, or we can start from a node image. This image is already built on top of linux.

Now how do I know these names? Well, these images are officially published on docker hub. So if you go to hub.docker.com and search for node, you can see the official node image here. So docker hub is a registry for docker images. Now back to our Dockerfile.

So we start from a node image, now if you look at Docker hub, you will see that there are multiple node images. These node images are built on top of different distributions of Linux. So Linux has different distributions or different flavors used for different purposes. Now here we can specify a tag using a colon to specify which Linux distribution we want to use. For this demo I'm going to use alpine, which is a very small Linux distribution.

So the size of the image that we're going to download and build on top of is going to be very small, okay? So we start from that image, then we need to copy our application or program files. For that we use the copy instruction or copy command, we're going to copy all the files in the current directory into the app directory into that image. So that image has a file system and in that file system we're going to create a directory called app, okay? Now finally, we're going to use the command instruction to execute a command.

What command should we execute here? Node app dot js. But this file is inside the app directory, so we have to prefix it with the directory name. Alternatively, here we could set the current working directory, work dir to slash app, and then we don't need to prefix this with the directory name. So when we use this instruction, all the following instructions assume that we're currently inside the app directory, okay?

So these instructions clearly document our deployment process, now we go to the terminal and tell docker to package up our application. So, we say docker build, we need to give our image a tag, a tag to identify. So, dash t, here we specify a name like hello docker, and then we need to specify where docker can find a docker file. So we are currently inside hello docker directory and our docker file is right here, so we use a period to reference the current directory. Let's go with that.

Now you might be expecting an image file inside the current directory, but back in versus code, look, there is nothing here. Because the image is not stored here, and in fact, an image is not a single file. How Docker stores this image is very complex, and we don't have to worry about it. So back to the terminal, to see all the images on this computer, we type Docker images or Docker image ls which is short for list. So, take a look, on this machine we have a repository called hello docker, in this repository we have an image with this tag latest, so docker added this by default, we'll talk about this later in the course, but basically we use these tags for versioning our images.

So each image can contain a different version of our application, okay? Now, each image also has a unique identifier, here we can see when the image was created and the size of this image. So because we used node from Linux alpine, we ended up with 112 megabytes of data in this image. So this image contains alpine linux, node and our application files. And the total size is one twelve megabytes.

Now if we use a different node image that was based on a different distribution of Linux, we would end up with a larger image. And then when deploying that image, we would have to transfer that image from one computer to another. So that's why we use node alpine because this is a very small image, okay? So we have built this image, now we can run this image on any computer running docker. So on this machine, which is my development machine, I can say docker run, and then type the image name, hello, docker, and it doesn't matter which directory I'm in, because this image contains all the files for running our application.

Now, look, we see the message on the terminal. Now, I can go ahead and publish this image to Docker Hub, so anyone can use this image. Then I can go on another machine like a test or a production machine and pull and run this image. In fact, I've done this before recording this video. So back to Docker Hub, look, I have this repository code with Mosh slash hello Docker, now we can take this image and run it on any computer.

Let me show you something really cool. So let's search for play with docker, let's go to this page, and log in, here we need to sign in with our Docker ID, anyone can create this ID on docker.com. So, let's go ahead and start a lab. Over here we can start a new virtual machine, so let's add a new instance, now this virtual machine is a blank machine, it only has an operating system which is Linux and Docker. So in this terminal window, if we type node look, node command not found.

So node is not installed here. But because we have docker, we can pull and run the image that I published on docker hub. So let me maximize this window by pressing alt and enter. First let's run docker version, so on this machine we're running docker version 20.10.0. So to pull and run my program, first we type docker pull, code with Mosh hello docker, alright, docker downloaded this image, we can verify it by typing docker, what command should we run here?

Docker images or image els. So on this machine we have this repository, go to moshhello docker, and this repository contains a single image with this tag, latest. So now we can run this application exactly the same way we ran it on my development machine. So from any directory we can type docker run code with mosh slash hello dash docker. And here's the message, beautiful.

Of course, I had to cut this down in editing, it took a little while to start this application on this very slow virtual machine, but you got the point. So we can take any application and dockerize it by adding a docker file to it. This docker file contains instructions for packaging an application into an image. Once we have an image, we can run it virtually anywhere, on any machine with docker.




Codes and Other Notes in this Discussion: 

Instructions: 

Start with an OS
Install Node 
Copy app files 
Run node app.js


https://hub.docker.com/



Dockerfile:

FROM node:alpine
COPY . /app
WORKDIR /app
CMD node app.js


docker build -t hello-docker .


docker image ls 


docker run hello-docker


















Summary:

Terms Involved:
Client/server architecture
Containers
Dockerfiles
Docker daemon (engine)
Docker registries
Hypervisors
Images
Kernel
Process
Virtual machines



Summary
• Docker is a platform for consistently building, running, and shipping applications.
• A virtual machine is an abstraction of hardware resources. Using hypervisors we can create and manage virtual machines. The most popular hypervisors are VirtualBox, VMware and Hyper-v (Windows-only).
• A container is an isolated environment for running an application. It’s essentially an operating-system process with its own file system.
• Virtual machines are very resource intensive and slow to start. Containers are very lightweight and start quickly because they share the kernel of the host (which is already started).
• A kernel is the core of an operating system. It’s the part that manages applications and hardware resources. Different operating system kernels have different APIs. That’s why we cannot run a Windows application on Linux because under the hood, that application needs to talk to a Windows kernel.
• Windows 10 now includes a Linux kernel in addition to the Windows kernel. So we can run Linux applications natively on Windows.
• Docker uses client/server architecture. It has a client component that talks to the server using a RESTful API. The server is also called the Docker engine (or daemon) runs in the background and is responsible for doing the actual work.
• Using Docker, we can bundle an application into an image. Once we have an image, we can run it on any machine that runs Docker.
• An image is a bundle of everything needed to run an application. That includes a cutdown OS, a runtime environment (eg Node, Python, etc), application files, thirdparty libraries, environment variables, etc.
• To bundle an application into an image, we need to create a Dockerfile. A Dockerfile contains all the instructions needed to package up an application into an image.
• We can share our images by publishing them on Docker registries. The most popular Docker registry is Docker Hub.















The Linux Command Line: 

Introduction: 

Alright, the next stop in our journey is the Linux command line. But why Linux? What if you're a Windows user? Well, you still need to know a bit of Linux for a number of reasons. For starters, Docker has its foundations built on top of basic Linux concepts.

So if you want to be productive and troubleshoot issues easily, you need to know some of the basic Linux commands. Also, most tutorials online are based on Linux commands, so if you don't understand these basic commands, you're not going to get far. In my opinion, learning Linux is like learning English. I think everybody should know some English these days. You don't need to speak it or write a book in it, but you need to understand it.

So unless you're a power Linux user, do not skip this section. It's going to be super easy and extremely useful. So, let's jump in and get started.








Linux Distributions: 

Let's start off this section by talking about Linux distributions, also called Linux distros. So as you probably know, Linux is open source software, and for this reason, many individuals and communities have created their own version of Linux called Linux distributions. Each of these distributions is made to fit specialized needs like running servers, desktop computers, mobile phones and so on. So we have Ubuntu which is one of the most popular Linux distributions, we also have Debian, Alpine which we briefly talked about, it's a very small Linux distribution. We also have Fedora, CentOS and so on.

Believe it or not, there are more than 1,000 Linux distributions out there. Now, most of these distributions support pretty much the same set of commands, but sometimes you might discover differences along the way, so be aware of that. In this section, we're going to use Ubuntu Linux because it's one of the most popular distributions, But if you have a preference for another distribution, that's totally fine.


Codes and Other Notes in this Discussion: 

Distros:

Ubuntu
Debian 
Alpine 
Fedora
CentOS













Running Linux:

Alright, let's see how we can run Ubuntu on this machine. From this lesson, I want you to start taking notes, so watch this video, take some notes, and then after the video, repeat the steps I have shown you. Okay? So, we go to hop.docker.com and search for Ubuntu, over here you can see the official Ubuntu image that's been downloaded more then 10,000,000 times, let's have a quick look here, so for each image, you can see the command to pull that image onto your machine. Now in this lesson, I'm not going to use the pull command, I'm going to show you a shortcut.

So here in the terminal, instead of running docker pull ubuntu, I'm going to run docker run ubuntu. Now, if you have this image locally, docker is going to start a container with this image, otherwise, it's going to pull this image behind the scene and then start a container. So, take a look, see, docker is unable to find this image locally, and now it's pulling it from Docker Hub. Now what happened? Well, Docker started a container, but because we didn't interact with this container, the container stopped.

Let me prove this to you. So if you're on Docker ps, we can see the list of running processes or running containers. Look, we don't have any containers running here, but if you type docker ps dash a for all, we can see the stopped containers as well. Let me increase the size of this window so we can see it clearly, good. So we have two stop containers.

The first one is using the Ubuntu image, this is the one that we just started. And the second one is hello docker which we started earlier in the course. So to start a container and interact with it, we have to type docker run dash it, that is short for interactive, we're going to start a container in the interactive mode. And in this container we're going to load the Ubuntu image which we have locally. Good, now what we have here is called the shell.

A shell is a program that takes our commands and passes them to the operating system for execution. Okay? Now what we have here is called the shell prompt, let me break it down for you so it doesn't look mysterious. The first part, root represents the currently logged in user, so by default I'm logged in as the root user which has the highest privileges. Then after the at sign, we have the name of the machine.

So this container has this id which is automatically generated by Docker, and in this case, it's like the name of a machine, okay? And after colon, you can see forward slash, that represents where we are in the file system. A forward slash represents the root directory, that is the highest directory in the file system, we'll talk about that soon. Then we have a pound, and this means I have the highest privileges, because I've logged in as the root user. If I logged in as a normal user, instead of a pound, we would see a dollar sign, okay?

So in this shell, we can execute a bunch of commands, for example we can say echo, hello, and this prints hello on the terminal, we can also say who am I, this shows the current user, so this commands that we are running here, this shell program that I told you about takes this command and passes them to the kernel for execution. Now let me show you something really cool. If we type echo dollar sign zero, we can see the location of this shell program. So, that is forward slash bin slash bash. So bin is a folder or a directory, and inside this directory we have a program called bash, which is short for born again shell.

So apparently, Steve Bourne is the first person who created a shell program, bash or born again shell is a reference to Steve Bourne. So bash is an enhanced version of the original shell program, okay. Now one thing you probably notice is that in Linux, we use a forward slash to separate files and directories. But in Windows, we use a backslash. So that's one of the first differences.

The other difference is that Linux is a case sensitive operating system. So if you type echo with a capital e, it's not going to work. Bash tells us echo command not found. So lower case and upper case letters are different. And this is not limited to commands, it's applicable everywhere.

If you want to reference a file or a directory or a user, pretty much anything, we should always spell it properly with the right upper case and lower case letters. Now one last thing for this lesson, using the up and down arrows, we can go through the commands we have executed so far, so this is a pretty useful shortcut you need to know because you don't want to type these commands manually all the time. Also, using the history command, we can see all the commands we have executed lately. So, take a look, so earlier we used who am I, we also used echo and so on. Now we can replay any of these commands by typing an exclamation mark followed by the command number.

So if I type two, this is exactly like writing who am I. Now it's your turn, I want you to pause the video, go through your note and execute the commands I have shown you in this video.


Codes and Other Notes in this Discussion: 


docker run ubuntu 

docker ps 

docker ps -a 

docker run it ubuntu 















Managing Packages: 


These days, most operating systems and development platforms come with a package manager. You've probably worked with tools like npm, yarn, pip and so on. Here in Ubuntu, we also have a package manager called apt which is short for advanced package tool. So let's execute this command, look, this command has a bunch of sub commands. So using list, we can see the list of packages, we can also search for them, we can show details about the package, we can install, reinstall, and remove a package and so on.

Now technically, apt is the newer package manager, we also have apt get which you see a lot in online tutorials. Going forward, we're going to use apt because it's easier to work with. So let's say we're going to install a package called nano. Nano is a basic text editor for Linux. Now if you press enter here, we get an error because this image, this Ubuntu image we are running does not have nano.

So this is where we use apt to install this package. So if you type apt install nano, we get an error saying unable to locate package nano. Why is this happening? Well, here in Linux, we have a package database and this database might contain hundreds of packages, but not all these packages are installed. So if you want to see all the packages in this database, we type apt list, look, these are all the packages, now in front of these packages you can see some of them are installed.

But not all packages in this database are installed. When we type app install nano, this command looks at the package database, and in this database it cannot find a package called nano. So this is where we use the update command to update the package database. Now, let me press enter, then it's going to all these sources like security.ubuntu.com and all these other websites to download the list of packages. So now our package database is updated, so if you run apt list, we see far more packages.

And as you can see, most of these packages are not installed, because we don't have installed in front of them. So now we can run app install nano, and nano is installed. So here is what you need to take away. Before installing a package, you should always run app update, to update your package database, and then you can install a package like nano. Now we'll talk about nano later in this section, but before going forward, let's make sure that this package is installed properly.

So if you type nano, great. So here we have a text editor, we can type something, let me resize the window, so down below you can see the shortcuts, to exit, you have to press control and x. Now it's asking if you're going to save the changes, no, so we're going to press n, good. We're back here, we can clear the terminal window by pressing control and l, okay? So we have installed nano, now let's say we want to remove it.

So we type apt remove nano. It's asking for confirmation, let's go ahead, great. So nano is gone, if I type nano, we get this error saying no such file or directory. Now here's a little exercise for you. In this image we don't have python, so we get an arrow, so I want you to use app to install python in this image, make sure it works and then remove it.



Codes and Other Notes in this Discussion: 

Package Managers: 
npm
yarn
pip
NuGet


apt install nano 

apt list 

apt update 


apt remove nano 



















Linux File System: 

So in Linux, just like Windows, our files and directories are organized in a tree, in a hierarchical structure. So in Windows we have a structure like this, with c drive on top of the hierarchy, then below that we have directories like program files, windows and so on. In Linux, we have the root directory on top of the hierarchy. Below that we have a bunch of standard directories, for example we have bin which includes binaries or programs, we have boot which includes all the files related to booting. Then we have dev, the first time I saw this I thought this is short for development, it's not.

It's short for devices. So in Linux, everything is a file, including devices, directories, network sockets, pipes and so on. So the files that are needed to access devices are stored in this directory. Then we have etc, there are different opinions, what is this short for? But one common opinion is this is short for editable text configuration.

So this is where we have configuration files. We also have home, this is where home directories where users are stored. So on a machine with multiple users, each user is going to have a home directory here. We also have root, which is the home directory of the root user. Only the root user can access this directory.

Then we have lib, which is used for keeping library files like software library dependencies. We have var, which is short for variable, and this is where we have files that are updated frequently, like log files, application data and so on. And finally we have proc, which includes files that represent running processes, so once again, in Linux, everything is a file. Processes, devices, even directories are files. Now, you don't need to memorize any of these directories, I just listed them here so as we go through the course, these directories look familiar to you.

That's all. Next, I'm going to show you how to navigate the file system, and there you will see these directories one more time.









Navigating the File System: 

Let's see how we can navigate the Linux file system. So the first command we're going to talk about is pwd which is short for print working directory. With this we can see where we are in the file system. So a forward slash represents the root directory. Now to see the files and directories here, we type l s which is short for list, so we have bin which we talked about earlier, that is short for binaries, this is where we have binary files and programs, we have dev which includes files for devices, we have boot which includes boot files, etc which includes configuration files, and so on.

Now, by default, l s lists these items on multiple lines, if you don't like this layout, and want to show one item per line, you need to pass an option, that is dash one. You might prefer this layout. We have another option, dash l for seeing a long listing. This listing includes more details. So in the first column, we have the permissions of this file or directory, the first time you see this, it might look really scary, but trust me, it's easier then you think, we'll talk about that later in this section.

Over here, you can see the user that owns this file or directory, we can see the size, we can see the date and so on. Now to change the current directory, we use the c d command, we have the same command in Windows. Now here we can type a relative or an absolute path. A relative path is relative to where we are, so in this root directory, we have directories like bin, boot and so on. So this is a relative path.

Now in contrast, an absolute path always starts from the root directory. So let's go to a directory starting with e, now we can press tab to get auto completion, beautiful, now let's go to a directory starting with a. Now if you press tab, nothing happens, because we have multiple directories starting with a. So we have to press tab one more time, great, so we have three entries, three directories, add user dot conf, alternatives and apt. So let's type the second letter, p, and then press tab, beautiful, now let's see what we have in this directory.

So these blue items are directories and sources dot list is a file. Now to get out of this directory, we can go one level up by typing two periods. Again, I'm pretty sure you're familiar with this, but I want to cover it to make this section comprehensive. So we can go one level up to get to the etsy directory, or two levels up to get to the root directory. Look, we're currently in the root directory.

Now, when using l s, we can optionally specify a path. Let's say I'm somewhere in the file system, but I want to look at the content of another directory, I don't want to navigate to that directory. So I'm currently in the root directory, I want to know what files and directories we have inside the bin directory. So once again, we can type a relative path or an absolute path starting from the root directory. So here are the binaries in this directory, look at p w d, that's the command that we just executed.

So p w d is a program in this directory. Here's another example, look at echo. So most of the commands we have been exploring so far are programs or binaries in the bin directory, okay? Now let me show you a shortcut. Earlier I told you that here we have this home directory where each user has a home directory.

But the root user has a special home directory called root. Now to get here, there are two options. We can type an absolute or relative path and go to root, but there is a shortcut. So let me get out of this directory, so I'm currently in the root directory, to get to my home directory, I can type a tilde, and this applies to all users, not just the root user. Whenever we type c d tilde, we go to our home directory.

Now, right now, there is nothing here, but in the next lesson, I will show you how to create some files in this directory. So, before moving on to the next lesson, I want you to spend a couple of minutes and play with the commands we explored in this lesson. Navigate the file system, get adventurous, see what you can discover. I'll see you in the next lesson.




Codes and Other Notes in this Discussion: 

pwd
ls
ls -1
ls -l

ls /bin


cd /root

or 
cd ..
cd ~










Manipulating Files and Directories:

Let's see how we can manipulate files and directories. So I'm currently in the root directory, and I want to go to my home directory. How can I do that? Do you remember? We type c d, tilde.

Right? Now, in this directory I want to create a directory called test. So we type mk dir test, let's verify, it's right here, as I told you before, blue represents a directory. Now let's say we want to rename this directory, how do we do that? We use the move command, with this we can rename files and folders, or move them somewhere else.

So, we can move or rename test to docker. Alright, beautiful. Now, let's go in this directory, now to create a new file, we use the touch command. So we can create hello. Txt, look, we have this new file here, this is a new empty file.

In the next lesson, I will show you how to edit files. So for now, don't worry about it. Also, using the touch command, we can create multiple files in one go. So we can create file one, file two, and file three, now take a look, beautiful. Now I've got a question for you.

How do we list these files with a single file per line? Do you remember? We type ls dash one. Now let's say we want to rename hello dot txt to something else. Once again, we can use the move command, so we type h, press tab to get auto completion, we can rename this to hello dash docker dot txt, or we can move it to a different directory.

For example, we can move it to the etsy directory, and here I'm using an absolute path, because my path has started from the root directory. Now in this lesson, I don't want to move this anywhere, so I just want to rename this to hello dash docker dot txt. And if you're wondering how I remove this entire word in one go, I press control and w. So, let's bring it back, hello docker dot txt, take a look, good. Now let's see how we can remove one or more files.

To do that, use the r m command. Now here we can type one or more files, so we can say file one dot txt, file two dot txt, or we can use a pattern. For example, I can say I want to remove all files that start with five. Let's verify it, beautiful, we only have hello docker. Now, let's get out of this directory and remove the directory itself.

So, we type r m docker. Now we get an error saying docker is a directory. So to remove directories, we have to use the r option, which is short for recursive. So we want to remove this directory and all it's content recursively. Now we press d, tab, beautiful.

So, let's verify there is nothing here. So, let's quickly recap. We use mk dir to create a new directory, we use touch to create a new file, we use move to move or rename files or directories, and we use r m to remove files and directories. Now, as an exercise, go to your home directory, create a bunch of files and directories, rename them and then remove them. In the next lesson, I'm going to show you how to view and edit files.




Codes and Other Notes in this Discussion: 

mkdir test 
mv test docker 
touch hello.txt 
touch file1.txt file2.txt file3.txt 

mv hello.txt /etc 
mv hello.txt hello-docker.txt 

rm file1.txt file2.txt 
rm file*


rm -r docker/ 



















Editing and Viewing Files: 


Alright, let's see how we can edit and view files. So earlier we briefly talked about nano. Nano is a basic text editor for Linux. Now on this image that we are running, we don't have nano, so we have to install it. Do you remember the command for installing nano?

That is apt install nano. Alright, now we have nano, so we can launch it, and optionally supply a file name. Let's say file one dot txt. So here we have a basic text editor, we can type whatever we want, and when we are done, look down below, the shortcut for exiting is control and x. So I'm going to press that, now it's asking if you want to save the changes, so we press yes, and here we confirm the file name.

We can keep the same file or change the file name. Let's go ahead, so now in this directory we have file one dot txt. Beautiful. Now, to see the content of this file, we have a few different commands. The simplest one is cat, and this has nothing to do with cat, it's short for concatenate.

So later I will show you how we can use this command to concatenate or combine multiple files. But with cat we can also see the content of a file, so if you say cat file one dot txt, we can see the content of this file. Now, cat is useful if our file is short and fits on one page, but if you're dealing with a long file, it's better to use the more command. Let me give you an example. So, I'm going to use cat to show the content of this file.

Slash etc, slash add user dot conf. This is a really long file, so if I scroll up, look, we have a lot of text. Now, sometimes you don't want to see all the content in one go, you want to scroll down, you want to go page by page. This is where we use the more command. So more, slash etc,/adduser.com.

Now down below, look, you can see more 15%, so we are seeing 15% of this file. Now if you press space, we can go to the next page, so now we're at the position 33%, alternatively, we can press enter to go one line at a time. Now the problem with more is that we can only scroll down, we cannot scroll up. So to do that, we have to use a different command called less. So to exit here, we press q, good, now in this image we don't have less, so once again we have to manually install it.

So apt install less. So less is a newer command that is supposed to replace more. Let's go ahead, great. So let's look at the same file using the last command. Slashhc/adduser.com.

So now using the up and down arrows, we can scroll down or up. So just by pressing up and down arrows. We also have space, we can go to the next page, and enter, just like the more command, and when we are done, we can press q. Now we have a couple more commands for viewing the content of a file, we have head which we can use to display the first few lines, so here we can supply an option and say the number of lines we want to see is five. Let's look at the same file, so this shows the first five lines of this file.

Similarly we have tail, which shows the last few lines, so we supply an option and say we want to look at the last five files. That's it. So, to recap, we can use nano to write something to a file, we can use cat to view the content of small files, less to view the content of long files in an interactive way, and head and tail to view the first few or the last few lines of a file.



Codes and Other Notes in this Discussion: 


apt install nano
nano file1.txt 

cat file1.txt 

more /etc/adduser.conf 

less /etc/adduser.conf 

head -n 5 /etc/adduser.conf  

tail -n 5 /etc/adduser.conf












Redirection: 

One of the important concepts in Linux is the concept of standard input and output. So standard input represents the keyboard and standard output represents the screen. But we can always change the source of the input or the output, this is called redirection. Let me show you using a few examples. So we talked about the cat command to see the content of a file.

Let's say file one dot txt. When we execute this command, this command or this program reads data from this file and prints it on the standard output which is the screen. That is why we see the content here. But using the redirection operator, which is the greater than sign, we can redirect the output from the screen to let's say a different file. So now, cat will read the content from this file and write it to this file.

Take a look. So, we have file two dot txt, if we view it, we see the exact same content as file one, okay? Now what is the point of this? Well, earlier I told you that we can use the cat command to concatenate or combine multiple files. So here we can say cat file one dot txt, and file two dot txt, if you press enter, cat is going to read the data from both these files and print it on the terminal, which is the standard output.

But once again, using the redirection operator, we can write the result to a different file. Combine dot txt. So this is how we can use this command to combine multiple files. Now, the redirection operator is not limited to the cat command, we can use it pretty much anywhere. For example, earlier we talked about the echo command.

If we say echo hello, we see the result on the terminal. But if we say echo hello to hello dot txt, Now we have a new file here, hello. Txt which contains hello. So if you want to write a single line to a file, we don't have to use nano, we can use the echo command. So echo whatever to whatever file dot txt, okay?

Now here's a little exercise for you. I want you to get a long listing of the files in the etsy directory, and write the output to a file. So pause the video and work on this for a few seconds. So, here's the solution. To get a long listing, we type ls dash l, then we specify the path, the etc directory, now instead of printing the result on the terminal, we're going to write it to a file called files dot txt.

Okay? Now let's view this file, perfect. Now one more thing before we finish this lesson. Using the greater than sign, we can redirect the standard output, but we also have the less than sign to redirect the standard input. I personally haven't found many use cases for this, so I didn't cover it in this lesson.

Alright, that's all about redirection, next we're going to talk about searching for text in files.




Codes and Other Notes in this Discussion: 

cat file1.txt > file2.txt 
cat file1.txt file2.txt > combined.txt

echo hello > hello.txt 


ls -l /etc > files.txt 





















Searching for Text: 


Let's see how we can search for a string in a file. So we have this grep command which is short for global regular expression print. I know, it's such a mouthful. But let's say you want to search for the word hello in file one dot txt. What happened?

Didn't we write hello to file one dot txt? Yes we did, but this search is case sensitive, just like everything else in Linux. If you want to remove case sensitivity, we have to use an option, that is dash I. So case insensitive, okay? Now we see the word hello highlighted in red.

Let's look at another example, this time I want to search for the word root int slash etsy slash password. This file contains the list of user accounts on this machine, not their passwords, just their accounts. So, take a look, on this line, which I think is the first line in this file, we have three occurrences of root, and that's why they are displayed in red. Now we can also search in multiple files. So, let's search for hello in file one dot txt and file two dot txt.

That's one way, we can also use a pattern. So we can say we want to search in all files whose name starts with file. So we use a wild card here. Now we can see in file one dot txt we have hello and the same is true in file two dot txt. Now we can also search in a directory, so instead of typing a file name, we can type a directory name, or we can use a period to refer to the current directory.

Now we get an error saying period is a directory, so here we have to use an additional option dash r which is short for recursive, so with this we can search this directory and all its sub directories recursively. Now look, in all these files, we have references to hello. Now one last thing, in Linux, we can combine multiple options. So instead of having two different options dash I and dash r, we can combine them into one option. This gives us the exact same result.

So using the grep command, we can search for a string in one or more files.


Codes and Other Notes in this Discussion: 

grep hello file1.txt 
grep -i hello file1.txt 

grep -i root /etc/passwd 


grep -i hello file1.txt file2.txt 
grep -i hello file*

grep -i hello /etc
grep -i -r hello .

grep -ir hello .





















Finding Files and Directories: 


Let's talk about finding files and directories. So I'm currently in my home directory, as you can tell from the tilde, let's run ls, so here we have combined file one, file two, files and hello dot txt. Now of course on your machine, you might have different files, it doesn't matter. Now, in Linux we have the find command for finding files and directories. If we execute this command without any arguments, we see all the files and directories in the current directory recursively.

So this command is going to go through every directory in this directory and list their files. So on the top we have dot bash r c and dot profile, these are two hidden files in the current directory. You didn't see this earlier when I executed ls, because by default, ls doesn't show the hidden files and directories. Now, what options should we use to see the hidden files and directories? Do you remember?

Dash a, which is short for all. So now we can see bash r c, profile, and local which is a directory because it's blue. So back to the find command, here you can see all the files and directories in the current directory. Now, if you want to look somewhere else, we can supply a path. So we can search in the etsy directory.

Now we can see all the files and directories starting from etsy, okay? Now, let's get back to the current directory, let me show you various ways to filter the result. If you want to see only the directories, you pass an option called type with the value of d. Now you can see only the directories in the current directory. Alternatively, we can search for files, we can also filter by name, so let's say I want to find all the files whose name start with let's say f, here in double quotes, we can type a pattern, we can say f star.

Now just remember that this search is case sensitive just like everything else in Linux, so if you type a capital f here, we don't find anything. Now to make the search case insensitive, instead of name we use a different option that is iname. Alright, beautiful. Now here's a little exercise for you. Using the find command, I want you to find all the python files in this image and then write the result to a file called Python files dot txt.

So pause the video, work on this for a few seconds, then come back and see my solution. Alright, so using the file command, we're going to look at all the files starting from the root directory, now we're going to supply an option, type f, so we only look at files, we're going to supply a second option for searching by name, here I'm going to type a pattern like star dot py, because all Python files have the py extension. Now if you didn't know this, don't worry, it doesn't really matter, So this way, we can find all the Python files in this image. Let's verify it, beautiful, now to write the result to a file, we're going to use the redirection operator. So, we can write it to a file called python files dot txt.

Good, now let's verify that everything is working, beautiful.



Codes and Other Notes in this Discussion: 

find 

ls -a  -> to show the hidden files 

find /etc

find -type d 

find -type f 

find -type f -name "f*"

find -type f -name "F*"

find -type f -iname "F*"


find / -type f -name "*.py" > python-files.txt 



















Chaining Commands: 

In Linux, we have a few different ways for chaining or combining multiple commands. So once again, I'm in my home directory, let's say I want to create a directory called test, and right after this I want to go into this directory and echo something to the terminal. Now I don't want to press enter here and then type the second command, I want to type multiple commands and execute them in one go. So here we type a semi colon, and then type the second command which is c d test. Now once again, we type a semi colon, and type the third command.

So echo, done. Now if I press enter, all these commands will get executed one after another. Now, some people prefer to add a space around these semicolons to make their command sequence more clear, more readable, that's optional, it's entirely up to you. So look, we see the done message, and we're currently inside the test directory. Now, if we go one level up, and execute the last command, we're going to get an error because we already have the test directory.

But the other two commands will get executed. Take a look. So, look, here's the error saying the test directory exists, but we also see the done message and we're currently inside the test directory. Now what if you want to stop execution? So if one command fails, the other commands are not going to get executed.

Well, this is where we use the and operator. So let me go up and clear the screen, now instead of a semi colon, we're going to use double ampersands. That is the and operator. So, if this command fails, the other commands will not get executed. Take a look.

Alright, we saw an error saying the directory exists, but we're still inside the home directory, we didn't go inside the test directory. Now we also have the or operator, so if we say create the test directory, or echo directory exists. If this command gets executed, this command will not get executed, but if this command fails, this command gets executed. Make sense? So essentially it's saying create this directory or print directory exists.

Let's try it, so we got this message because the directory already exists. But if I bring up this command and change the directory name to test two, this time, we are not going to see this message because we can successfully create this directory. There you go. So these techniques are very powerful, you'll use them a lot when it comes to deploying your applications using Docker. You'll see more examples in the future.

Now, another way to chain commands is piping. And this is extremely powerful. So, let's look at the content of the bin directory, so here we have a long list of files, but what if you want to look at this list using the less command. So, remember, earlier we talked about the less command, with this we can look at the content of a file, and then we have the ability to scroll up and down or go line by line and then quit. This is where we can use piping.

So, we use the ls command, get the content of the bin directory, and then create a pipe. So we get the output of this command and then send it to the last command. So essentially we're creating a pipe, what comes out of this command goes into the second command. So now less doesn't need a file name because it gets the input from the first command. Take a look.

So now, we are seeing the output of the last command, using the up and down arrows, we can scroll, using a space, we can go to the next page, and then we can exit using q. We can also use the head and tail commands here, pretty much any commands. So, if we use head, by default we can see the first 10 lines, we can supply an option and see the first five lines. Pretty useful. Now, one last thing before we finish this lesson, sometimes when dealing with a long command, our command sequence might look a little bit hard to read, so let me show you how you can split it into multiple lines.

So, I'm going to create a directory called hello, then I'm going to go into this directory and echo done. Now this is a fairly short command, but imagine this was really long and we had to scroll horizontally. We don't want to do that. So we create a directory, and then terminate this command using a semi colon, now if we type a backslash and press enter, we go to the second line, and the command prompt changes, so we can type the rest of the command here. Now we're going to go to the hello directory, then terminate this command and add a backslash, so on the third line, we can echo, done.

And of course, we can keep going as much as we want. So this is how we can break up a long command into multiple lines, by using a back slash. There you go.




Codes and Other Notes in this Discussion: 

mkdir test;cd test;echo done

mkdir test && cd test && echo done  

mkdir test || echo "directory exists"


ls /bin | less 
ls /bin | head -n 5 

mkdir hello;\
cd hello;\
echo done; 













Environment Variables: 
Let's talk about environment variables. This is one of the areas that a lot of people are confused about. So just like we have variables in our programming languages, in Linux, we have environment variables which we can set for storing configuration settings for our applications. So our applications can read configuration settings from these environment variables. In this lesson, I'm going to show you a few different commands for viewing all these environment variables and setting them.

So the first command is print env, with this we can see all the environment variables in this machine. Take a look. So, let me scroll up, alright, so here we have a bunch of key value pairs, separated by an equal sign, the first variable is host name, which is set to the id of our container. And as you know, this is generated by Docker automatically. Then we have p w d, home, l s colors and so on.

Now down below, we have a very important environment variable called path. Have you noticed that sometimes when you run a program from the command line, you get an error saying the program or the command was not found, even though you have installed that program. Quite often that happens because your operating system whether it's Linux or Windows cannot find that program. Now, here's the interesting part, to find a program, your operating system is not going to go through your entire hard drive, it's only going to look at specific directories. And those directories are specified using the pass variable.

So this is set to a list of directories separated by colon. So here's the first directory, then we have a colon, here's the second directory and so on. These are the directories that Linux or Windows searches for to find a program or a command, okay? Now, what if you want to see the value of a particular variable? We can type print m and then path.

That's another way. Now, once again, everything is case sensitive here, so if I type path in lower case, we're not going to see anything, okay? Now there is another way to see the value of an environment variable. Instead of print m, we can use echo, but here we have to prefix the variable with a dollar sign. So Linux knows that, we are referring to an environment variable, like path.

Now how can we set a variable? Using the export command. So let's define a variable called db underline user, and I'm going to set that to MOSH. Now this variable is stored in the current session, in the current terminal session. So we can read it using the echo command, db user, we can also read it using print env, however, this variable is only available in the current terminal session.

So if I close this terminal session, and open a new terminal session, this variable will not exist. Let me show you. So we can terminate this session by typing exit, now we're outside of our container environment, so we're back on my mac terminal. Now, let's run docker p s to see all the docker processes or containers, there is nothing here, our container is stopped. Now to see all the containers, including stopped containers, we type docker ps dash a, that is short for all.

So, let me expand this window, so we can see clearly, so we have two stuffed containers, one was running our Ubuntu image, and the other was running hello docker. This container was the one that we have been working with throughout this section. Now I want to start this container so we can work with it again. So just like virtual machines, we can stop or start a container, okay? So look at this container ID, 2F whatever, I'm going to grab the first few letters, 2F7.

So to start this container, we type docker start, we use dash I so we can interact with it, and then type the container id. Quite often we can type only the first two or three letters, unless there is another container whose id starts with the same sequence of characters. Then we have to type more characters, okay? So, let's start it again, we are back in the same Linux machine, and if we type echo db underline user, you don't see anything. Because that variable does not exist anymore, it was only available in that terminal session.

Now, to make it persistent, we have to write it to a special file. So, we go to our home directory, now we list all the files, including hidden files, earlier you saw this file, bash r c, this file is a user's personal start up file. So every time a user logs in, Linux loads this command from the user's home directory. So this is where we have to write permanent environment variables. Now we can edit this using nano, so nano dot bash r c, but let me show you another technique.

Earlier we talked about redirection. So using echo, I want to write something to this file. I want to set db underline user to Mosh, and then I want to redirect the result to dot bash r c. Don't execute this yet. Because if you do this, you will overwrite the entire bash r c file.

We don't want to overwrite it here, we want to append something to it. So we add another greater than sign, and this will append db user to bash r c. Okay? Now to verify our work, let's look at bash r c, so take a look, the last line contains our permanent environment variable. And of course we can always come back and change this value.

Now, one thing I want to emphasize here is that you should never store sensitive information in environment variables. Because at the end of the day, these variables are stored in plain text files. So we don't want to store d b password here, because anyone who has access to this machine can read the d b password. So just be aware of this, I think most software engineers have made this mistake at least once in their career, and I'm no exception. So, we have set db user permanently in this file, now if we exit, and open a new terminal session, we can still read the value of that variable.

So, let's look at all the stop containers, here's our container, we're going to start it one more time. Docker start dash I two f seven. We're back here, so let's echo db user. Beautiful. Now one last thing before we finish this lesson, the changes we make to bash r c file are only effective in the next terminal session.

So if you write another environment variable here, that variable is not going to be available until we open another terminal session. Let me show you. So, let's go to my home directory, and echo, let's say color equals blue, we're going to write that to dot dash r c file. Now if I echo color, look, that variable is not there, even though we wrote it to this file. Because this file is loaded only once when we start a terminal session.

So now we have two solutions, we can terminate the session and come back in, or we can use the source command to reload the bash r c file. So, source dot bash r c. And we have to execute this from our home directory. If you're not in our home directory, we can type source home directory slash dot bash rc. Good, now let's echo color, and here's the result.

Beautiful.



Codes and Other Notes in this Discussion: 

printenv 

printenv PATH 

echo $PATH 

export DB_USER=mosh 
echo $DB_USER
prinenv DB_USER



docker ps -a 
docker start -i container-id



nano .bashrc 
or 
echo DB_USER=mosh >> .bashrc 


echo COLOR=blue >> .bashrc 
echo $COLOR 


source .bashrc 
source ~/ .bashrc 












Managing Processes:

Let's talk about processes. Do you know what's a process? A process is an instance of a running program. To see all the running programs or all the running processes, we can use the ps command. So, we have two processes, one is running bash, the other is running ps.

Now technically ps was a very short lived process, it only existed while this command was producing this output. So now it doesn't exist anymore, however, if we run ps one more time, we see ps again, because once again, while ps was preparing this command, it was being executed, okay? So technically the only process that is running right now is bash. Do you remember bash? Bash is short for born again shell.

It's the program that we're interacting with here. The program that takes our commands and sends them to Linux for execution. That's bash. Now as you can see, each process has a unique identifier that is generated by the operating system. Now what is tty?

That is short for teletype. And over here we can see the type of terminal the user is logged into. So p ts is short for pseudo terminal, don't worry about what it really means, there is some long history, but a short explanation is that this kind of terminal window that we are using, this is called a pseudo terminal. So pts slash zero, that represents the first terminal window. So if I open another terminal window and start this container and then execute the same command, we are going to see pts slash one.

So in this column, we can see which terminal the user is logged into, okay. Now, time is the amount of CPU time each process consumed, so both these processes are very lightweight, they're not taking much of CPU time. But sometimes you notice your system getting slow, that's because some process is taking so much of CPU time. In that case, you want to kill that process. Let me show you how to do that.

First, I want to create a process and put it in the background. So in Linux we have the sleep command, if I type sleep three, now the prompt sleeps for three seconds and then wakes up. Now, what if you want to put this in the background so we can execute other commands. We append an ampersand here. Now this is in the background, and we can execute other commands.

So let's run sleep with a longer wait, let's say one hundred seconds, and we want to put this in the background. Now, if we run ps, we can see the sleep command, as we can see, this command is not taking much of CPU time, so it's a light process. But let's say we want to kill this. This is where we use the kill command, and here we specify the process id for the bad boy. So that is 38.

Now that is gone, so let's run ps one more time, that process is terminated. Beautiful.




Codes and Other Notes in this Discussion: 

ps 

sleep 100 &

kill 38 



















Managing Users: 


Alright, let's talk about managing users. So in this lesson, I'm going to show you how to create a new user and then log in as that user. It's going to be fun. So in Linux we have this command user add for adding a new user, we also have user mod for modifying a user and user del for deleting a user. So, we're going to call user add, now let's look at the options.

Here we have a bunch of options and none of them are mandatory. We can use them based on our needs. In this lesson, I'm going to use an option, this one, each option as you can see has two forms. The short form with one hyphen and the long descriptive form with two hyphens. Different people have different preferences.

So with this option, we can create the home directory for this user. So we're going to say user add dash m and the name of the user is John. Okay, where is this user? Well, this user is stored in a configuration file in the etc directory. So using cat, we can look at that, cat slash etc slash password.

Now, the name is misleading, passwords are not stored here, we only have user account information. So, take a look. So, down below, look over here, we have John, so we have multiple fields separated by colon. First we have the username, then we have a colon as a separator, x means the password is stored somewhere else. I'll show you that in a second.

Then we have this field, that's the user id, now on your machine you're probably going to see 1,000, because prior to recording this video, I created another user, Mosh, and the id of that user is one thousand. So John's id is one thousand and one, then we have the group id, we'll talk about this in the next lesson. Over here we have the home directory of this user, so slash home slash john. And then we have the shell program used when this user logs in. So slash bin slash shell represents the old original shell program.

But we also have bash which is born again shell which is the enhanced version of this program, okay? Now let's say when I log in, instead of using shell, I want to use bash. So we're going to use user mod to modify this record. So, let's look at user mod, the option that I'm going to use in this lesson is dash s or shell, this is for setting the shell for this user. So, we're going to say user mod dash s, now which shell are we going to use?

Slash bin slash bash. Which user? John. Okay, now let's look at etc slash password one more time, so, look over here, we have John's record and his shell program is bin slash bash. Beautiful.

Now where are the passwords? Well, in the same directory, we have another file that is shadow. So this is where passwords are stored in encrypted format. I don't know how this really works, but just be aware of this file, this file is only accessible to the root user. Okay?

Now let's say we want to log in as Mosh, how do we do that? So we open up a new terminal window to log into our container as John. First we run Docker ps, to see the running containers, so here's our container id, now we're going to execute a bash session inside this container. So we type Docker execute, then we paste the container id, or we can type the first few letters, okay? Now, you want to run a bash session here.

Now if I press enter, nothing happens, because we didn't interact with bash. So we have to specify dash it interactive, okay? Now if we run this, we'll log in as root, so now we have two back sessions, one is in this window, the other is in this other window, okay? Now, we don't want to log in as root, we want to log in as John. So let's terminate this session one more time.

You're going to execute a back session in this container, but before the container name, we're going to supply another option for specifying the user. So dash u, which user? John. Now, I've logged in as John, you can see that, so John at and here's the container id which acts like the host name, now look at the prompt, here we have a dollar sign which means I'm not a root user, I'm just a regular user. In contrast, in this other window, where we logged in as root, we have a pound.

So here we have extra privileges. So, back to John's Reno, let's see what happens if I try to access the shadow file. So, cat slash etsy slash shadow. We get a permission denied error. So this verifies that I'm not a root user, okay?

Also, John has a home directory, so if you type c tilde, we go to John's home directory, let's look at the path for this directory, so that is home slash John. So in this directory, we can store John's files. And when we are done, we can remove John by typing user dell John. Now I'm not going to delete John, because in the next lesson we're going to add John to a group. So we're going to keep him for now.

Now one last thing before we finish this lesson. Back to our first window, with the root user, we talked about user app. Now we also have another command that is add user. What is the difference? Well, user app is the original API that was built, but add user is a pro script that is more interactive and uses user ad under the hood.

Let's look at it real quick. So I'm going to execute add user and say Bob. So, let's see what's going on here. We see a few messages, adding new user Bob, adding new group Bob, so every user that is created is automatically placed inside a group with the same name. So here we can see the group was created and then this user was inserted in this group.

Next we can see the home directory was created, and some files were copied, I don't really understand what that means, but here we have a chance to set the password. So this is what I meant by this command is more interactive then the old command. So here we can set the password and then confirm it, and then we can specify additional information about this user. For example, we can specify the full name, we can specify the room number, work phone and so on. So this is the difference between add user and user app.

Quite often, when using Docker for deploying our application, we don't want to use add user because we don't want to interact with this command. So we want this command to be executed under the hood. So be aware of the differences between these two commands. Next we're going to talk about managing groups.



Codes and Other Notes in this Discussion: 

useradd 
usermod
userdel 

useradd -m john 

cat /etc/passwd 


usermod -s /bin/bash john 

cat /etc/shadow 


docker exec -it container-id bash


docker exec -it -u john container-id bash 


adduser bob 














Managing Groups:

So we created a new user in the last lesson, now let's talk about managing groups. So in the last lesson we talked about three commands that start with user, we have user add, user mod and user del, we have similar commands for managing groups, so we have group add, group mod and group del. So let's add a new group called developers, now why do we need groups? You probably know this but we use groups so all users in the same group have the same kind of permission, okay? So now we have a new group, where is this group?

It's in a file in the etc directory. So, cat etc group. Here are all the groups in this image. So down the bottom, you can see the group of developers and this is the id of this group, 1003. Now we want to add John to this group.

How to do that? Let's bring up user mod, so here we have an option for setting the group, here it is, again we have a short form with a capital g, and a long form that says groups. With this we can set the supplementary groups for this user. But we also have another option with a lower case g or gid, that is for setting the primary group of the user. So what is the difference?

Well, every linux user has one primary group and zero or more supplementary groups. Now why are these groups separated? Well, let's say John is part of five groups, and now he wants to create a new file. Every file, as I will show you in the next lesson, is owned by one user and one group. Here's a question, if John is part of five groups, which group should we use for owning that new file that John is going to create?

That's why we need a primary group. Now the primary group is automatically created when we create a new user. It's the group with the same name as the user. So here, I'm going to use this other option to set the supplementary groups for John. So, we say user mod dash capital g, now here we type the group name, that is developers, and then we add John to this group.

Great, now let's look at the password file one more time, so cat etc password, now what if we only want to see the record for John? We can just pipe in again. So, here we can use grep and search for John. That's one way, another way is to use grep and search for John in etcpassword. So, in this record, I told you that this is the user id for John, and this is the group id, this is the id of John's primary group.

Now where the secondary groups are stored I can't recall on top of my head, and it doesn't really matter. We actually have a command for seeing the groups of a user. We type groups and then the name of the user, so John. So John is currently part of two groups. One group is John, which is John's primary group, and the other one is developers which is a supplementary group, okay?

Now of course we can add John to more supplementary groups, and that's what I want you to do as an exercise. So pause the video, add John to a new group called artists. In the next lesson we're going to talk about file permissions.




Codes and Other Notes in this Discussion: 

groupaddd developers 
cat /etc/group 

usermod -G developers john 

cat /etc/passwd  | grep john 


groups john  



















File Permissions:

Alright, the last thing we're going to talk about in this section is file permissions. This is one of those areas that a lot of people find confusing or complicated, it's actually very easy. Let me make it super simple for you. So I've logged in as root, and I'm in my home directory, let's go to the home directory, and in this directory I want to create a file called deploy dot sh. Files with this extension are called shell scripts.

In this file, we can write any of the linux commands we have learned so far. So we can combine all these commands and create a deployment script. Okay, so to write to this file, I'm going to use the echo command, I'm going to write echo hello to deploy dot sh. So I'm writing this line to this file. Let's go ahead, and now let's verify it, so we have echo hello, so when we execute this file, we're going to see hello on the terminal, okay.

Now to see the permissions for this file, we have to get a long listing. Take a look, so in this directory we have three directories, Bob, John and Mosh and one file. Now in the first column, we can see the permissions for each item. Now look at the first letter, if the first letter is d, that means this is a directory, if it's a hyphen, that means this is a file. Pretty straightforward.

Now what about the other letters? Well, these letters are actually nine letters divided into three groups. Here's the first group, then we have the second group and the third group. In each group we have read, write and execute permissions. So for this item, for deploy dot sh, for this file we only have read and write permissions, but the execute permission is missing because we have a hyphen.

In contrast, look at the permissions for this directory. We have read, write and execute. Execute is used so we can go into that directory, so we can use the c d command with it. So by default, all directories have the execute permission. Now, in the second group, we only have the read permission, and this is true for the third group.

But what are these groups for? Well, the first group represents the permissions for the user who owns this file. That is the root user here. The second group represents the permissions for the group that owns this file, and that is this group. So by default, every user that is created is automatically placed inside a group with the same name, okay?

Now, the third group represents permissions for everyone else. Now, to execute this file, we have to type period, referring to the current directory, then forward slash deploy dot sh. We get a permission error because even I as the root user do not have to execute permission on this file. This is where we use the change mode command. So, we type ch mod, and here we can change the permissions for the user that owns the file, or the group, or others.

For the user, I'm going to add the execute permission. If I want to remove the execute permission, I would have to use a hyphen, okay? So we're going to add the execute permission to where? To deploy that sh. Now, let's get a long listing one more time, look at the color of this file, it turned green because now it's executable.

Now look at the first group, we have read, write, and execute. So, I can call deploy dot sh and we see hello on the terminal, beautiful. Now, with this setup, the root user is the only user who can execute this file. So if you go to this other window where I've logged in as John, we're currently in John's home directory, so let's go one level up, now we are inside the home directory of all users, and over here we have the deployment script. But if we try to execute this file, we get a permission error.

So how do we enable John to execute this file? Well, back to our first window, we're going to use chmod one more time. This time we're going to apply the permission for others, we we want to enable the execute permission to deploy dot sh. Good, now, back to John's window, we can call deploy dot sh and we see hello on the terminal. Beautiful.

So this is how we can change permissions for files. Now, let's talk about variations of this command. So, we can also combine groups, so we can say o g, that means others and the group that owns this file, it's not short for original gangster, so for others and the group owner, we can add the execute permission, we can also add the write permission, and we can remove the read permission. So this is the syntax to use this command. And of course, we don't have to type a single file name, we can type multiple file names or use patterns for example, we can apply these permissions to any files with the shell extension.

So we're done with this section, I hope you have learned a lot. With the foundation that you have built in this section, now you're ready to learn docker the right way. So, I will see you in the next section.


Codes and Other Notes in this Discussion: 

echo echo hello > deploy.sh 

cat deploy.sh 

ls -l 

chmod u+x deploy.sh 


chmod o+x deploy.sh 

chmod og+x+w-r deploy.sh 
chmod og+x+w-r *.sh 
   
   
   
   
   
   
   
   
   
Summary: 

The Linux Command Line

Managing Packages
apt update
apt list
apt install nano
apt remove nano


Navigating the file system

pwd # to print the working directory
ls # to list the files and directories
ls -l # to print a long list
cd / # to go to the root directory
cd bin # to go to the bin directory
cd .. # to go one level up
cd ~ # to go to the home directory   



Manipulating files and directories

mkdir test # to create the test directory
mv test docker # to rename a directory
touch file.txt # to create file.txt
mv file.txt hello.txt # to rename a file
rm hello.txt # to remove a file
rm -r docker # to recursively remove a directory




Editing and viewing files

nano file.txt # to edit file.txt
cat file.txt # to view file.txt
less file.txt # to view with scrolling capabilities
head file.txt # to view the first 10 lines
head -n 5 file.txt # to view the first 5 lines
tail file.txt # to view the last 10 lines
tail -n 5 file.txt # to view the last 5 lines 




Searching for text

grep hello file.txt # to search for hello in file.txt
grep -i hello file.txt # case-insensitive search
grep -i hello file*.txt # to search in files with a pattern
grep -i -r hello . # to search in the current directory




Finding files and directories

find # to list all files and directories
find -type d # to list directories only
find -type f # to list files only
find -name “f*” # to filter by name using a pattern




Managing environment variables

printenv # to list all variables and their value
printenv PATH # to view the value of PATH
echo $PATH # to view the value of PATH
export name=bob # to set a variable in the current session




Managing processes

ps # to list the running processes
kill 37 # to kill the process with ID 37





Managing users and groups

useradd -m john # to create a user with a home directory
adduser john # to add a user interactively
usermod # to modify a user
userdel # to delete a user

groupadd devs # to create a group
groups john # to view the groups for john
groupmod # to modify a group
groupdel # to delete a group





File permissions

chmod u+x deploy.sh # give the owning user execute permission
chmod g+x deploy.sh # give the owning group execute permission
chmod o+x deploy.sh # give everyone else execute permission
chmod ug+x deploy.sh # to give the owning user and group
					 # execute permission
chmod ug-x deploy.sh # to remove the execute permission from
					 # the owning user and group 
					 
					 
					 
					 
					 
					 
					 
					 



Building Images: 

Introduction: 
The first step in using Docker to build and deploy applications is creating images. So having a solid understanding of Docker images is crucial for you, and that's what this section is all about. We'll be talking about creating Docker files, versioning images, sharing them, saving and loading them, and a few optimization techniques for reducing the image size and speeding up builds. I'm so excited about this section, I hope you are too, so let's jump in and get started.

In this Section: 
Creating Docker files 
Versioning Images 
Sharing Images 
Saving and Loading Images 
Reducing the Image Size 
Speeding up builds 












Images and Containers: 


Alright, before we get started, let's make sure you have the right understanding of images and containers. Can you define what an image is and how it's different from a container? Pause the video and think about it for a few seconds. Here's the answer, an image includes everything an application needs to run. So it contains a cut down operating system like Linux or Windows, it also contains third party libraries, application files, environment variables and so on.

So an image contains all the files and configuration settings needed to run an application. Once we have an image, we can start a container from it. A container is kind of like a virtual machine in the sense that it provides an isolated environment for executing an application. And similar to virtual machines, we can stop and restart containers. Now technically, a container is just an operating system process, but it's a special kind of process because it has its own file system which is provided by the image.

Now, let me show you something interesting. In the previous section, we started a container from the ubuntu image, right? Now I'm going to open up another terminal window, let's run docker ps to see the running processes or running containers. So here's the container that we started in the previous section. Now I'm going to start a new container from the same image.

Let's see what happens. So do you remember how to start a container from an image? We type docker run, we want to run this in the interactive mode so we can work with it, and then we type the name of the image, so ubuntu. Alright, here's the container id, and as you can see, this is different from this other container. Now, back to our first container, I'm currently inside the home directory, so in the previous section we created a bunch of directories and this deployment file, now back to our new container, let's go to the home directory and see what is here.

There is nothing here. Here's the reason, a container gets it's file system from the image, but each container has it's own right layer. So what we write in a given container is invisible from other containers. Of course, there is a way to share data between containers, and we'll talk about that later in the course. But what I want you to understand here is that each container is an isolated environment for executing an application.

It's an isolated universe. So whatever happens inside that universe is invisible to other containers, okay? Now that you have a proper understanding of images and containers, you're ready to dockerize an application. So in the next lesson, we're going to take a front end application built with react and start dockerizing it.

Code and Other Notes in this Discussion: 

Image:
A cut-down OS
Third-party libraries 
Application Files 
Environment Variables  


Container: 
Provides an isolated environment 
Can be stopped & restarted 
Is just a process!
















Sample Web Application: 

So over the next few lessons, we're going to take a front end application built with react and package it into a docker image. I've attached this application below this video, so go ahead and download the zip file, then extract it somewhere on your machine. Now, I want to emphasize, you don't need to know react or even javascript to go through this course, because docker is not about javascript or react. So if you're a c sharp or java or a python developer, you can still go through the course and learn a lot about Docker. In this course, we're only going to take this application as an example so we can learn about Docker.

Now, let me give you a quick overview of what we have in this project in case you are not familiar with JavaScript or React projects. So in this project, we have this file, package dot json, which is basically like an identification card for our application. So on the top we have the name and the version of our application, and over here, we can see the dependencies of this application. These are the third party libraries that our application needs to run. Now, let's say you want to take this application and run it on a brand new machine, so on this machine, we don't have anything but an operating system, whether it's windows or linux, it doesn't matter.

So there are a number of steps we have to follow, take a note of this. First we have to install node. Once we have node, then we can use node's package manager or npm to install the dependencies of this application. Let me run this command, npm is going to look at package dot JSON, then it's going to automatically download and store all these third party libraries in this project. So, let's go ahead, this is going to take a few seconds, so I'll be right back.

Alright, back to our project, now we have a new directory called node modules. In this directory, you can see all the dependencies of this application and their dependencies. So this is a very large directory with hundreds or even thousands of sub directories, okay? So let's quickly recap. To run this project on a new machine, first we have to install node, then using npm we have to install third party dependencies, and finally to start the project we have to type npm start.

We have the same concept in other development stacks, so whether you use c sharp or java or python or ruby, you have some tool to manage the dependencies of your application, and then you have a way to start your application. Here we have to use npm start. So, let's go ahead, this started a development server listening on port 3,000. So, if you go to local host port 3,000, we can see our react application. This is just a basic react application, like a brand new project, I haven't done anything here, and it doesn't really matter.

Later in the course, I'm going to show you a real project that I have built using react and node. So now that you understand what this project is and how we can run it on a new machine, let's see how we can use docker so we don't have to repeat all these steps every time we want to deploy this on a new machine. So, throughout the rest of this section, we're going to dockerize this application and package it into an image. Once we have that image, we can deploy this application virtually anywhere.



Codes and Other Notes Discussion: 

react.js application contains:

package.json 


Steps:

Install Node 
npm install 
npm start 




















Dockerfile Instructions: 


The first step to Docker as an application is adding a Dockerfile to it. Do you remember what a Dockerfile is? It contains instructions for building an image. You saw a few of these instructions before, but let's go through the complete list so you know what is available. We have from for specifying the base image, so we take that base image which contains a bunch of files and directories and then we build on top of it.

Then we have work dir for specifying the working directory, Once we use this command, all the following commands will be executed in the current working directory. We have copy and at for copying files and directories, we have run for executing operating system commands, So all the Linux commands that we talked about in the previous section, you can execute them using run. Now if you're on Windows, you can execute Windows commands using run as well. We have n for setting environment variables, expose for telling Docker that our container is starting on a given port, user for specifying the user that should run the application. Typically we want to run our application using a user with limited privileges.

And we have command and entry point for specifying the command that should be executed when we start a container. So that's the big picture. Now over the next few lessons, we're going to explore each of these commands in detail.


Codes and Other Notes in this Discussion: 


DockerFile: 
FROM
WORKDIR 
COPY 
ADD 
RUN 
ENV 
EXPOSE 
USER
CMD 
ENTRYPOINT 



















Choosing the Right Base Image: 
Alright, let's start off by adding a Dockerfile to this project. So here in the root of this project, we're going to add a new file called Dockerfile. Now the first instruction we're going to use is from, which we use for specifying the base image. The base image can be an operating system like Linux or Windows or it can be an operating system plus a run time environment. For example, if you're a c sharp developer, you want to start from a dot net image, if you're a python developer, you want to start from a python image, if you're a javascript developer, you want to start from a node image.

Now if you Google docker samples, you can find this page, docs.docker.com/samples. Over here, you can see various examples of docker files for different technology stacks. For example, for an aspen.net core application, you can see a docker file right here, now look at the from instruction, over here, we have a full URL instead of an image, because Microsoft images are not hosted on Docker Hub, they're hosted in Microsoft container registry, that's why we have m c r which is short for Microsoft container registry. So an image can be in any registries. The default registry that Docker uses is Docker Hub.

For any images, in other registries, we have to type the full URL. Okay. Now don't just blindly take this URL, because the URL can change or the version can change, so always do your own research. So that's one example. If you're a Django developer, you want to start from Python three.

Or in the future, you might want to use Python four. Now what about this project? Well, for this project we need node, because as you saw, we need npm to install the application dependencies and store the application. So let's go to hop.docker.com, and search for node. Now this is where things get a little bit interesting.

Because node repository on docker hop has hundreds of node images. And this can be a little bit confusing. So let's go to the tags, here you can see various node images, so if you scroll down you can see there are tons of node images for different versions built on different versions of linux. For example, we have node version 14.16, on top of buster, which if I'm not wrong, is Debian Linux version 10. So we have different versions of node on different versions of Linux.

The image you should choose really depends on your application. What version of node do you want to target? We can go to our Dockerfile and say you want to start from node, and by default, Docker assumes the latest tag, do not ever use this. Because if you build your application against the latest version of node, next time there is a new version of note, if you rebuild your application's image, your application will be built with a different version of note, and things can get unpredictable. So always use a specific version.

So back to Docker Hub, let's say I want to build this application against node version 14. So here we search for 14, now once again we have tens or maybe hundreds of node images for version 14. Here's one we just talked about, there's more, we have version 14 on top of buster, so this one doesn't have the minor build number, it's just a major version number. Now look at the size of this image. It's around 300 megabytes.

And why do we have multiple items in this list? Well, this image is built for different operating system and CPU architectures, so there's two more here, as you can see all of these are built on top of linux. So as far as I know, we don't have a node image built on top of windows, I could be wrong, but if you want to run on top of windows, you have to start from a windows image and then install node on top of it. There's no reason you want to do this because windows images are really large. I think they're over two gigabytes.

So, back to the story, depending on your CPU architecture, when you pull an image, docker will automatically download the right docker image for your CPU architecture, okay. Now, I don't want to use any of these images because these are way too large. And this is the compressed size. When this is uncompressed, it's going to be around one gigabyte. I want to go for a smaller image because I want to make my builds and deployments faster.

So on this page, let's search for alpine, earlier I told you that alpine images are really small. Look at this. The compressed size is around 40 megabytes. This image is almost 10 times smaller than the other image you saw. Now look at the image tag, it's very specific.

Version 14.16 of node built on top of alpine 3.13. I'm happy with this version, so I'm going to copy this, and then paste it here. Okay, so that was the first step. Now, back to our terminal window, here in the project directory, we're going to build an image. So, we run docker build dash t for tagging the image, we're going to call this image react app, and we type a period to tell docker where it can find a Docker file.

That means in the current directory. Let's go ahead, okay, the image was built, so now we're going to look at all the images we have on this machine using docker images or docker image ls. So currently we have three images, here's the image that I just built, now let's start a container with this image and see what happens. So, docker, run, we want to run this in the interactive mode so we can play with it, and the image is react app. What's going on here?

We're inside a node environment. So here we can write javascript code, and node will execute it. For example, we can define a constant, and then log the constant, so we're inside a node environment. We don't want this. We want to run bash, so we can look at the file system.

So, we press control and c, now it says to exit, press control and c one more time. Good. So the container is stopped, let's run it one more time, so docker run interactive react app, now at the end, we can specify the command to run when starting this container. We want to run bash. Now look, we get an error saying this module is not found.

Why is that? Because alpine linux doesn't come with bash, that's why it's a very small Linux distribution, so it doesn't have many of the utilities you are familiar with. Instead of bash, it comes with shell, the original shell program. So one more time, docker, run interactive, react app, instead of bash, we're going to use shell, s h. Okay, now we're doing a shell session inside this container.

So let's look around, we have all these directories you are familiar with, we talked about this in the previous section, I hope you didn't miss that. So this is alpine linux, and in this image we also have node. So if you type node dash dash version, we can see the version of node which is 14.16. Now in this image, we don't have our application files, we only have alpine linux and node. So in the next lesson, I'm going to show you how to copy application files into this image.



Codes and Other Notes in this Discussion: 

https://docs.docker.com/reference/samples/


Dockerfile: 

FROM node:14.16.0-alpine3.13


docker build -t react-app .


docker images 


docker run -it react-app sh 
























Copying Files and Directories:

Now that we have a base image, the next step is to copy the application files into the image. So for that we have two instructions, one is copy the other is add, they have the exact same syntax but add has a couple additional features, we'll talk about that later in this lesson. So, let's start with copy, with this we can copy one or more files or directories from the current directory, meaning this directory over here into the image. So we cannot copy anything outside of this directory. Here's the reason, when we execute the build command, look at the last argument, a period means the current directory.

So when we execute this command, docker client sends the content of this directory to docker engine. This is called the build context. So docker client sends the build context to docker engine and then docker engine will start executing these commands one by one. So at that point, docker engine does not have access to any files or directories outside of this directory, okay? So here we can copy one or more files, for example we can copy package dot JSON into slash app into the image.

Now if this directory doesn't exist, docker will automatically create it. We can also copy more than one file, so we can say readme dot md, and by the way remember this is case sensitive, because under the hood we are using Linux. Now here we have a syntax error, look, when using copy with more than one source file, the destination must be a directory and end with a forward slash. So here we need to add a forward slash, okay? We can also use patterns, so let's say we want to copy all files that start with package and end with JSON.

So whatever comes in between we don't care, that's why we're using a wild card. So in this directory we have two files that match this pattern, we have package dash lock dot json and package dot json. Package dot json as I told you before, includes the list of dependencies and their versions. But the actual versions that may be installed on this machine might be a little bit different from what we see here. So package lock JSON keeps track of the exact version of these dependencies installed on this machine, okay?

Now, what if you want to copy everything in the current directory into the app directory? We use a period. So this is all about source files and directories. Now, let's talk about the destination. So here we're using an absolute path because our path starts with a forward slash.

We can also use a relative path if we set the working directory first. So using the work directory, we can set the working directory, now all the instructions that come after will be executed inside this working directory. And with this, we can replace this absolute path with a relative path. So we can use a period meaning the current directory, okay? Now, one last thing about copy.

What if you want to copy a file that has a space in it? So, let's say we have a file called hello world dot txt. Look, we immediately get a syntax error. This is where we use the other form of the copy instruction. So copy has another form where we can pass an array of strings.

So multiple strings, each string represents an argument of the copy instruction. So here, we're going to wrap all these arguments inside brackets, then we're going to wrap the first item, which is our source file in double quotes, then we add a comma, and wrap the second argument in double quotes. It's not a form that you use that often, but I thought to cover it to make this lesson comprehensive. So, let's simplify things, we're going to copy everything in our context directory, which is the current directory into the current working directory of the image, okay? Now, we also have add, it has the exact same syntax as copy, but add has two additional features.

With add, we can add a file from a url, so here we can type http some url and let's say some file. Json, so if you have access to this file, we can add it to our image. The other feature is that if you pass a compressed file, add will automatically uncompress this into a directory. So you can use either of these instructions but the best practice is to use copy because it's more straight forward, there's no magic around it, use that only if you need one of these additional features. If you want to add a file from a URL or if you want to uncompress a compressed file.

So, let's get rid of this, we're going to copy everything in the current directory, enter image, so let's go ahead and rebuild our image. Alright, now look at this line. Transferring context. So all the files and directories in our current directory are being sent to Docker engine. Alright, the image is built, so let's start a container with this image.

Docker, run, interactive, react app, we're going to run shell so we can look at the file system. Okay, look, we're inside the app directory, because in our docker file, we set this directory as the current working directory. Now let's run ls, so here you can see all the files and directories we have in our project, including node modules. So this is all about copying files. Now, what if you want to exclude some files or directories?

We'll talk about that next.



Codes and Other Notes in this Discussion: 

Dockerfile:

FROM node:14.16.0-alpine3.13
WORKDIR /app
COPY . .
 

docker build -t react-app .


docker run -it react-app sh 
























Excluding Files and Directories: 

In the last lesson, you saw that when we build our application, Docker client takes everything in this directory which is called the build context or the context directory. So Docker client takes everything here and sends it to Docker engine or Docker Damon. Now for this simple application which has absolutely no features, our build context was about 150 megabytes. But why is it so large? That's the result of the node modules directory.

So if you look over here, you can see tons of directories and subdirectories and this is a very simple project. As our application gets more complex and we use more third party libraries, this node modules directory gets larger and larger. Now, there's a problem here. Later in the course when we talk about deployment, you will see that our docker client will talk to a docker engine on a different machine. So that means whatever we have in the build context has to be transferred over the network.

So if you have a large build context with a million files in it, all these files have to be sent to the docker engine on the remote machine. We don't want that. We don't really need to transfer this node modules directory, because all these dependencies are defined in package dot JSON. So we can simply exclude this directory and copy everything else and then restore these dependencies on the target image. And this has two benefits.

The first benefit is that our build context is smaller, so we transfer less data over the network, the second benefit is that our build is going to be faster, so we don't have to wait for all these files to be transferred to Docker engine, okay? Now how do we exclude this directory? Very easy. You are probably familiar with this file, dot git ignore. With this file, we can exclude some files and directories from git.

Now we have the same concept in docker, so we can create a file in the root of this project, called dot docker ignore, and everything is in lower case, and here we can list the files and directories that should be excluded. So we can exclude node modules. Now, when we build a new image, Docker will no longer transfer this large gigantic directory to Docker engine. Let's take a look. So, I'm going to run the build command one more time, good, okay, look at our build context, it's only 10 kilobytes, so we reduce our build context from 150 megabytes to 10 kilobytes, that's a huge improvement.

There is a problem though, if you start a new container and look at the file system of that container, we're not going to see the node modules directory because we excluded it, right? Let's verify it. So I'm going to run a new container from react app and then run shell inside that container. Now in the app directory, let's look at all the files and directories, look, we only have two directories here, public and source. So node modules is not here.

This is where we need to run npm install to install these dependencies. And that's what we will do next.


Codes and Other Notes in this Discussion: 


.dockerignore 
node_modules/


















Running Commands: 


Alright, the next step is to install our project dependencies using npm. This is where we use the run command. With this command, we can execute any commands that we normally execute in a terminal session. So we can run npm install, we can also run any of the linux or windows commands, for example, let's say we want to install python on this image as well. We can use apt or apt get to install python.

Now if you run this, we're going to get an error because alpine linux doesn't have apt package manager. It has another package manager called apk. So be aware of these differences depending on the type of linux or windows you are using. Okay? So, we don't need to do this in this lesson, let's just install the dependencies of our project.

So now, back to our terminal, let's rebuild the image. Alright, so now, Docker engine is running npm install to download and install these dependencies, this is going to take a few seconds so I'll be right back. Alright, our image is ready, so let's start a new container with this image and run shell. Good, now let's look at our app directory, alright and here we have the node modules directory, beautiful. So we're done with this step, next I'm going to talk about setting environment variables.



Codes and Other Notes in this Discussion: 

Dockerfile:

FROM node:14.16.0-alpine3.13
WORKDIR /app
COPY . .
RUN npm install 


docker build -t react-app . 




















Setting Environment Variables: 

Sometimes we need to set environment variables. For example, let's say this front end application needs to talk to a back end or an API. Quite often we set the URL of the API using an environment variable. This is where we use the end instruction, so we can set api url to let's say httpapimyapp.com, whatever, it doesn't really matter. We also have an older syntax without an equal sign, that also works, I personally prefer to have an equal sign so we can clearly see the value of environment variables.

But that's just my personal preference. Now, let's rebuild the image and inspect this environment variable in our container. So, we're going to rebuild the image, alright, the image is built, so let's start a new container. Now, do you remember the command that we use for inspecting environment variables? We have a few options.

One way is to use print env to see all environment variables, so you can see, API URL over here, beautiful. We can also print the value of a particular environment variable, another option is to use echo, but here we have to use a dollar sign to print API URL. Okay, so now whenever we start this container, this environment variable is automatically set for us.



Codes and Other Notes in this Discussion: 

Dockerfile:

FROM node:14.16.0-alpine3.13
WORKDIR /app
COPY . .
RUN npm install 
ENV API_URL=http://api.myapp.com/



docker build -t react-app . 

docker run -it react-app sh 

printenv
printenv API_URL 
echo $API_URL 
















Exposing Ports: 
Alright, let's take a quick break from our Dockerfile and start this application outside of Docker. The way we have always started it. So from the project directory we run npm start. Now this starts at development server on port 3,000, so if you go to local host port 3,000, we can access this application. Now in the future, when we run this application inside a docker container, this port, port 3,000 will be open on the container, not on the host.

This is something important you need to understand. So on the same machine, we can have multiple containers running the same image. All these containers will be listening to port 3,000, but the port 3,000 on the host is not going to be automatically mapped to these containers. So later in the next section I will show you how to map a port on the host to a port on these containers. But for now we're going to go back to our Dockerfile and use the expose command to tell what port this container will be listening on.

That is port 3,000. So I just want to clarify something, the expose command doesn't automatically publish the port on the host, it's just a form of documentation to tell us this container will eventually listen on port 3,000. So later when we properly run this application inside a docker container, we know that we should map a port on the host to Port 3000 on the container. We'll do that in the next section.


Codes and Other Notes in this Discussion: 

Dockerfile:

FROM node:14.16.0-alpine3.13
WORKDIR /app
COPY . .
RUN npm install 
ENV API_URL=http://api.myapp.com/
EXPOSE 3000













Setting the User: 
Alright, now let's talk about users. So by default, Docker runs our application with the root user that has the highest privileges. So that can open up security holes in our system. So to run this application, we should create a regular user with limited privileges. But before doing that in a docker file, let's open up a shell session on alpine linux and play with a few commands.

Now if you're a windows user, I still want you to follow along, even though the commands I'm going to show you are not going to be relevant, the approach I'm taking is absolutely relevant. So take this approach and implement it on top of windows, you just have to use different commands, okay? So we're going to run docker run-in the interactive mode, alpine. So here we have a shell session, now in this image we don't have the user add command. Instead we have add user, okay?

So look at these options, there are two options we're going to use, one of them is dash g for setting the primary group of the user and the other is dash s for creating a system user. We're going to use a system user because this user is not a real user, it's just for running an application, okay? Now, before using this command, we have to create a group so we can add this user to that group, okay? So to do that, we're going to use add group, we're going to give this group a name, let's say app, okay, now we can run add user dash s for creating a system user, dash g for setting the primary group, which is app, and finally we specify the name of the user. We're going to use the same name, this is a common best practice in Linux.

So whenever we create a new user, we create a primary group for that user with the same name, okay? Now we have a new user, let's verify that this user is in the right group. So, we type groups app, this shows the groups for the app user. Perfect. Now, we're going to combine these two commands into a single command.

So let's create another group on a user. We type add group, we can say Mosh or whatever, we use double ampersands to combine two commands, add user dash s dash g, Mosh, Mosh. So we use a single command to perform two tasks, okay? Now let's verify the groups for Mosh, perfect. So this is the command that we're going to run in our Dockerfile.

So let's copy this, now, back to our Dockerfile, we're going to run a command. How do we do this? We use the run instruction. Then we paste our command, and instead of Mosh, we're going to use app. So we're going to create a group and a user called app, okay?

Once we do that, then we can set the user using the user command. So all the following commands will be executed using this user. Let's save the changes and rebuild our image. So, here's the build command, let's go ahead, alright, our image is ready, now look at the fifth step. That is our run command for creating a new group and a new user.

Now you might have noticed that building this image took probably a long time, that was because of this step. Installing the dependencies. Now you might argue that earlier, we excluded the node modules directory, so our builds would be faster, But our builds are still slow, because every time we build this image, all these dependencies have to be installed. Don't worry about that, we're going to optimize this later in this section. Now let's start a new container and make sure the current user is the app user, not the root user, okay?

So, docker run, in the interactive mode, we're going to run react app and start a shell session. Good? Now, what command should we use to print the current users name? Do you remember? I told you at the beginning of the linux section, that is who am I?

So the app user, beautiful. Now let's look at something interesting. I'm going to get a long listing, these are the files in our application, now all these files as you can see are owned by the root user. And in this column, you can see the permissions of the root user. So the root user can write to any of these files or directories, but the current user running this session is the app user.

So this user falls into the others group, that means the app user is not able to write to any of these files. In contrast, if we executed this application with the root user, a hacker could potentially rewrite something in our application. So we're done with users, next we're going to talk about defining entry points.



Codes and Other Notes in this Discussion: 


docker run -it alpine 

addgroup app 

adduser -S -G app app

groups app 

addgroup mosh && adduser -S -G mosh mosh 
groups mosh 



Dockerfile:

FROM node:14.16.0-alpine3.13
WORKDIR /app
COPY . .
RUN npm install 
ENV API_URL=http://api.myapp.com/
EXPOSE 3000
RUN addgroup app && adduser -S -G app app 
USER app  


















Defining Entrypoints: 

Alright, our Dockerfile is almost ready, now how do we start our application? Well, earlier you saw that. Here in the project directory, we can start the application by running npm start. So this is the command that we should execute when starting a container. So, let's start a container, docker, run, now we're not going to use the interactive mode, because we don't want to interact with this container.

We're not going to run a shell session. We just want to start the application. So, we type the image name, now let's see if we execute this command up to this point. Our container started and then stopped, because we didn't specify a command or a program to execute. So this is where we're going to type npm start.

Take a look. Alright, we got a permission error, look over here, permission denied. Can you tell why we're getting this error? Think about it for a few seconds, and I'll give you a hint. Look at the dockerfile.

Here's the answer. In our dockerfile, we set the user at the end. So we executed all these previous instructions as the root user, but then we switched to a regular user with limited privileges. So, we should move these two lines on the top. So first, we set the user, then we create the working directory, copy files and so on.

Now we have to rebuild our image and start a new container. So back in the terminal, let's stop this process by pressing control and c, good, now, docker build dash t, react app, period. Alright, our image is ready, so let's start a new container, alright, our web server started on port 3,000, but as I told you before, this is port 3,000 of the container, not local host. So if you go to this address, you're not going to be able to access the application. In the next section, I will show you how to map a port from the host to a port on the container, okay?

Now, let's start this process, let's bring up the run command, now here's the problem, we don't want to have to specify this command every time we start the container. This is where we use the command instruction. So back to our Dockerfile, at the end, using the command instruction, we can supply a default command to be executed. So, npm start. Now, back to our terminal, once we rebuild the image, then we can start a container by simply running docker run react app, we don't have to specify the command here.

Great. Just remember, because the command instruction is for supplying the default command, it doesn't make sense to have multiple command instructions in a docker file. If you have multiple command instructions, only the last one will take effect, okay? Be aware of that. Now you might be wondering what is the difference between the command instruction and run.

Because with both these we can execute commands. Here's the difference, the run instruction is a build time instruction. So this is executed at the time of building the image. So when building the image, we're installing npm dependencies, and these dependencies are stored in the image. In contrast, the command instruction is a run time instruction.

So it's executed when starting a container. Okay? Now, this command instruction has two forms. What you see here is called the shell form, we also have another form called execute form, which takes an array of strings. So, npm and start.

What is the difference? Well, if you use this syntax, Docker will execute this command inside a separate shell, that is why it's called the shell form. Now, on Linux, that shell is slash bin slash shell, the original shell program, on Windows, is cmd or command prompt. Now the common best practice is to use the execute form, because with this we can execute this command directly and there's no need to spin up another shell process. Also, this makes it easier and faster to clean up resources when you stop containers.

So always use the execute form. So, let's get rid of these, so here's our command instruction, now we also have another instruction called entry point which is very similar to command. So it has two forms, the shell form, or the execute form. So it takes an array of strings. Now, what is the difference?

Well, we can always override the default command when starting a container. So, back to the terminal, when we run this container, we can supply another command. So, we can say echo hello, and this will override this command over here. So for the same reason, here we can run a shell session and of course we want to use the interactive mode. Now, in contrast, we cannot easily override the entry point when running a container.

If you want to do that, we have to use the entry point option. Now a lot of people forget to use this, that's why it's a little bit harder to overwrite the entry point. So in practical terms, we often want to use entry point when we know for sure that this is the command or this is the program that should be executed whenever we start a container. There is no exception. In contrast, with the command instruction we have a bit more flexibility, we can always overwrite this, so we want to use this instruction for executing adhoc commands in a container.

Now, which instruction you use is a matter of personal preference, with both these instructions we can supply a default command. Some people prefer the command instruction, other people prefer the entry point. I personally prefer the former, so I'm going to get rid of this, good, so our Dockerfile is in good shape, but if you have noticed, our builds are slow. We'll talk about optimizing them next.



Codes and Other Notes in this Discussion: 

docker run react-app npm start 


Dockerfile:

FROM node:14.16.0-alpine3.13
RUN addgroup app && adduser -S -G app app 
USER app  
WORKDIR /app
COPY . .
RUN npm install 
ENV API_URL=http://api.myapp.com/
EXPOSE 3000
CMD npm start 


docker run react-app 


#shell form 
CMD npm start 

#Exec form 
CMD ["npm", "start"]


ENTRYPOINT npm start 

ENTRYPOINT ["npm", "start"]













Speeding Up Builds: 
You've probably noticed that our builds are fairly slow, every time we make a small change, we have to rebuild and wait almost half a minute. So let's see how we can optimize these builds. The first thing you need to understand here is the concept of layers in docker. An image is essentially a collection of layers. You can think of a layer as a small file system that only includes modified files.

So when docker tries to build an image for us, it executes each of these instructions and creates a new layer. That layer only includes the files that were modified as a result of that instruction. So for our first instruction, docker is gonna take the node image and put it in a layer. Now more accurately, the node image itself is several layers, but let's not worry about that for now. Let's just imagine we have a single layer and this layer includes all the linux and node files.

Then docker is going to execute the second instruction to add a group and a user. Once again, it's going to create a new layer because as you saw in the previous section, when we add a user or a group, something is written to the file system. So a couple of files are modified. So this layer will only include the modified files, okay? Similarly, Docker will execute all these other instructions and create several layers.

Let's have a quick look at these layers. So here in the terminal we type Docker history and the name of our image, react app. Alright, let me move this window around so we can see things clearly. Okay, we're only going to look at these two columns, created by and size. In this column you can see the instruction that created a new layer, and in the size column you can see the size of that layer.

Now we have to read this list from bottom to top. So, look over here, first we have this group of layers that come from node and linux. So earlier I told you that when we use the from instruction, that instruction is going to bring in several layers that belong to Linux and node, and then we're going to build on top of those layers, okay? Now look at our run instruction for adding a new user. That's the command that we typed earlier, right?

Now look at the size of this layer, it's four kilobytes because only a couple of files are modified as a result of this command. Now look at how this instruction is executed. It's executed inside slash bin slash shell because we use this instruction in the shell form. In contrast, if we use the execute form, this command would be executed directly, okay? Next we set the current user, the size of this layer is zero, no files are changed, then we set the working directory, then we copy all the application files, and over here you can see the size of this layer is 1.6 megabytes.

Then we run npm install, and once all these dependencies are installed, they are stored in this layer and the size of this layer is 178 megabytes. So an image is essentially a collection of these layers. So you understand what layers are. Now, docker has an optimization mechanism built into it. So next time we ask docker to build this image, it's going to look at the first instruction and see if the instruction is changed or not.

If it's not changed, it's not going to rebuild this layer, it's going to reuse it from it's cache. That's the optimization I was talking about. Then docker is going to look at the second instruction, once again, nothing has changed here, so docker is going to reuse this layer from it's cache. In contrast, if we made a tiny change here, let's say if we changed app to app two, then docker had to rebuild this layer, okay? Now, look at this line, copy all these files, this is a special instruction because with this instruction, docker cannot tell if anything has changed or not.

So it has to look at the content of files as well. And that means if we make a tiny change in our application, if we write one extra line of code, docker cannot reuse this layer from it's cache, it has to rebuild it. And this is where the problem happens, once the layer is rebuilt, all the following layers have to be rebuilt as well. So docker cannot reuse this layer from it's cache. It has to install all npm dependencies, and this is exactly where we have a bottleneck.

We have to wait half a minute for all these dependencies to be installed. It doesn't make sense. So to optimize our build, we have to separate the installation of third party dependencies from copying our application files. How do we do that? Very easy.

Instead of copying all the files in one go, first we want to copy the files needed for installing third party dependencies. Do you know which files I'm talking about? These two files, package dash lock dot json and package dot json. These are the only files that we need for running npm install, okay? So we're going to copy package star dot JSON, then we run npm install, and finally we copy all the application files.

Now, with this new setup, if you haven't changed any of our application dependencies, Docker is going to reuse this layer from it's cache because package dot JSON is not modified, right? Similarly, Docker is not going to reinstall all npm dependencies because this instruction is not changed. If it changes to npm update, then yes, docker had to rebuild this layer, okay? But nothing has changed here, so docker can reuse it's cache. Then we're copying our application files and of course, this layer should always be rebuilt, and that's totally fine.

So let's run a quick test. Back to the terminal, let's rebuild this image. Alright, now look over here. On this line you can see cache, because docker is reusing the layer cache for this instruction. The same is true for setting the working directory.

Now this is the first time I'm building this Dockerfile, that's why this is a new layer that we have, this layer has to be built, and of course, dependencies have to be installed as well, so let's wait until this is done. Alright, the image is built, now let's go modify our application code and do another build. So, I'm going to go to this file, readme. Md and add a plus here, very simple. Now, let's do another build, alright, our build was super fast, because we didn't have to wait for the installation of third party dependencies.

Look at this line, once again we are using layer caching. So here's what you need to take away from this lesson. To optimize your build, you should organize your docker file such that the instructions that don't change frequently should be on the top, and the instructions or files that change frequently should be down the bottom.




Codes and Other Notes in this Discussion: 

docker history react-app 


Dockerfile:

FROM node:14.16.0-alpine3.13
RUN addgroup app && adduser -S -G app app 
USER app  
WORKDIR /app
COPY package*.json .
RUN npm install 
COPY . .
ENV API_URL=http://api.myapp.com/
EXPOSE 3000
CMD npm start 
















Removing Images: 
Let's talk about removing images. So first we run docker images, now in this list we have a bunch of images that have no name and no tag. These are what we call dangling images, meaning loose images. These are essentially layers that have no relationship with a tagged image. So as we were changing our docker file and rebuilding our image, docker was creating these layers and at some point, these layers lost their relationship with our react app image.

So as you work with Docker, you see these dangling images popping up all the time. To get rid of them, we have to use the prune command. So we type Docker image, prune. It's asking for confirmation, yes, alright. Nothing was deleted in this case because we still have containers running an older react app image.

Let's verify. So we run docker ps, currently we have no running containers, but you probably have a bunch of containers in the stopped state. So, we run docker ps dash a, that is short for all, so look, we have tons of containers that are currently stopped. So we can get rid of these containers as well. How?

Very easy, docker container prune. So it's asking if you want to remove all stopped containers. Yes, good, so all these stopped containers are deleted and this frees up on this machine, 60 megabytes of space. Now, let's run docker image prune one more time. Yes, great.

So all these dangling images are removed, now, if you run docker images, we only see the proper images we have created or pulled from Docker hop. Great. Now how do we remove one of those images? We run docker image, so all image management operations start with docker image. If we simply press enter here, you can see all these sub commands.

So we can build an image, this is equivalent to running docker build. We can also see the history of an image, we can load an image, we can prune images and so on. Now in this list we also have remove for removing one or more images. So, let's bring up the list of images one more time, because here we need the name or the id of an image. So we can run docker image remove, we can either reference an image by it's name, for example we can remove react app, or we can reference using it's image id, d f three.

And if you want to remove multiple images, we type them all here, separated by a space. Now in this demo, I want to delete hello docker image. Okay, let's verify it's gone, docker images, good.


Codes and Other Notes in this Discussion: 

docker images 

docker image prune 


docker ps -a 

docker container prune 

docker image rm react-app 

docker image rm df3

docker image rm hello-docker 















Tagging Images: 

Alright, let's talk about tags. If you have noticed, whenever we build an image or pull it from Docker hub, by default Docker uses the latest tag. Now this latest tag is just a label, there is nothing special about it. So it doesn't necessarily mean this is the latest version of your image. If you don't tag your images properly, latest can point to an older image.

I will show you that in a second. Also keep in mind that latest is totally fine for development, but you shouldn't use it in production. You don't want to put an image with the latest tag in your production or staging server because if something goes wrong, you cannot easily troubleshoot issues. Because you don't know what version you're really running in production. If you want to do a rollback or an upgrade, how can you tell what version you're running in production if it's always latest?

So you should always use explicit tags to identify what version you're running in each environment, in your test environment, staging and production. For development, latest is totally fine. Now, in this lesson I'm going to show you how to tag your images properly, but before we get started, I want to do a clean build, so we start on a clean canvas. So, docker build dash t, react app, period. Good, so, let's look at our images, we have three images and all of them have the latest tag.

Now how can we tag an image? Well, there are two ways. One way is to tag an image while building it. So, we type docker build dash t four tag, so far we have just typed the image name, but we can type a colon followed by a tag. Now what tag should we use?

Well, this is where different teams have different preferences, some teams like code names, let's say you have a version called buster, and everyone in your team knows what buster version is all about, Then you can use it as a tag. Other teams prefer semantic versioning, like three point one point five. This approach is common amongst teams that don't release that often. Now teams that release frequently prefer build numbers, let's say build 76 or 77. Now quit often this is done automatically by continuous integration and deployment tool, so if you use a continuous integration or deployment tool in your organization, you can configure your tool to check out the latest code from your repository, build an image and automatically tag it with the current build number.

Now, that's also the scope of this course. So let's go ahead and give this image a tag like version one, and build it. Alright, our build was blazing fast, let's look at the images one more time, alright, here we have an interesting situation. This react app image that we have has two tags, one and latest. So it's the same image with multiple tags.

How do I know? Look at the image id, both these images have the same identifier. So an image can have multiple tags. Now, what if we made a mistake and what if we remove this tag? Well, we use the remove command, so docker image remove, and here we type the image name followed by the tag.

Okay, the tag is gone, so let's look at the images, perfect, we only have the latest tag, so one way to tag an image is while building it. But what if you want to tag an image after? We use the tag command. So docker images tag, first we identify our source image, we can use the full image name and it's tag, or we can use an image id. Whatever you prefer.

So we're going to get this image and apply a new tag to it. React app version one. I made a mistake because I spelled images as plural. So, that should be docker, image tag, react app, latest, we're going to give it a new tag. Good?

Now let's verify that everything is in the right shape, perfect. Now, earlier I told you that the latest tag can get out of order. Let's see that in action. So, I'm going to go back to our project, and make a tiny change. In this readme file, I'm going to add a couple more pluses simulating a change in the applications code.

Now we're going to build a new image. So, let's say docker build dash t, react app, let's say this is version two. Okay, great. Now, let's look at the images. So, we have two versions of this image, version two and version one.

Now look at the latest tag, it's pointing to the first version, because both these tags are pointing to the same image, which is older, I built this image at the beginning of this lesson. So here is what you need to take away, the latest tag doesn't necessarily reference the latest image. You have to explicitly apply it to the latest image. So how do we do that? Pause the video and do this on your own.

Here's how we do it. Docker image tag, we're going to identify our image, this time I'm going to use the image id, so we can type B06. We're going to give this image a new tag, react app colon latest. Okay, now let's look at the images again, good, so now the latest tag is pointing to version two of our image. Great.

Next we're going to talk about sharing images.




Codes and Other Notes in this Discussion:  

docker build -t react-app:buster 

docker build -t react-app:3.1.5

docker build -t react-app:76


docker image remove react-app:1


docker image tag react-app:latest react-app:1


docker image tag b06 react-app:latest 















Sharing Images: 
Let's see how we can share our images with others. So head over to hop.docker.com and create a new account. It's absolutely free and it's going to take only a few seconds. Now let me log in with my account, good. So on this page we can create a new repository, and this is similar to creating a github repository.

So we can have a repository and in this repository we can have multiple images with different tags. So, let's create a new one, we're going to give it a name, I'm going to use the same name that I used for our image, now we can give it a description, we can make it public or private, if you have a free account you only get one private repository, but you can always upgrade to get additional private repositories. Now optionally, we can connect our account to our github account, so every time we do a push, Docker Hub automatically pulls the latest code and builds a new image. Now I'm not going to do that in this lesson, so let's go ahead and create, good. So here we have a repository and this is the name of our repository.

So our username slash our image name. Now, to push our image to this repository, we have to give it this tag, code with Mosh or whatever your username is, slash react app. So back to the terminal, let's look at our images, let's say I want to publish the latest version of this image. So, docker, image, tag, we're going to tag this image, I'm going to use the image id, so B06, I'm going to give it a new tag, go with Mosh, slash, react app. Now if you don't specify a tag, docker will automatically use latest, we don't want to do that, we want to use an explicit tag.

Okay, now, let's look at our images one more time, so now this image with this id has three tags. Here's the first tag, tag two, we also have the latest tag, but we also have code bin mosh slash react app with tag two. All these tags are pointing to the same image on my machine. Okay? So now we are ready to push this image.

First we have to log in. So docker login, we use our credentials, good, I'm logged in, now we type docker push, and here we specify our image. So code with MOSH slash react app version two. Alright, now Docker is pushing each of the layers in our image. So the first time we push this image, this is going to take a little while, because one of our layers, the layer that includes all npm dependencies is fairly large.

So once we push this image, our future pushes will be faster, assuming that we have not changed our application dependencies. So our image is pushed, now let's refresh this page. So in this repository, we have one tag that is tagged two, which is built for Linux. Great. Now back to our project, let's make a small change to this readme file, and do another push.

Back to the terminal, first we need to build this image, so build dash t, react app, let's say version three, that was super fast, now, let's look at the images, this is our version three, now we want to give this image an extra tag that starts with code with Mosh, or our username. So, docker, image, tag, I'm going to tag react app version three with code with Mosh slash react app version three, good, so, let's say docker push, code with Mosh, react app version three. Alright, now look, some of these layers already exist, so this push is a lot faster than the previous push. Done. Now back on docker hub, let's refresh, now in this repository we have two text, beautiful.

Alright, now that our image is on Docker Hub, we can put it on any machine that runs Docker. Just like pulling any other images from Docker Hub.



Codes and Other Notes in this Discussion: 


docker image tag b06 codewithmosh/react-app:2 


docker login 

docker push codewithmosh/react-app:2 


docker build -t react-app:3 . 

docker image tag react-app:3 codewithmosh/react-app:3

docker push codewithmosh/react-app:3















Saving and Loading Images: 
Alright, the last thing we're going to talk about in this section is saving and loading images. Let's say you have an image on this machine, and you want to put it on another machine but without going through docker hop. In this case, you can save that image as a compressed file, and load it on the other machine. So, let's run Docker image save, now for any of these commands, if you want to learn more about them, you can always add double hyphen help at the end. So here's the syntax, docker image save, we have to use one of these options, like o which is the short form or output which is the long form, here we specify the file to write to, and then we identify the image.

So I'm going to say docker image save dash o, so I'm going to save this to a file in the current directory called react app dot tar, a tar file on linux is like a zip file on Windows, it's a compressed file. And then we specify our image. Let's say react app version three. Now, this is going to take a while, so I'll be right back. Alright, our image is saved, so here we have a tar file, we can double click it to uncompress it.

Now let's see what we have here. Each of these folders represent a layer. Now if you're adventurous, we can go inside each of these layers and see what is really inside that layer. So each layer contains a JSON file and a tar file which contains all the files in that layer. So let's open this up, here we have an app folder, so that means this is the layer that represents our application code.

You can go look at other layers, you will find a layer for Linux, you'll find a layer for node and so on. So we have the save command for saving an image into a file, we also have the load command, let's look at that real quick. So we type docker image load and then use one of those options. We can use I or input to specify the file name, and optionally we can use the quiet option to suppress the load output. So to demonstrate this, first I'm going to delete the image from this machine, and then load it using our image file.

So, Docker, image, remove, react app version three, it's gone, let's look at the images, good, now technically we still have this image, because there is another tag pointing to this image. So let's remove that tag as well, docker image remove, in fact you know what, I'm going to use the image ID, that's easier. Good, now let's look at the images one more time, alright, perfect. That image is gone from this machine, now I'm going to load it using our image file. So, we type docker image load dash I react app dot tar.

Alright, we can see this image is loaded, so let's verify it. And here we have react app with tag three. And this is the same image I did that we had earlier. So, that brings us to the end of this section, I know this was a long section, we had a lot of ground to cover, but now that you have a solid understanding of images and how to work with them, everything is going to go smoothly from the following sections. So, I'll see you in the next section.



Codes and Other Notes in this Discussion: 


docker image save --help 

docker image save -o react-app.tar react-app:3 

docker image load --help

docker image load -i react-app.tar 





















Summary: 
Images

Dockerfile instructions

FROM # to specify the base image
WORKDIR # to set the working directory
COPY # to copy files/directories
ADD # to copy files/directories
RUN # to run commands
ENV # to set environment variables
EXPOSE # to document the port the container is listening on
USER # to set the user running the app
CMD # to set the default command/program
ENTRYPOINT # to set the default command/program



Image commands:

docker build -t <name> .
docker images
docker image ls
docker run -it <image> sh



Starting and stopping containers:

docker stop <containerID>
docker start <containerID>



Removing containers:

docker container rm <containerID>
docker rm <containerID>
docker rm -f <containerID> # to force the removal
docker container prune # to remove stopped containers




Volumes:

docker volume ls
docker volume create app-data
docker volume inspect app-data
docker run -v app-data:/app/data <image>



Copying files between the host and containers:

docker cp <containerID>:/app/log.txt .
docker cp secret.txt <containerID>:/app




Sharing source code with containers:

docker run -v $(pwd):/app <image>
















Working With Containers: 

Introduction: 

Welcome back to another section of the Ultimate Docker course. In this section we're going to explore containers in more detail. We'll talk about starting and stopping containers, publishing ports, viewing container logs, executing commands in running containers, removing containers, persisting data using volumes, and sharing source code with containers so we don't have to rebuild our image every time we make a change in our code. This is a short and sweet section, so let's jump in and get started.

In this Section: 

Starting & stopping containers 
Publishing ports 
Viewing Container logs 
Executing commands in containers 
Removing containers 
Persisting data using volumes 
Sharing source code 





Starting Containers: 

Alright, we briefly talked about container commands throughout the course, so you're already familiar with many of them. In this section, we're going to review them one more time, but I'm also going to give you some extra tips along the way. For starters, let's look at our images. So I've cleaned up some of the images that I created in the previous section, now we only have react app with the latest tag. We also have Ubuntu and alpine which we're not going to use in this section.

Now hopefully you completed the previous section, but if you didn't, I highly encourage you to download the zip file app attached to this lesson. In this zip file, we can find a react application as well as the docker file that we created in the previous section. Use that docker file to build an image, and now we are on the same page. Okay? Now, question for you, how can we see the running containers?

Using docker what? Docker ps, short for processes. Because a container is just a process, but it's a special kind of process because it has it's own file system which is provided by the image. Okay, currently there are no running containers here, so I want to run a new container using the react app image. This is going to start a development web server, so I'll be right back.

Okay, our web server is ready, now there's a problem, I cannot type any additional commands on the terminal window. If I press control and c to get out of this, our container stops. Let's verify that. So docker ps, look, no container is running. So let me show you a cool technique.

Let's bring up the run command one more time, now this time we're going to use an option dash d that is short for detached. With this, we can run this container in the detached mode which means in the background. So, look, that's it. Now the terminal window is free and we can do whatever we want. Of course, the container is going to take a few seconds to start because that's the time we need for the web server to start, okay?

So that's one option. Now, let's look at the running processes, so here we have one container, here's my container id, now if you look on the far right column which is not visible in my recording window, and I don't want to resize this window because the text is going to get small. In the last column you can see the names column. So docker automatically associates each container with a random name. So in the future when we want to reference a container, we can either use it's id or it's name.

But we can also give our container a name when starting them. So, let's start another container in detached mode, this time we're going to use dash dash name to give it a name. I'm going to call this blue dash sky. It's easier to work with then a randomly generated name by docker. And we're going to use the react app image, good, so, let's look at the running containers, now we have two containers, and one of them is called blue sky.



Codes and Other Notes in this Discussion: 

docker ps 

docker run -d react-app 

docker run -d --name blue-sky react-app














Viewing the Logs: 

So now we have two containers running in the background, but there's a problem. We don't know what is going on inside these containers. What if something goes wrong? What if our server generates an error? That's where we need to use the log command.

But first, let's bring up our running containers, so now we're going to look at the logs for this container. We type docker logs, and then the container id. Of course, on your machine, the container id is different. So whatever it is, just type the first few letters. Okay, so here we can see the output of our web server, this is exactly the same output that we saw when we started this container in the foreground.

Now, this logs command has some additional options, so as I told you before, whenever you want to play with the docker command, always use dash dash help to learn about various options, so one of the options here that is useful is dash f or follow, this is useful if your container is continuously producing output. So instead of running docker logs, some container id multiple times, you can just use dash f to follow the log. Then whatever is written to the log, you can see it in real time on the terminal. Of course you have to press control and c to get out of that. Let's look at the other options.

Another option is dash n or tail, and with this we can specify the number of lines to show. This is useful if we have a really long log. So docker logs, what was that container ID? 655. I'm going to use dash n five to look at the last five lines.

That's it. Pretty useful. Now what if you want to see time stamps? We can use dash t. So, dash t, and now we see the time stamp in front of each message.

So if you encounter any issues when running your applications inside Docker, the first thing I want you to look at is the logs. Next we're going to talk about publishing parts.



Codes and Other Notes in this Discussion: 

docker logs 655

docker logs --help

docker logs -f 123

docker logs -n 5 655 

docker logs -n 5 -t 655 











Publishing Ports: 

So currently we have two containers running our react application. But if you go to local host port 3,000, we cannot access this application. Can you explain why? Because as I told you earlier, port 3,000 is published on the container, not on the host. So on the same machine we have multiple containers, each of these containers is listening on port 3,000, but the host itself is not listening on this port.

So this port is currently closed, there is no way to send traffic into local host at this port. This is where we need to publish a port. So first of all, let's look at our running containers, Now here we have a column called ports, let me resize the window, it's going to look a little bit ugly, but bear with me. So we have wrapping here, here's the ports column. In this column, you can see the ports and their mapping.

As you can see, both these containers are listening to port 3,000. Now I'm going to start a new container and publish a port at the same time. So just like before we run docker run dash d for detached, here we're going to use another option called p that is for port, we're going to publish a port on the host, let's say port 3,000 to port 3,000 of the container. And of course, these numbers don't have to be the same. So I can publish port 80 of the host to port 3,000 of this container.

Now I can give this container a name so we can identify it, I'm going to call this c one. Now, what image? React app. Good? So this is going to take a few seconds until our web server is ready.

Now if you go to local host, port 80, we can see our react application, beautiful. Now, let's look at the running containers one more time. Now look at the port mapping for our c1 container. You can see port 80 of the host is mapped to port 3,000 of the container. We do not have this notation for other containers.



Codes and Other Notes in this Discussion: 

docker run -d -p 80:3000 --name c1 react-app 










Executing Commands in Running Containers: 


So you'll learn that when you start a container, it executes the default command that we specified in our docker file. Now what if you want to execute a command in a running container later on? Let's say you want to troubleshoot something and you want to look at the file system of that container. Let me show you how to do that. So we have docker exec or execute, and with this we can execute a command in a running container.

Now some people say what is the difference between docker exec and docker run? With docker run, we start a new container and run a command, whereas with docker exec, we execute a command in a running container. Okay? Now for this I'm going to use my c one container, this is the benefit of using friendly names. Now what do we want to execute here?

We can type any commands, any operating system commands. So this can be linux commands or windows commands, let's run ls here, now we can see the content of our app directory. But why the app directory? Can you explain why? Here's the answer.

Because earlier in our docker file, we set the working directory to the app directory. So, when we run ls, we see the content of this directory, okay? Now using the same command, we can open up a shell session. So we can say Docker exec, we want to do this in the interactive mode so we can interact with it, our container is c one, and the shell we want to run is sh. Good, so now we have a shell session inside this container, we can run ls, we can run print working directory, we can do anything we want.

Now, when we are done, we type exit, and this doesn't impact the state of our container. Our container is still running, let's verify it. Docker p s, so our c1 container is the first item you see here. So using the exit command, we can execute any commands in a running container.




Codes and Other Notes in this Discussion: 

docker exec c1 ls 

docker exec -it c1 sh 












Stopping and Starting Containers: 

Earlier in the course, I told you that a container is like a lightweight virtual machine, so it can be stopped and then restarted. Let's see this in action. So I'm going to stop our c1 container, good, now let's verify that it's not running, it's not running anymore, great. So now if you go back to our browser and refresh this page, look, our application is not available. Because that container that was serving this application is no longer running.

It stopped. So we need to bring it back up. And that is very easy. We just type docker start c1. Now what is the difference between docker start and docker run?

With docker run, we start a new container, whereas with docker start, we start a stopped container, okay? Next we're gonna talk about removing containers.



Codes and Other Notes in this Discussion: 

docker stop c1 

docker start c1 










Removing Containers:

There are two ways to remove a container. We can type docker container remove, and then specify the name, or we can use the shortcut, docker remove. I prefer this. So, let's go with that, we get an error saying you cannot remove a running container. So here we have two options, One option is to stop the container and then remove it.

The other option is to use the force option. So if you want to force the removal, we bring up the last command and type dash f here. Okay, that container is gone, so let's look at the running containers, it's definitely not here. Now how can we see the stock containers? Do you remember?

Docker ps dash auth. Now even though we have a bunch of containers here, that container c one is not in this list. Now let me show you a cool trick. Let's say you have so many containers here and you only want to look at c one. Here we can use piping.

We talked about piping linux commands earlier in the course. So we can say docker ps dash a, here we type a pipe, and then use grep to filter by c1. Of course, this only works on linux. So, let's go with that, look, we don't have a stub container called c1. Here we also have the prune command, so docker container prune, and with this we can get rid of all the stub containers in one go.

Take a look. Good. So, let's look at the running containers one more time, two containers here and all containers, two containers. Great. Next we're going to talk about persistent data using volumes.



Codes and Other Notes in this Discussion: 

docker container rm c1 

docker rm c1 

docker rm -f c1 

docker ps -a 

docker ps -a | grep c1 

docker container prune 














Containers File System: 

Earlier in the course, I told you that each container has it's own file system that is invisible to other containers. So let's do a quick experiment together. First, we look at the running containers, so on this machine I'm running two containers, I want you to run two containers as well. So if you are not, start two containers from the react app image. Then, I want you to start a shell session on the first container, and write something to a file in the application directory.

Then start a new shell session on the second container and see if that file is there or not. So pause the video and work on this for a minute. Alright, here's my solution. We're going to run docker exec in the interactive mode, the container ID is 655 and we're going to run shell. Now here in the app directory, I'm going to echo data to data dot txt, then I'm going to terminate this session and start a new shell session on the second container, so six e b.

Good? Now, I'm going to list all the files and I want to filter using grab to find all files that have data in their name. Nothing. So data is not here because each container has its own file system that is invisible to other containers and that means if we delete this container, it's file system will also go with it and we'll lose our data. So we should never store our data in a container's file system.

That's what volumes are for, we'll talk about them next.



Codes and Other Notes in this Discussion: 

docker exec -it 655 sh 

echo data > data.txt 


docker exec -it 6eb sh 

ls | grep data












Persisting Data Using Volumes: 

Let's talk about volumes. A volume is a storage outside of containers. It can be a directory on the host or somewhere in the cloud. So let's see how we can work with volumes. We have this command, docker volume, that has a bunch of sub commands, so we can create volumes, we can inspect them, we can list them, we can also prune and remove them.

So let's create a new volume called app data, and of course we can call it anything, it doesn't really matter. Now let's inspect this volume, so, docker volume inspect app data. Now here we have a bunch of properties, we can see when this volume was created, we can see the driver which is local by default, that means this is a directory on the host. We also have drivers for creating volumes in the cloud, so if you use a cloud platform, you need to do your own research and find a driver for creating a volume in that cloud platform, okay? Now look at mount point, this is where that directory is created on the host.

So this is a linux path, if you're on Windows, you would probably see something like c drive backslash program files, whatever. Now a note for my Mac users, earlier in the course I told you that Docker on Mac runs inside a lightweight Linux virtual machine. So this path that you see here is a path inside that virtual machine, it doesn't exist on your Mac. So if you try to go to this directory, you're not going to find anything, okay? So now we have a volume, let's see how we can start a container and give it this volume for persisting data.

So, docker run, we're going to run it in detached mode, I'm going to map let's say port 4,000 of the host to port 3,000 of the container. Now we're going to use a new option, dash v for volume, we're going to map app data to a directory in the file system of the container. So here we type a colon followed by an absolute path in the file system of the container. So we can go to slash app slash data. Now we don't have to explicitly create this volume before running this command, so if I type a new volume here, Docker will automatically create it.

The same is true for this directory, currently we don't have a data directory inside our app directory, so Docker will automatically create this, But there's a problem with this, we'll talk about it in a second. So we give it a volume, and then the image, react app. Good? So here's our container id, let's start a shell session, so Docker exec interactive, 7.16 shell. Good, now, let's go to the data directory, and write something to a file.

So data to data dot txt. We get a permission arrow, can you explain why this is happening? I want you to pause the video and think about it for a few seconds. Here's the reason. Let's go out of this directory, and get a long listing.

So, here's our data directory, now look at the owner of this directory, it's the root user. And over here, as we can see, the owner is the only user that has right permission. So the app user which is the user that runs our app belongs to the others group, and in this group we don't have a write permission. The reason we're facing this issue is because we let docker automatically create this directory for us. So to prevent this from happening, we have to go to our Dockerfile, so back to our Dockerfile, after we create the app directory, we're going to run mkdir data.

We're going to create this directory using the app user that we set earlier in this file. With this, this directory will be owned by this user and it will automatically have the right permission. But now we have to rebuild our image because we have modified our docker file. So, let's get out of this, so Docker build dash t, react app, period. Okay, our build was super fast, great.

So let's start a new container. Docker, run dash t, this time I'm going to use a different port mapping, let's say port 5,000 of the host to 3,000 of the container. And for volume, we're going to map app data to slash app slash data. But this time, the data directory already exists inside the file system of this container, and the app user owns this directory, okay? Now we give it our image, which is react app.

Good, now let's run a shell session inside this container. So docker exec interactive double zero seven shell. Now let's go to the data directory and write something over here. Perfect. Now here's the beauty of volumes, if I delete this container, this file will still exist.

Because this directory is stored outside of this container, it's actually a directory on the host. Let's see that in action. So, I'm going to exit and remove this container. So remove dash force, we have to force it because this container is still running. 007.

That's gone, Now let's start a new container with the same volume mapping. So this is a brand new container, now we're going to run a shell session here, so docker exec interactive e one c shell. Let's go to the data directory, and list, look, our data file is here, beautiful. So volumes are the right way to persist data in Dockerized applications because they have different lifecycles from containers. If we delete a container, the associated volume is not deleted, it still exists.

Also, we can share a volume amongst multiple containers.




Codes and Other Notes in this Discussion: 

docker volume 

docker volume create app-data 

docker volume inspect app-data 

docker run -d -p 4000:3000 -v app-data:/app/data react-app



Dockerfile:

FROM node:14.16.0-alpine3.13
RUN addgroup app && adduser -S -G app app 
USER app  
WORKDIR /app
RUN mkdir data 
COPY package*.json .
RUN npm install 
COPY . .
ENV API_URL=http://api.myapp.com/
EXPOSE 3000
CMD ["npm", "start"] 



docker run -d -p 5000:3000 -v app-data:/app/data react-app


docker exec -it 007 sh 












Copying Files between the Host and Containers: 
Sometimes we need to copy files between the host and a container. For example, let's say we have this log file in our container, and we want to bring it to the host and analyze it. So let's look at the running containers, alright, I'm going to go to this first container, and create a log file in the application directory. Then I'm going to copy that file from the container to the host. So, let's run docker exec interactive shell, now here in the app directory, I'm going to echo hello to log dot txt, so we have this log file inside the app directory, inside the container, and we want to copy it to the host.

Let me show you how to do this. First, let's get out of here, now docker has a copy command for this, here we need to specify a source and a destination. In this case, our source is the container, Now here we add a colon and then type the full path to a file or directory. So we're going to go to our app directory and copy log dot txt to where we use a period to reference the current directory in the host, which is our project directory. Okay, now let's list the files, so here's our log file.

Beautiful. Now we can also copy a file from the host to a container. So let's create a secret file here and copy it to the container. So echo hello to secret dot txt. And of course, this only works on Linux to the best of my knowledge, if you're on Windows, you can just create a file using your favorite text editor, okay?

Now we have a file in our project directory in the host, and this file is not part of our version control system, it's not part of our git repository. So we want to manually copy this file into the container. So I'm going to copy the container name one more time, so one more time, Docker, copy, now this time the source is secret dot txt, and the destination is the container followed by a colon and the path, slash app. Done. Now let's run a shell session and list all the files.

So here's our secret file inside the container's file system.



Codes and Other Notes in this Discussion: 

docker cp e1c9043ea8ce:/app/log.txt . 

docker cp secret.txt e1c9043ea8ce:/app












Sharing the Source Code with a Container: 

So we have this react application available at local host port 5,000, now let's see how we can publish our application changes. So back to v s go, we're going to go to the public directory and open index dot html. Here I'm going to update the title of this app to dockerized react app. This title is what we see in the browser window. So let's save the changes, now back in the browser, if we refresh, we don't see anything.

We still see the old title. So what should we do here? Well, for production machines, we should always build a new image, tag it properly, and then deploy it. We'll talk about that later when we get to the deployment section. But what about development machines?

We don't want to rebuild the image every time we make a tiny change in our code. That's just too time consuming. Now you might say, what about copying? Well, that's a pain in the butt as well. We don't want to manually copy files from our development machine into a container every time we change our code.

So let me show you a cool trick. We can create a mapping or a binding between a directory on the host and a directory inside the container. So this way, any changes we make to any files in this directory are immediately visible inside the container. So back to the terminal, we're going to start a new container. Docker run dash t, this time I'm going to use a different port so we can distinguish it.

We're going to map this port to port 3,000. Now earlier we used a volume mapping, so we mapped the app data volume to slash app slash data directory inside the container. Now using the same syntax, we can map a directory on the host to a directory inside the container. So instead of using a named volume, which is a directory that docker manages, we're going to use the current directory, the directory that holds our application. So we don't want to type the full path here, this is where we use the p w d command, print working directory.

Now if we execute this command as is, docker thinks this is a named volume, we don't want that, we want a full path. So we're going to wrap this with a dollar sign and put it in parenthesis. Now when we execute this command, this part of the command will be evaluated first and the result will be a full path to the current working directory. Then we're going to map this to not the data directory, we're going to map it to the app directory inside the container. Okay?

Now what about a named volume? Well we can still add another option and use a named volume here if you want to, but this react application doesn't really store anything on the disk, it doesn't have a database, it's just a basic front end application. So we don't need a named volume here. Now, let's add the image, good, so here's the container id, let's look at the logs, this time I'm going to use the follow option so we can see the changes as they come up. Okay, we're waiting for the web server to start, so I'll be right back.

Alright, our web server is ready, so let's go to local host, port five thousand and one. Alright, we can certainly see the updated title, now let's make one more change to the applications code. So I'm going to add an exclamation mark at the end, save the changes, now back in the browser, I didn't even have to refresh. Look, we've got the exclamation mark here. Because in react we have this feature called hot reloading, so whenever we change any of our files, our application gets automatically reloaded in the browser, okay.

So to share our source code with the container, once again we use the volume option to map the project directory to a directory in the container's file system. Now you might say, but Mosh, running applications with docker is a pain in the ass. We have to remember all these options and type them every time we want to run an application. No No, you don't. In the next section, we're going to talk about docker compose, and you will see how easy it is to bring up an application with multiple components.

I had to cover all these commands and options so when we get to the next section, you know exactly how docker compose works under the hood. So, I will see you in the next section.




Codes and Other Notes in this Discussion: 

docker run -d -p 5001:3000 -v $(pwd):/app react-app












Summary:

Containers

Running containers:

docker run <image>
docker run -d <image> # run in the background
docker run —name <name> <image> # to give a custom name
docker run —p 3000:3000 <image> # to publish a port HOST:CONTAINER



Listing containers:

docker ps # to list running containers
docker ps -a # to list all containers



Viewing the logs:

docker logs <containerID>
docker logs -f <containerID> # to follow the log
docker logs —t <containerID> # to add timestamps
docker logs —n 10 <containerID> # to view the last 10 lines



Executing commands in running containers:

docker exec <containerID> <cmd>
docker exec -it <containerID> sh # to start a shell 




Starting and stopping containers:

docker stop <containerID>
docker start <containerID>





Removing containers:

docker container rm <containerID>
docker rm <containerID>
docker rm -f <containerID> # to force the removal
docker container prune # to remove stopped containers




Volumes:

docker volume ls
docker volume create app-data
docker volume inspect app-data
docker run -v app-data:/app/data <image>




Copying files between the host and containers:

docker cp <containerID>:/app/log.txt .
docker cp secret.txt <containerID>:/app



Sharing source code with containers:

docker run -v $(pwd):/app <image>




























Running Multi-Container Applications: 
--------------------------------------

Introduction: 

Welcome back to another section of the ultimate Docker course. In this section we're going to talk about running multi container applications. So I'm going to give you a real world application with three building blocks. A front end built with React, a back end built with Node and a MongoDB database. Once again, you don't need to be familiar or use any of these tools.

Our focus here is on Docker and not on development tools. I think this is the most exciting part of this course where you can see everything coming together. We'll talk about Docker Compose for building and running multi container applications. We'll also talk about Docker networking, database migration and running automated tests. So, let's jump in and get started.



In this Section: 

Docker Compose 
Docker networking 
Database migration 
Running automated tests










Installing Docker Compose:

In this section we're going to use a tool called docker compose which is built on top of docker engine. It makes it incredibly easy to start applications with multiple containers. So Google Docker compose install, you will find this page, docs.docker.com/compose/install. On this page you can see the installation instructions, now at the time of recording this, Docker compose is shipped with Docker desktop for Mac and Windows. So if you're on Mac or Windows, you don't have to do anything extra, you already have Docker compose.

To verify it, just go to the terminal window and type docker compose dash dash version. So I'm running docker compose version one point twenty eight point five, make sure your version is the same or newer. Now if you're using an older version, again Google upgrade docker compose. Or you might just install the latest version of Docker. Now, back to this page, if you're using Windows Server or Linux, there are specific instructions you have to follow to install Docker compose.

So go ahead and install Docker compose and I will see you in the next lesson.
 


Codes and Other Notes in this Discussion: 

docker compose install --> https://docs.docker.com/compose/install/


docker-compose --version 



















Cleaning Up our Workspace: 

Before we get started, I want to show you a couple of techniques for cleaning up our workspace. So on this machine we have a bunch of images and some running containers. They're getting in the way, I want to get rid of them all. How do we do this? Well, you know that we can remove images using docker image remove command.

And here we can type one or more image id's. Now how can we get all image id's and pass them here? Let me show you a cool trick. So we can run docker image ls, we see all the images, right? But if you pass dash q at the end, we only get image ids.

Now we can pass this as an argument to docker image remove. So, docker image remove, now here we add a dollar sign, and in parenthesis, we type that other command. So docker image ls dash q. Now if we run this we're going to get an error, because some of these images are already in running containers or stopped containers. So we should always remove containers first.

We're going to do that using the same technique. So I'm going to replace image with container, so we get all container id's and then we're going to remove them all in one go. Also, I would like to add dash a here as well, this will bring stop containers as well. Okay? We can also combine switches, that's another technique, let's go ahead, alright, we get an error saying you cannot remove a running container because I forgot to pass the force option.

So, let's bring this up one more time, when removing, we're going to use dash f, okay. Great, so all these containers are removed, now let's remove the images. So, docker image ls and docker image remove. Great, now take a look, we don't have any images here, and no containers. Including stopped containers.

So we have a clean workspace. That's one way. There is a shortcut for this as well. If you're on Mac, you can find the docker icon on the top status bar, if you're on Windows, you will find it in your notification tray. Let's click on this and then go to preferences.

Now on this page, let's click on the troubleshoot icon, on this page we have a bunch of useful utilities, for example we can restart Docker desktop, we can also clean and purge data, This will essentially remove everything in Docker, your images, your containers, your volumes and so on. Now, be aware of that, if you click on this, this is going to restart Docker engine, so on the top look, you can see this animation showing that the Docker engine is not started yet. So at this point, if you go to the terminal window and execute any of Docker commands, you're going to get an error. So you'll have to wait about half a minute for the Docker engine to start, that's another way. So now that we have a clean workspace, next we're going to talk about our application.




Codes and Other Notes in this Discussion: 

docker container rm -f $(docker container ls -aq)

docker image rm $(docker image ls -q) 












The Sample Web Application: 

So in this section we're going to look at a real world application with multiple building blocks. A front end, a back end and a database. So below this video I've attached a zip file, go ahead and download it, inside that zip file you're going to find this folder structure. We have this backend folder which is our node project, this is a basic node project that starts a web server on port 3,001. Once again, you don't need to know node to go through this section.

Then we have the front end project which is a react application that talks to the back end. Now, if you want to run this application outside of docker, there are a number of steps we have to follow. Let's say we just check this out from a GitHub repository. First we have to go to our backend project, install all dependencies and then start the web server. At the same time, we have to open up another terminal window and do the same steps with our front end project.

So we have to go to the front end project, install all the dependencies and then start the web server. And of course, we need two more terminal windows for running our front end and back end tests. And not to mention that, we should also download and install MongoDB on this machine. So there are so many steps we have to follow the moment we check out the source code from our GitHub repository. Now with docker, we don't have to do any of these things.

All we have to do is run a single command. Let me show you. So I'm going to get outside of the front end folder. Now we are in the root of this project. If you look, here we have a file called docker compose, which is used for composing a multi container application.

We're going to talk about that in detail soon. Now once we have this file in our project, we can simply run docker compose up. That's all we have to do. Now, docker is automatically downloading this particular version of MongoDB, so it's downloading all these layers, then at the same time, it's going to install all the dependencies for our front end and back end projects, it will start web servers and run automated tests all in this window. Now this is going to take a little while, so I'll be right back.

Alright, our application is up and running, and we can access it at local host port 3,000. So, here's what we get, we have a mini application for managing a list of movies. Now you know what's the beauty here? The beauty is that our database is populated with these movies as part of bringing up our application. I didn't have to manually insert these movies in our database.

So we have a migration script for populating our database, and Docker automatically executed our migration script as part of bringing up this application. This is a very common real world scenario. Now here we can add new movies, movie one, movie two, whatever, And we can also delete these movies. So we brought up this application using a single command. Now I briefly mentioned this file, docker compose dot yamo.

Before we talk about this file, first you need to understand the yaml format. This is a format that a lot of people are not familiar with. So in the next slide, we're going to talk about JSON and yaml formats.




Codes and Other Notes in this Discussion: 

docker-compose up 
















JSON and YAML Formats: 

Let's talk about JSON and YAML formats. If you know these formats well, feel free to skip this lesson. So, in the root of this project, we're going to add a new file called data dot JSON, now JSON as you probably know is a language, it's a human readable language for representing data. So in this JSON file, we can have an object or an array, let's say we want to represent a course, a course can have properties like name, price and so on. On.

So in this object, we can add one or more key value pairs. Our keys should always be surrounded in double quotes. So we can add a key called name, and set it's value to, we can use a string, the ultimate docker course, then we add a comma to define the next key value pair. So we can say price, we can set this to a number, now the value can also be a boolean, so we can define another key value pair, and set the value to true or false, we can define another key value pair and set the value to an array. So we define an array using square brackets.

Now in this array we can have any valid objects, so we can have strings, numbers, booleans, or other objects. So I'm going to add a couple of strings, let's say software and dev ops. And one last key value pair. Author, I'm going to make this an object, so once again we use curly braces to define an object. In this object we add a couple key value pairs, first name is Mosh, and last name is what?

Hamadani. And yes, I am Iranian, I get that question all the time. Alright, so here we have a JSON file, now let's see how we can convert this to yaml. Yaml is another language for presenting data, but it has less clutter then JSON, it's easier to read. So, I'm going to copy all this code, here in the project, we're going to add a new file called data dot yml, the extension can be yaml or yml.

Now, on the top, we add three hyphens to indicate the beginning of a yaml file. Then we paste our code, now in yaml, we don't use curly braces to indicate hierarchy. This idea has come from Python. If you have programmed in Python, you know that in Python, we use indentation to represent hierarchy. So we don't have curly braces.

So, let's get rid of these braces, and remove the indentation, good, now the next thing you need to know about yaml is that we don't have to use quotes. So we can bring up the replace dialog, and replace all these double quotes with nothing. That immediately takes a lot of clutter away. Also, we're not going to use commas to separate key value pairs, so on the top we have name, price, is published, and how do we represent a list or an array? We use hyphens, so I'm going to remove this, we press enter, add a tab, on a new line, we type hyphen to define the first item in the list, software, then at the same indentation, we add the next item, devops.

Now author is an object, but as I told you, we don't use curly braces, we use indentation. So because these two properties are indented, they belong to the author property, okay? So this is our yaml file, let's compare this with JSON. As you can see, yaml is easier to read and understand. Now, why don't we use yaml all the time?

Well, because parsing yaml files is a little bit slower than parsing JSON files. Because the parser doesn't know if this is a string or a number. So it has to read everything as a string and then try to evaluate it. In contrast, in JSON, strings are represented using quotes, and more specifically double quotes. So the parser knows that, this is a string and it shouldn't evaluate it.

Okay? So quite often we use yml files for configuration files, and json for exchanging data between multiple computers, like a client and a server. So, now that you understand these formats, next we're going to talk about compose files.




Codes and Other Notes in this Discussion: 

data.json: 

{
	"name": "The Ultimate Docker Course", 
	"price": 149, 
	"is_published": true, 
	"tags": ["software", "devops"],
	"author": {
		"first_name": "Mosh",
		"last_name": "Hamedani"
	}
}



data.yaml: data.yml 

name: The Ultimate Docker Course
price: 149
is_published: true
tags: 
  - software 
  - devops
author: 
  first_name: Mosh
  last_name: Hamedani
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  

Creating a Compose File: 

Alright. Let's see how we can create a compose file from scratch. So for this lesson, I'm going to rename this file to underline docker compose. We want to set it aside and create a new compose file from scratch. So, here we add a new file, called docker dash compose, all in lower case, make sure to spell it properly, otherwise docker compose is not going to find this file.

Because this is the default name that Docker compose assumes. Okay, so yaml, now the first thing that we need to set here is the version property. What version should we use? Well, let's search for Docker compose file, on this page, you can see various compose file formats and their compatibility with Docker engine. We are using the latest version of Docker engine, so I want to use the latest compose file format so we have access to the latest features.

So we're going to set this to 3.8. Now here we need to wrap this number with double quotes, otherwise it will be evaluated as a number. But docker compose expects this value to be a string. Why? I have no clue.

So here's a version, now in this file we define various building blocks or services of our application. So we have a property called services, now what services do we need here? Well, our application has a front end, a back end, and a database. Now your application might have other moving parts, so you can define them here. Now, these names are arbitrary, so we can call them anything, we can change this to db, we can change the back end to API, and the front end to, well, the idea here is that we're defining various services and telling Docker how to build images for each service and how to run these images.

So here we're going to have properties, and the value of these properties will eventually be used when running our containers. So in the previous section, we had to manually run our containers using docker run, and here we used parameters like dash p for port mapping or dash v for volume mapping, we also had to specify an image like react app, all these values can be defined in our compose file. So we don't have to manually start our containers. Docker compose will take care of starting our containers under the hood, okay? So for each service, we need to tell docker how to build an image for that service.

So here we can use the build property and tell docker compose where it can find a docker file, so if you look at this project, you can see that in our back end and front end folders, we have a Dockerfile. This Dockerfile is almost identical to the one we created in the previous section. So we start from a node image, we create a user, we set our working directory, copy all the files and install the dependencies, then expose port 3,001 and start the web server. We have a similar docker file in our front end project. Let's have a quick look, so that was the back end, here's the front end, and here we have a dockerfile.

Almost identical, but the front end application or the front end server starts on a different port. That is the only difference. So each service should have it's own dockerfile, okay? Now, back to our compose file, for our web or front end, we're going to set the build property to period, meaning current folder, slash front end. This is where we have a docker file.

For our API, we're going to set build to back end, now for our database, we're not going to build an image, we're going to pull an image from Docker Hub. So instead of the build property, we're going to use the image property. Now for this application I'm going to use mongo version four point zero dash zenio. So that is mongo version four built on top of zenio which is ubuntu version 16. Now if you look at Docker Hub, you can see that Mongo also has images built on top of Windows, but Windows images are very large, over two gigabytes.

So that's why I prefer to use Linux images. So for any of these services, we can either build an image or pull it down. Now here we also have port mappings, so we set ports two, now because we can have multiple port mappings, here we need to use the array or list syntax, so we use a hyphen and then define a port mapping. So our front end application starts on port 3,000, so I want to map port 3,000 of the host to port 3,000 of the container running this image. That's similarly for our API, we're going to define a port mapping, this one is going to be 3,001 to 3,001.

Now, MongoDB by default, listens on port twenty seven zero seventeen, so I want to map the same port, so we can access mongodb using a mongodb client like mongodb compass. Now if you don't use mongodb, you have the same concept with other database engines. All these database engines listen on a default port, you want to map that port so you can connect your database engine using your favorite database client. Okay? What else do we have here?

Back to this page for compose file, if you look at version three, on the right, you can see all valid properties. Now a lot of these are for really special cases, so you don't need to use them all the time, but the ones that we use most of the time are build or image, we also use ports, volumes, environment and so on. So, our API project needs an environment variable that tells where our database is. So here we set environment, now here we can use the list syntax because we can have multiple environment variables, so we set db underline URL to, here we need to type a MongoDB connection string. These connection strings always start with mongodb colon, two forward slashes, here we need to type the name of a host.

So as I will show you later in this section, when we start an application with docker compose, under the hood, a network is created. On this network we're going to have three hosts. The name of these hosts are equal to the names we have defined here. So we're going to have a host called db, so that is the connection string to our mongodb server. Now, on this server we can have multiple databases, so we're going to specify the database name and the connection string as well.

So this is one way to set an environment variable. But instead of using the list syntax, we can also use the object or property value syntax. So we get rid of the hyphen, we say d b url is a property, and this is the value of that property. I find the syntax more readable because we get color coding and it's just cleaner. Similarly, we can add additional environment variables.

Now, we're almost there, the last thing we want to add here is a volume. Because we don't want mongo DB to write data to the temporary file system of the container. So, here we set volumes, and again, we can have one or more volume mappings, so we add a hyphen, we're going to map a volume called Vidly, and of course we can call it anything, Vidly is the name of this application in case you didn't notice. So we're going to map this volume to a directory inside the container. Now if you look at the documentation of MongoDB on Docker Hub or just a typical MongoDB documentation, you know that by default, mongo d b stores this data in slash data slash d b.

So we want to map this volume, through this directory, so whatever that is written inside this directory is actually outside of this container, It's somewhere else in our volume. Now because we have used this volume here, we have to define it in our compose file. So we press enter, remove all the indentations, so now we are at the same level as services. Here we're going to define another property called volumes, and here we're going to add another property called bitly with no value. I know this looks a little bit weird, but this is the syntax we have to follow.

We just have to define the volume first before we can use it. So, this is our compose file, now we can make this more readable by adding line breaks in between these properties, we can also order these services any way we want, so currently I'm ordering them from front to back, we can also order them from back to front, so we'll put database first, then API and then web. So we're done with our compose file, next I'm going to show you how to build the images.




Codes and Other Notes in this Discussion:   

docker-compose.yml 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 3000:3000
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db

volumes:
  vidly: 

	  
For references: https://docs.docker.com/reference/compose-file/



















Building Images: 

Earlier I told you that docker compose is built on top of docker engine. So everything we have done with docker engine like building images, listing them, starting containers and so on, All of these operations are also available using Docker compose. Let me show you. So we type Docker compose without any arguments, enter, look, we have all the subcommands like we have r m for removing stopped containers, we have run, we have push, pull and so on. The difference is that any of these commands will apply to our application as a whole, so most of these commands will impact multiple services or multiple containers in our application.

So let's look at Docker compose build, and also use the help option. So we have a bunch of options here, a couple of them I want to point out that are useful to know is no cache, with this we can prevent caching when building the image. Sometimes you encounter weird issues and you want to make sure that cache is not used, in that case, use this option. Another useful option is dash dash pull, with this we can always pull a newer version of the image. That is also good to know.

So in this lesson I'm not going to use any of this, we're just going to run docker compose build. This built our web and API services, and as you noticed, our build was super fast. Because pretty much everything came from the cache. So let's run docker images, So I have five images on this machine. Vidli front end, Vidli web, Vidli API, Vidli backend and Mongo.

Mongo obviously came from Docker Hub. Now as part of this build process in this lesson, we built with lib web and with lib API. These two other images with the front end and back end were built when we started this application earlier. So back to our project, in this original compose file that I included in this project, look, I call these services front end and back end instead of web and API. That is why we have these two images, with the front end and with the back end.

Also, as you have noticed, when building images with docker compose, our images are prefixed with the name of our application. Now where does this come from? It is the name of the directory. So currently we are inside a directory called withly and that is why all these images are prefixed with withly. I think this is a great convention.

Now I've got a question for you, if you look at the created column, you can see all these images were created an hour ago. But didn't we just build the web and API images? Why do you think this happened? Here's the answer, because I built these images front end and back end an hour ago when I was recording the first lesson in this section, now when building these new images, docker used everything in the cache because all those files were already available, all those layers were there, so Docker didn't have to do a full rebuild. That is why we are still using the build from an hour ago.

Okay? Now, if you want to force a full rebuild, we can say Docker compose build dash dash no cache. Alright, this is going to take a few seconds, so I'll be right back. Alright, our images are built, so let's run Docker images. There you go.

Look at the first two images, API and web, were built less than a minute ago. So that's all about building images, next we're going to talk about starting the application.




Codes and Other Notes in this Discussion: 

docker-compose 

docker-compose build --help 

docker-compose build 

docker-compose build --no-cache 
















Starting and Stopping the Application: 

You briefly saw how we can start an application with docker compose. We just type docker compose up. Now if the images are ready, docker compose will run them inside containers, otherwise it's going to build the images automatically. Now, before executing this, let's look at the available options. So here we have a ton of options, a couple of them that are useful are built, with this we can force a rebuild every time we want to start our application, so we don't have to explicitly run docker compose build and then up.

We can combine the two using the build option. The other useful option is dash d for detached mode. So we will start these containers in the background. So, take a look, alright, now if we run docker compose ps, we can see all the containers relevant to this application, in contrast, if you type docker ps, we can see all the running containers across all applications, okay? So here we have three containers, vidly, API one, vidly db one, and web one.

Now what does this one? Well, we can start multiple containers from the same image. And this is used for high availability and scalability. It's something we'll look at in the future. So here you can see the container, you can see what command started that container, so for our API, that was npm start, for our database, that was mongod or mongo daemon process, and for our web front end, that was npm start as well.

You can see all these containers are up and running, and over here you can see port mappings. So now if you go to local host, port 3,000, we can see our application. Beautiful. Now, how do we take this down? Let's say we're done with this application, and we want to free up resources.

Back to the terminal, we type docker compose down. This will stop and remove these containers, but the images are still there. So next time we want to start the application, our application will start pretty quickly.



Codes and Other Notes in this Discussion: 

docker-compose up --build 

docker-compose up -d 

docker-compose ps 

docker ps 

docker-compose down 























Docker Networking: 

Let's talk about networking in Docker. Let me run our application with Docker compose, Docker compose will automatically create a network and add our containers on that network, so these containers can talk to each other. Let's see this in action. So I'm going to bring up the application one more time, in the detach mode, good, now look at the first line. Create a network with the default.

So, we can run docker network ls, here we can see all the networks on this machine, I think every docker installation has three networks, bridge, host, and none. Honestly, I'm not sure what these networks are for, but what matters here is that we have a network called vidly default. The driver for this network is bridge on Linux or nat on Windows. Now, this network contains three hosts or three containers. Web, API, and DB.

So these hosts, or these containers can talk to each other using their name. Let's see this in action. So back to the terminal, let's look at the running containers, so we have mongo, web, and API. Now we're going to start a shell session on the web container and ping the API container. Take a look.

So, we're going to execute in the interactive mode, the container id is eight cs6, and we're going to run shell. So let's ping API. We get a permission error, because we have logged in with the app user, that comes from our Dockerfile, remember? So we have logged in with the app user and this user doesn't have pink permission. So let's exit.

I'm going to bring up the last command, now here we have to use an extra option for setting the user. We're going to log in as the root user. Good. Now look at the shell prompt, we have a pound sign, which means we have the highest privileges. So here we can ping API.

Now look, we're getting responses from a machine with this ip address. Now on your machine, this ip might be different. Now, let's press control s c to get out of this. So this is what happens under the hood. Docker comes with an embedded dns server that contains the name and IP of these containers.

Now inside each container, we have a component called the DNS resolver. This DNS resolver talks to the DNS server to find the IP address of the target container. So when we ping the API container, this dns resolver asks the server what is the ip address of the API machine or API container? The dns server returns the ip address, and then the web container can directly talk to the using it's ip address. So each container has an ip address and is part of a network.

Let me show you one more thing. So back to the terminal, here we can run ifconfig to see the ip address of this container. Take a look. So this container has two network adapters, one of them is ethernet zero, and over here, you can see the ip address of this container. So one seven two twenty one zero two is the IP address of the web container.

Now back to our compose file, earlier when we defined the API service, we added an environment variable that contains a database connection string. In this connection string we have d b, which is the name of a host, that is the d b host or the d b container. You saw that our API container can talk to this container because both these containers or all containers in this application are part of the same network. Now one thing I want you to understand here is that this host is only available inside the docker environment. So if I open up my browser and go to local host slash d b, I'm not going to get anything.

So the API container can directly talk to the db container, but if you want to access this container, we need port mappings. And that is why we have this port mapping over here. So this port on the host is mapped to this port on the container. So if you open up mongo DB compass, which is a popular mongo DB client, we can establish a connection to local host port twenty seven zero seventeen, because this port is mapped to our container. Let's verify this real quick.

So connect, great, so here we can see all our databases, here's our fitly database, and in this database we have a collection called movies with four documents. So, here are the movies that we currently have in the database. So this is all about docker networking, next we're going to talk about viewing logs.




Codes and Other Notes in this Discussion: 

docker-compose up -d 

docker network ls 















Viewing Logs: 

Let me show you a few different techniques for viewing the logs. So if we type Docker compose logs, we can view the logs across all containers of this application in one place. Take a look. So, here we have some messages coming from our database container, above that, where is that? We have some messages coming from our web container, and these are color coded, it's beautiful.

Now, let's bring up this command and add dash dash help at the end to view the available options. Here we have the same options we talked about earlier in the course. So using dash f or follow, we can follow the log output, so we can continuously see new messages as they come out. Also, we can add a time stamp using dash t. Now, what if we don't want to see the logs for all these containers in one place?

Some people have big monitors or multiple monitors, they prefer to have different windows for different container logs. That is very easy. So we look at the running containers, let's say we only want to look at the logs for our web container. So we can say docker logs, here's the container id, HC6. And of course, we can follow this log as well.

So here is the log for this container. We can put this on a separate window or on a separate monitor. So that's all about logs, next we're going to talk about publishing changes.




Codes and Other Notes in this Discussion: 

docker-compose logs 

docker-compose logs --help 

docker logs 8c6 -f 











Publishing Changes: 

Alright, let's talk about publishing changes. So obviously, we don't want to rebuild our application images every time we change our code. So we're going to map our project directory like the back end directory to the app directory inside our container, exactly like before. This way, any changes we make in this directory are immediately visible inside our container. And of course we have to do the same with the front end directory separately.

So, let's open up our compose file, here's the definition of our API service, in this definition, we're going to add a new property, volumes. Now we can have one or more volume mappings, so here we need the list or array syntax. Now here we can type a relative path, so starting from the current directory, we go to the back end directory, and map this to the app directory inside the container. Now in contrast, earlier when we started our containers manually, we had to type an absolute path. So there, we had to use this syntax to get the current working directory and map it to the app directory.

Remember? With compose files, we don't have to do this. So we can type a relative pass here, that's a lot easier. So that's pretty much all we have to do to share our source code with our container. There's just one tiny problem.

Let's go ahead and start our application, so docker compose up, and I'm not going to use the detached option because we're going to encounter an error and I want you to see that immediately. So let's bring up the application, great, we saw the error, look, it says nodemon not found. What is this? Well, nodemon is one of the packages that our backend project is dependent on. So back to our project, here in the backend folder, we have this package dot json and if you pay close attention, you can see under dev dependencies or development dependencies, we have a dependency to node mon or node monitor.

Node mon is a utility that watches all of our files and anytime it detects a change, it restarts our node server. Now, this error is saying that node mon cannot be found, can you tell why? Here's the reason, in the backend folder, we don't have the node modules folder, right? Because we haven't installed npm dependencies on this machine. These dependencies were only installed inside a docker container, but now that we're sharing our applications code with our container, what the container sees is essentially this directory.

And here we don't have the node modules folder. So back to the terminal, let's stop this by pressing control l c, first we go to the backend directory and run npm install. We can also type npm I, that's a shortcut. Alright, all our dependencies are installed, so let's bring up the app one more time. Alright, over here you can see node mon is starting our node process.

So let's go to our browser and head over to local host port three thousand and one slash API. This is like the homepage of our API. So our API is running, now let's make a small change in our code and see if that change is visible here or not. So, back to our project, here in the back end folder, let's go to route and then index dot js. This is where we can change that message.

So API is running, I'm going to add a few exclamation marks, save the changes, now in the terminal, you can see node one detected our change and restarted our web server. So back to the browser, refresh, we can see the changes, beautiful. Now we can apply the same technique to our front end project, these steps are repetitive, so I leave it up to you as an exercise.




Codes and Other Notes in this Discussion: 

docker-compose.yml 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 3000:3000
	volumes: 
	  - ./frontend:/app
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	volumes: 
	  - ./backend:/app
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db

volumes:
  vidly: 



















Migrating the Database: 

Most of the time when we release our application, we want our database to be in a particular shape with some data, this is called database migration. Now in this backend project, I'm using a database migration tool called migrate mongo. So if you look at package dot json, over here, under dev dependencies, you can see we have a dependency through this package, migrate mongo. We have a similar tool in other development stacks, for example in dot net we have entity framework, in Django we also have database migrations and so on. So you should probably be familiar with these migration tools, but if not, let me give you a two minute summary on how they work.

So using these migration tools, we can create database migration scripts. In this project, our migration scripts are stored in this migrations folder. So currently we have one script, this is just a basic javascript file, in each file we have two functions, up for upgrading the database and down for downgrading it. So in the up function, we're going to the movies collection and inserting these movie objects. And obviously, when downgrading the database, we're going to remove these movies, okay?

Again, don't worry about the javascript code, think about the concept. So we have one script which is called populate movies, and as you can see it's time stamped. Now, using a tool like migrate mongo, we can go to the terminal and say migrate mongo up, This will look at our migrations folder and execute all migration scripts. Now what if we execute this command multiple times? Well, our scripts are not going to be executed twice.

Because if you look at our database, look, here we have a collection called change log, in this collection we have the scripts that have been executed. So migrate mongo will not execute the script multiple times, okay? Now back to package dot json, in the script section we have these commands which are aliases for the command in front of them. So here we have db up which is mapped to migrate mongo up. So here in the terminal we can say migrate mongo up or we can say npm run db up.

With npm run we can run any of the commands in package dot JSON, okay? Again, if you don't use node or npm, you have a similar concept in your development stack, okay? So that's the item migration tools. Now how can we execute the database migration as part of starting our application? Well, let's look at our docker file in the back end project.

So here's our docker file, we talked about this before, we simply call npm start and this will start our web server. Now in our compose file, we can override the command that we have here and do something else. So let's bring up our docker compose file, here's the definition of our API service, we can add a new property here, command, and here we can override the default command in our docker file. So here we can say migrate mongo up and npm start. Now there is a problem here, it is possible that our database server is not ready at the time of executing this command.

So even though our database container might be running, the actual database engine, in this case Mongo, may not be ready because starting a database engine often takes several seconds. This is where we use a waiting script. So if you search for docker wait for container, on this page, which is part of docker documentation, you can find a list of tools that give you that capability. So we have wait for it, dockerize, shell compatible, wait for as well as relay and containers. And of course there is more.

I've played with these tools and the one that has worked for me personally is wait for it. If you look at this, this is just a github repository and includes a shell script, wait for it dot sh. With this script, we can wait for our database engine or any other processes to get ready before doing some work. So back to our docker file, before migrating our database, we want to use our wait for script. Now if you look at our project, you can see I've stored the wait for script here inside the back end folder, so here we can say wait for, then we specify the name of our computer or host which is db, that is the name of our container, right?

Now we want to wait for port twenty seven zero seventeen to receive traffic. That is the default MongoDB port, right? Once this is ready, then we're going to migrate Mongo. Now as you can see, this command is a little bit too long, so there's another way to actually get the same result. We can create an entry point script, so once again, in this project I've supplied a file called docker dash entry point dot shell, this is a basic shell script.

So first we echo a message saying waiting for MongoDB to start, and this is where we're using the wait for script. Now that Mongo is up and running, we print a message saying we're migrating to the database, so npm run dp up, and finally we start our web server. Now, we can go to our compose file and simplify this command. All we have to do is run our Docker entry point script. Now this is for Linux, if you're on Windows, you can write the powershell script to do the same thing, okay?

So this is how we can migrate the database. Now to verify this is working, let's go back to the terminal, stop this process, now I'm going to bring down the application, so this has removed our containers like API, DB and web, as well as our network. But our volume is not deleted. So Docker compose doesn't automatically delete the volume in case we have some data we don't want to lose. So let's look at all our volumes, alright, here we have bitly, underline bitly, so the first bitly represents our application, and the second bitly is the name of our volume, okay?

So let's go ahead and remove this volume, bitly, underline bitly, good, so now our mongodb database is gone. So now let's restart the application and see if we get a fresh mongodb database populated with our movies. So, docker compose, let's pray that everything is going to work, alright, I think we are almost ready. Okay, so back to the browser, we can go to local host, port 3,001 slash API slash movies, that's our endpoint, beautiful. So all these movies are here, we can also access our front end application at port 3,000, refresh, beautiful.

So our migration is working perfectly. Next we're going to talk about running tests.




Codes and Other Notes in this Discussion: 

docker-compose.yml 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 3000:3000
	volumes: 
	  - ./frontend:/app	
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	volumes: 
	  - ./backend:/app
	command: ./wait-for db:27017 migrate-mongo up  && npm start 
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db

volumes:
  vidly: 
  
  
  
docker wait for container --> https://docs.docker.com/reference/cli/docker/container/wait/



docker-entrypoint.sh:

#!/bin/sh

echo "Waiting for MongoDB to start..."
./wait-for db:27017

echo "Migrating the database..."
npm run db:up

echo "starting the server..."
npm start 




docker-compose.yml: 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 3000:3000
	volumes: 
	  - ./frontend:/app 
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	volumes: 
	  - ./backend:/app
	command: ./docker-entrypoint.sh
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db

volumes:
  vidly: 
  
  

docker volume rm vidly_vidly 

docker compose up 



















Running Tests:

Alright, the last thing we're going to talk about in this section is running automated tests. So for both these projects, our front end and back end project, I've written a bunch of unit and integration tests. So we can go to our, let's say front end directory and run npm test. Alright, we have nine beautiful passing tests, so let's press q to get out of this. Good.

So this is one way to run these tests and that's how I like to run my tests. Now, I've seen some people run their tests inside a docker container. I've done this before and found it to be very slow. We want our tests run quickly so we can get rapid feedback as we are writing code. But in case you are curious, let me show you how you can run your tests inside a container.

So back to our compose file, here's the definition of our web service. I'm going to duplicate this and then rename the second instance to web tests. Now, we're not going to build a new image, we want to reuse an existing image. The image that was built for this service. So we're going to change the build property to image, now what is the name of the image?

Vidly underline web. Now do we need ports? No we don't, because we don't want to access the service directly like accessing our web service. Delete. What about volumes?

We want to keep this here so any changes we make to our application code or our tests are immediately visible inside this container, okay? Now here we're going to overwrite the default command for this image and run NPM test. So save the changes, now let's get out of the front end directory and run Docker compose up. Alright, our application is up and over here, you can see the output of our web tests container. So all of our tests are passing, but if you go to our application code and make a change, it takes a little while until these tests are rerun.

I find this to be a little bit slow for my taste, but if you like this approach, by all means go for it. The good thing about this approach is that you don't need to open up different windows and run tests for front end and back end separately. So just like we defined a service called web tests, we can define another service called API tests. So when we start our application, both the front end and back end tests get executed and we see the result all in one window. But again, this is a little bit too slow.

So that's all about running tests, we reached the end of this section, in the next section we're going to talk about deployment. So I will see you in the next section.



Codes and Other Notes in this Discussion: 


docker-compose.yml: 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 3000:3000
	volumes: 
	  - ./frontend:/app 
  web-tests: 
    image: vidly_web 
	volumes: 
	  - ./frontend:/app 
	command: npm test 
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	volumes: 
	  - ./backend:/app
	command: ./docker-entrypoint.sh
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db

volumes:
  vidly: 
  
  


docker-compose up 










Summary:

Multi-containers apps

Docker Compose commands

docker-compose build
docker-compose build --no-cache
docker-compose up
docker-compose up -d
docker-compose up —build
docker-compose down
docker-compose ps
docker-compose logs























Deploying Applications: 

Welcome back to the last section of the ultimate Docker course. In this section, you're going to learn how to deploy your dockerized applications. So we're going to take the same application we have been working with and put it in the cloud. We'll be talking about various deployment options, getting a virtual private server or VPS, using Docker machine to provision hosts and connect with them, creating optimized production images, and of course, deploying the application and its updates. I think this is the most exciting part of this course because by the end of this section, our application will be live on the Internet with its front end, back end, and database.

So let's jump in and get started.


Introduction: 
Deployment Options
Getting a Virtual Private Server (VPS)
Using Docker Machine 
Creating optimized production images 
Deploying the application 











Deployment Options: 

To deploy our docker as applications, we have two options. We can deploy our application to a single host or cluster, which means a group of servers. As you might guess, deploying to a single host is really easy, but the problem with a single host deployment is that if our server goes offline, our application will not be accessible. And also, if our application goes rapidly and we get hundreds of thousands of users, a single server is not able to handle that load. That's why we use clusters.

So with clusters we get high availability and scalability. Now, to run clusters, we need special tools called orchestration tools. Docker has its own orchestration tool built into it called Docker Swarm, but as far as I know, it's not really that popular, I've done some research, most people these days use another tool called Kubernetes which is a Google product. Now initially I was planning to include Docker Swarm in this course, but after I did a survey and realized that only 5% of my audience really use Docker Swarm and everyone else uses Kubernetes, I thought that would be a waste of time and that would delay the production of this course. So I decided to drop Docker swarm.

Now why didn't I include Kubernetes in this course? Because Kubernetes is fairly complex and really requires its own course or book, and quite frankly, it's not something that I know a lot about. So in this section, our focus will be on a single host deployment which works for a lot of people. Don't assume that you need a cluster right from the get go. You can always start simple, and if needed you can transition to a cluster.


Codes and Other Notes in this Discussion: 

Deployment Options: 

Single-host deployment 
Cluster deployment 


Cluster Solutions:

Docker Swarm 
Kubernetes 















Getting a Virtual Private Server:

So to deploy our application, we need a virtual private server or a VPS. Now there are a ton of options to get a VPS, we have DigitalOcean, Google Cloud Platform, Microsoft Azure, Amazon Web Services and of course many many more. These are the platforms that I personally have played with. Now out of these, DigitalOcean is the simplest and most beginner friendly one, and as we go down this list, we get more features but at the cost of complexity. So I decided to choose DigitalOcean because I didn't want us to get distracted from Docker and go somewhere else.

Now, all these platforms require you to provide a credit or a debit card to get a VPS. So to execute all the steps I'm going to show you in this section, you will need a debit or credit card unless you already have access to a VPS. But if you don't, don't let that discourage you. I still want you to watch this section so you have an understanding of the deployment process. So if you want to follow along with me, head over to digitalocean.com, create a new account and then watch the next lesson.




Codes and Other Notes in this Discussion: 

Digital Ocean 
Google Cloud Platform (GCP)
Microsoft Azure 
Amazon Web Services (AWS)





















Installing Docker Machine: 

Once we have a server, we need to use a tool called Docker machine to talk to the Docker engine on that server. So this way we can execute Docker commands in our terminal and our commands will be sent to the docker engine on our server. So head over to github.com/docker/machine/releases. On this page you can find the latest release, at the time of recording this lesson it's version zero point sixteen point two. Now down below you can find the installation instructions for various platforms, it's super easy, we just have to copy this command and execute it in the terminal.

So copy, paste it here, done, alright, Docker machine is installed, let's verify it, so Docker machine dash dash version. Perfect. So go ahead and install Docker machine, when you're done, come back and watch the next lesson.



Codes and Other Notes in this Discussion: 

https://github.com/docker/machine/releases

docker-machine --version 














Provisioning a Host:

Alright, now we're going to use Docker machine to create a virtual private server on DigitalOcean. So we type Docker machine, create, now this is going to be a long command, so we're going to split it onto multiple lines. Now if you're on Linux, you can do this by typing a backslash at the end, if you're on Windows, you need to type a back tick, okay? And of course this only works in PowerShell, not command prompt. Okay?

So backslash, now on the second line we type dash dash driver, now Docker machine has drivers for different platforms, if you Google docker machine drivers, you'll find this page, so we have drivers for AWS, Microsoft Azure, digital ocean and so on. Now if you have a server in your own network and not in one of these platforms, we can set the driver to none. But here we're going to use digital ocean, now another backslash or back tick to go to the next line, now here we need to supply an option specific to digital ocean. So if you use AWS or another platform, your options are going to be different. So that option is double hyphens, digital ocean, hyphen, access, hyphen, token.

So now we need to go to digital ocean, and get an access token so Docker machine can talk remotely with digital ocean. So here on digitalocean.com, on the left panel, we expand account and go to API, and of course this user interface might change in the future, so if you cannot find it, just search for API on digital ocean. Now here we can generate a personal access token, let's go ahead, we're going to give this a name bitly, the name of our application, generate token, and here's a token. Now this token is visible only once, so once we move away from this page and come back, we're not going to be able to see it. So copy this, and keep it somewhere safe if you want to reuse it in the future.

Okay? Now, back to the terminal window, we're going to pass that token here, and then add a space and a backslash at the end, so we can type the rest of the command. Now at the time of recording this lesson, there is a problem on digital ocean, we cannot install the latest Docker engine on the server. We have to use an earlier version. Now put a link down below, if you open that link, you're going to see this page, this question was asked in 12/21/2020, so it's fairly new.

Now down below you can see the solution, so you have to add this extra option when creating a Docker machine. So with this we can specify the engine install URL, so at this address, look, we're installing version 19 whatever. Hopefully this issue will be fixed in the future and you don't have to add this option. So let's copy this line as well including the backslash and of course if you're on Windows, you're going to replace that with a back tick. Now on the last line, we're going to give this server a name, we can call it vidlin or whatever.

Now, Docker machine is going to create a virtual machine by that name, and on that machine, it's going to install docker. So then we'll be able to talk to that docker engine remotely on our machine. Now this is going to take a little while, so I'll be right back. Alright, our server is ready, now look at the output, the Docker machine was provisioning this with ubuntu, this is the default image. Now if you don't want to use ubuntu, you can use a different image.

So, back to this page where we can see the list of drivers, let's look at the digital ocean driver. Now down below you can see all the available options. So we have an option for specifying the image to use. Okay? Now, once Ubuntu was installed, then Docker machine tried to install Docker.

So now we have docker up and running on this machine, and we can talk to it remotely. Also, if you go to the homepage of your account on digital ocean, you can see a new droplet called withly. A droplet on digital ocean means a server. Now over here, you can see the public IP address of the server. So we can copy this, open the browser, currently there is nothing there because we haven't deployed our application yet.

So now that we have a server up and running, next I'm going to show you how to connect with the server.




Codes and Other Notes in this Discussion: 

docker-machine create \
--driver digitalocean \
--digitalocean-access-token b3f0.......... \
--engine-install-url \
vidly 
















Connecting to the Host: 

Let's see how we can connect to our server. First we run Docker machine LS, with this we can see all of our Docker machines, so currently we have a single machine called vidlin that is not active, we'll talk about that shortly. Here we can see the driver for this machine which is digital ocean, our machine is up and running, here we can see the IP address of this machine, In the next column, which is called swarm, let me resize the window so we can see. Alright, so here's the swarm column, this tells us if this machine is part of a swarm or not. A swarm as I told you at the beginning of this section represents a cluster.

So this is a single host, it's not part of a cluster. In the next column, you can see the version of Docker engine running on this machine, and currently we have no errors. Great. Now, to connect to this machine, we're going to use SSH. SSH is short for secure shell and it's a protocol for connecting with servers.

So using SSH or secure shell, we can open a secure shell session with our server. Now, if you have worked with SSH before, you probably know that setting it up is a bit of pain. There are a number of steps you have to follow so you can connect to a server using SSH. Now the beauty of docker machine is that it abstracts away all this complexity from us. So here we can simply type docker machine, SSH, and then type the name of our machine which is bitly.

Now, wait a sec, okay great, now we are logged into our machine using ssh, and all of this was set up when we created this docker machine. So as you can see, I'm logged in as the root user, we can look around, there's currently nothing in my home directory, let's get out and run ls again. So here we can see the file system for Ubuntu. Our application is not deployed yet, that's why we don't have the app directory. So looks like our server is in a good shape, next we're going to talk about defining the production configuration.




Codes and Other Notes in this Discussion: 

docker-machine ls 

docker-machine ssh vidly 


















Defining the Production Configuration: 

Alright, this compose file we have created is great for development but it's not ideal for production. For example, in our production environment, we don't need this volume mapping. Because we add this only for sharing our source code with our container. Similarly, we don't need this other container because we don't want to continuously run our tests in a production environment because this is going to slow down our server. It's going to unnecessarily use resources.

So we're going to create a separate compose file for our production environment. So I'm going to copy this file, paste it, and then rename it to docker compose dot prod dot yamo. This is an arbitrary name, you can call it anything. But this is a convention that a lot of people follow. Now, let's modify this file bit by bit.

So for our web service, we're going to keep the build property, but I want to use a different port mapping. I don't want to use port 3,000 on the host. Because I want to type the host name in the browser and bang, I want to see the application. So we want to map port 80 of the host to port 3,000 of the container. Now, we don't need the volume mapping, good.

Next, we're going to get rid of the web tests service, now for our API, we're going to keep the build property, I'm going to keep the port mapping as well. Next we have our environment variable, this is totally fine, but we don't need this volume either. Good, now our database service is in a good shape, we don't need to make any changes here, but I want to add an extra property to each of these services, and that is our restart policy. So we set restart, the default value is no, so that means if this container crashes, it's not going to be restarted, our application will be unavailable. So then we have to connect with our server using the SSH and manually restart the container.

We don't want to use this, we want to use one of the other possible values. Earlier we talked about the compose file format, so if you Google compose file format, you can find this page. Now over here, we can see version three of compose files, here we can see all the valid properties and their values, we're going to look at restart. So where is it? It's right here.

So here are all the possible values. We have no always on failure and unless stopped. Always will always restart the container, on failure will only restart it if the container crashes. And unless stopped is kind of similar to always, the difference is that it will only restart the container if we manually stop it. So I'm going to change this to unless stopped.

Now I want to use the same restart policy for the other services. Good, So now we have a compose file for our production environment, similarly, we could create a compose file for our test and staging environments.


Codes and Other Notes in this Discussion: 

docker-compose.prod.yml: 

version: "3.8"

services: 
  web: 
    build: ./frontend 
	ports: 
	  - 80:3000
	restart: unless-stopped
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	command: ./docker-entrypoint.sh
	restart: unless-stopped
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db
	restart: unless-stopped
	
volumes:
  vidly: 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
Reducing the Image Size: 

So here are the images I have on this machine. Now look at our web image, look at the size of this image, it's 300 megabytes. In this lesson, I'm going to show you a cool technique to reduce this image size beyond your imagination. First, I have to give you a bit of background about react. So back to our project, let's go to the front end folder and open up package dot json, so here in the script section we have a bunch of commands, you have seen start and test so far, but we also have the build command for creating optimized assets for production.

So, we can go to our front end directory, and run npm run build. This is going to create optimized assets for production. Now as you can see here, all these assets are stored in the build folder. Now what if you don't use react? Well, these days a lot of frameworks have the same concept.

They have a tool for creating optimized assets for production. How you use that tool is probably different, but that is irrelevant. So back to our project, here in the front end folder, now we have this build folder that contains all the assets we need for deploying this application. So we can put these assets inside the web server and serve them. Now, let's see how we can create an optimized image for this front end application.

So let's look at our docker file, what are we doing here? We are starting from a note image so we get access to npm for installing these dependencies. But what if we already have these dependencies? Then we don't need npm, we don't need node. All we have to do is copy these files onto a web server, okay?

So this is where we need to create a separate docker file for production because the changes we're going to make only make sense in our production environment, not for development. So, back to our project, I'm going to copy this file, paste it, and then rename it to docker file dot prod. And of course we could call this anything, this is just a convention. Now in this file, first I want to get rid of all these white spaces because I want to restructure this code in a slightly different way. So on the top, we are starting from note, we're going to add an additional keyword here as to give this stage a label, we can call this anything like build stage.

So in a docker file, we can have multiple stages, we can have a build stage and a production stage. We can also add a comment here to make our code more readable, saying step one, build stage. Now, while building this application, we don't need this user, this is only used for serving it, right? So I'm going to cut these two lines from here, and move them down below, we'll get back to them in a second. So we create our application directory, copy all the files, install the dependencies, copy the other files, now we don't need to expose a port here, again this is used for running the application.

So I'm going to cut this line and move it here, now instead of npm start, we're going to run npm build because in the build stage we just want to build our application. Also, we're going to replace the command instruction with the run instruction because as I told you before, we can have a single command instruction in a docker file, and this is used for specifying a default command when our container starts. So let's get rid of this, we're going to use run npm run build. So at the end of this stage, our application is built. Now we're going to start the second stage.

How? Well, we're going to add another from statement, but this time we're not going to start from node, because for our production or for running our application more accurately, we don't need node, we just need a web server. There are many web servers out there, we have nginx, apache, IIS for Microsoft and so on. In this lesson, I'm going to use nginx because it's a very popular and fast web server. Now nginx has different tags, I'm going to use 1.12 dash alpine.

So this is nginx 1.12 based on alpine linux, super small. Now if you want, we can give the statement a label as well, so we can call this production stage. We only need this if we're going to refer to this later in this file. We're not going to do that so I'm going to delete it. So in the second step, which we can specify here using a comment, we can say step two, this is for production, we're going to start from a web server image which is far smaller, then we create our user, now we're going to copy our files from the build stage.

So here we add an option called from, and this is where we reference our build stage. That is why we gave it a label up here, okay? So we're going to go to our build stage and copy all the files from slash app slash build to slash usr slash share slash nginx slash html. Now this path you see here is a standard path for serving files using nginx, you can find this in nginx documentation on Docker Hub. Now instead of exposing port 3,000, we can expose port 80 which is the default port for web traffic.

Now finally we can specify a default command or an entry point, in this case I'm going to use an entry point because every time we start this image, I want this command to be executed, there are no exceptions. So we're going to run nginx and give it a couple of arguments, the first one is dash g, the second one is daemon off, with a semi colon at the end. Now once again, you can find these arguments in the documentation of nginx, there is nothing magical here. So now that we have a Docker file for production, let's build an image using this Docker file. So here in our front end directory, we're going to run docker build dash t, I'm going to call this image, withly, underline web, underline opt meaning optimized.

Now we need to specify our docker file, so dash f docker file dot prod and then we use a period to specify the build context. Alright, our build is ready, so let's look at our images, take a look. Here's our new image, look at the size of this image, it's only 16 megabytes, it's a huge improvement. So when deploying this application, instead of transferring a 300 megabyte image, we transfer a 16 megabyte image. Now there is one more step left.

So in our front end directory, we have two docker files, one for development, the other for production. Now, here we have two compose files as well. We have to go to our production compose file and tell this compose file that we want to use the production docker file and the front end directory. So we are going to expand this build property and give it two sub properties, one of them is context, which we set to the front end directory, the other is docker file. So what we had earlier was the shorthand syntax, this is the long hand syntax.

Here we're going to type our docker file name, docker file dot prod and we're almost done. Now we should also change the port mapping because now in our production Dockerfile we're exposing port 80, not port 3,000, right? Okay, good. Now let's verify it's working before moving on to the next step. So, back to the terminal, let's get out of the front end directory, we're going to run docker compose build, but first we have to specify our compose file.

Docker compose dot prod dot yml. Like this. Let's go, good, the build was super fast because everything was in the cache. So let's look at our images one more time, so here is our new web image, look at it's size, 16 megabytes, beautiful. So now with this amazing optimization, we are ready for deploying our application, and that's what we're going to do next.




Codes and Other Notes in this Discussion: 


Dockerfile: (Initial Setup)

FROM node:14.16.0-alpine3.13 
RUN addgroup app && adduser -S -G app app
USER app
WORKDIR /app
COPY package*.json ./
RUN npm install 
COPY . . 
EXPOSE 3000
CMD ["npm", "start"]



Dockerfile.prod: 
#Step 1: Build Stage 
FROM node:14.16.0-alpine3.13 AS build-stage 
WORKDIR /app
COPY package*.json ./
RUN npm install 
COPY . . 
RUN npm run build 

#Step 2: Production
FROM nginx:1.12-alpine AS 
RUN addgroup app && adduser -S -G app app
USER app
COPY --from=build-stage /app/build /usr/share/nginx/html  
EXPOSE 80 
ENTRYPOINT [ "nginx", "-g", "daemon off;" ]



docker build -t vidly_web_opt -f Dockerfile.prod . 



docker-compose.prod.yml: 

version: "3.8"

services: 
  web: 
    build: 
	  context: ./frontend 
	  dockerfile: Dockerfile.prod 
	ports: 
	  - 80:80
	restart: unless-stopped
  api: 
    build: ./backend 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	command: ./docker-entrypoint.sh
	restart: unless-stopped
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db
	restart: unless-stopped
	
volumes:
  vidly: 
  
  
  
docker-compose -f docker-compose.prod.yml build 


















Deploying the Application: 

Alright, now the fun part. We're going to deploy this application. First we're going to bring up the list of our Docker machines. So we have this vidla machine on digital OSHA. Now this Docker machine has another command called env for seeing the environment variables that we need to set to talk to this vidla machine.

So take a look, here are a bunch of environment variables, now we're not going to manually set these one by one, instead we're going to execute this last command. So let's see what's going on here. We're using the eval command on Linux, and giving it this argument. You have seen this syntax before, right? So if you have a dollar sign and a pair of parenthesis, whatever is inside the parenthesis will be evaluated first and then passed as an argument to this eval command.

So inside the parenthesis, we're getting all the environment variables to talk to this machine. Now, we pass all of those to eval and eval will execute them. So, let's copy this line and paste it, good. Now in the past we had another command called activate, so we could activate the machine, I don't know why they removed this, this was much easier to work with. But anyway, so now that these environment variables are set, any commands that we type here will be sent to the Docker engine on this machine.

So our Docker client, this Docker client here will be talking to the Docker engine on this machine. Take a look, so Docker images, look, it's getting a little bit slower because the command is going over the network. So on this machine we don't have any images. Now, here in the project directory, to deploy our application, all we have to do is run docker compose up, As simple as that. So docker compose up, now we're going to supply our production compose file, so dockercompose.prod.yml, and I'm also going to use the detach mode, I made a mistake, I should have put dash d at the end.

So let's bring up the last command, so up dash d. Alright, good. So now, it's creating this network, this volume, it's trying to build an image for our back end I believe, but here we might get a permission error, let's see if it happens or not. Alright, we got a permission error saying missing write access to the app directory. To understand why we're getting this error, let's go back to one of our Docker files.

So, this is the Docker file for our back end project, now look at this instruction, it worked there. In the older versions of Docker, this instruction didn't respect the user instruction. So even though we set the user here, workdir created this directory using the root user. And the root user is the only user who can write to this directory. Now this issue is fixed in the latest version of Docker, because the latest version of Docker uses a different build tool called the build kit.

If you have noticed, when we build images locally, our build output is different from what you see here. This is the old build tool. The new build tool which is called the build kit gives us colorized output, so some of the lines are blue, you have noticed that, right? So the reason we're experiencing this problem here is because on this VPS we're using an older docker engine, and this is a temporary problem with digital ocean, hopefully you will not encounter this in the future, so you can install the latest version of docker engine and everything will work magically. So what should we do here?

Well, we have to go back to our docker file, and instead of relying on the work dot instruction to create this directory for us, we have to create this directory using the root user, and change it's owner to the app user. So before we set the user, we have to run mkdir slash app, and using this command, we can change the owner of this directory. So we're going to set the owner user to app, and the owner group to app as well. And here's the directory. Okay?

So we create this directory and change it's owner to app, then we set the current user to the app user and everything else is fine. So with this change, let's bring up our application one more time and see if we encounter any other issues. So back to the terminal, we're going to run Docker compose, then we're going to give it our production yamo file, we're going to bring our application up in detach mode, and you want to use the build option so our images are rebuilt.





Codes and Other Notes in this Discussion: 

docker-machine ls 

docker-machine env vidly 

eval $(docker-machine env vidly)

docker-compose -f docker-compose.prod.yml up -d 



backend/Dockerfile: 

Dockerfile: (Initial Setup)

FROM node:14.16.0-alpine3.13 

RUN addgroup app && adduser -S -G app app
USER app

WORKDIR /app
COPY package*.json ./
RUN npm install 
COPY . . 

EXPOSE 3001

CMD ["npm", "start"]



Dockerfile:
FROM node:14.16.0-alpine3.13 

RUN addgroup app && adduser -S -G app app
RUN mkdir /app && chown app:app /app  
USER app

WORKDIR /app
COPY package*.json ./
RUN npm install 
COPY . . 

EXPOSE 3001
CMD ["npm", "start"]



docker-compose -f docker-compose.prod.yml up -d --build 

























Troubleshooting Deployment Issues: 

Okay, our application is up, so let's see if we can access it in our browser. First we need to find the IP of our server. We can go to digital ocean's control panel or we can run Docker machine ls, so here's the IP address of our web server. So let's copy this, now back to our browser, let's go to this address, nothing is here. What is going on?

That's a great opportunity for troubleshooting. So our front end application is not up, but what about our back end application? Our back end application is available on port 3,001 slash API. Okay, so our API is running, we can also go to API slash movies, so all movies are fetched from our database, that part of the application is working perfectly fine. The only problem is in the front end.

So back to the terminal, let's look at our running containers, okay, so here's the web container running nginx. Now if you look at the status column, you can see that this container is restarting. Let me resize my window to to show you this message. So, okay, it looks really ugly, but I will do my best. So here is the web container, okay, and you can see that this container is constantly restarting.

That's because of the restart policy that we applied. So something is wrong in this container, it keeps crashing and restarting. So what is the next thing we need to do? We need to look at the logs. So, let me resize this one more time, good.

So let's bring up the container list again, alright, we need the id of this container to look at it's logs. So docker, logs, and here's the container. Good. So take a look, there's so much going on here, all these repetitive messages are saying the same thing. We have a permission problem.

And here we're getting a warning saying the user directive makes sense only if the master process runs with the super user privileges. The reason we're encountering this issue is because we are running nginx with a non root user. So, back to our production Dockerfile in our front end project, look, this is where we're creating a custom non root user. Now, we can definitely run nginx with a non root user, but setting it up involves a bit of configuration and some Linux work. It really goes after this Google plus course.

If you're interested, you can read the documentation of nginx on Docker Hub. So to go forward, I'm going to remove these two lines and run nginx using the root user. Of course, as I told you before, this is not the right way to do this, but I don't want us to get distracted from what we are doing. So now we have to go back to the terminal and redeploy our application. So docker compose dash f docker compose .prod.

Yamo, we're going to bring the application up and use the build switch to rebuild each image. Okay, it's going to take a while, so I'll be right back. Alright, our application is up, let's verify that all containers are running. Great. So now if you look at the status column, you can see that the web container is running.

Great. So back to the browser, alright, we can see the homepage of our application, but we cannot fetch the movies. So we need to troubleshoot one more time. Look, I deliberately included these issues in this lesson, so you learn troubleshooting skills. I could have executed these steps in a way that you could never see these, and everything looks so clean and polished.

So how do we troubleshoot this? We're going to bring up chrome dev tools and look at the requests sent to our API. So, here's chrome dev tools, in the network tab we select sh r to look at our API requests, then we refresh by pressing command and r on mac or control and r on Windows. Okay, here's the bad boy, take a look, look at the URL of this request, it's going to local host for 3,001, this is only for the environment. Our front end application running on the web should have sent a request to local host, it doesn't make sense.

So, back to our project, let's go to the front end folder, source, services, and API dot JS. This is where we have all the code, we're talking to our API. So look at this line of code, we are saying if this environment variable is set, use it, otherwise, use local host for 3,001 slash API as the default value. So what we need to do now is to set this environment variable in our production Dockerfile. So we're going to go to Dockerfile dot prod for front end, now in the build step, this is where we need to set the environment variable, because in react, environment variables are processed during the build stage.

So the build tool that comes with react reads all these environment variables and hard codes them in the final code, okay? So before running the build, we can say environment, here's the key, we're going to set this to the IP address of our server. So let's run Docker machine ls, alright, here's the IP of our web server, we're going to grab that, first we start with http, add the IP, now the port is 3,001, and here we should also add slash API. So save the changes, now we need to redeploy. So here's our last compose command, again we're using the build option, so our images are rebuilt.

Let's go with that, alright, perfecto. So let's go back to the browser, refresh, and here are all the movies, great. So our application is up and running on our internet. So let's quickly recap. If we put aside all the little issues that we encountered in this lesson, deploying Docker applications is actually very easy.

First we have to activate a docker machine, so we run docker machine ls, let's say we want to deploy to this machine, this could be our production machine, we could have another docker machine for our test environment, we could have another one for our staging environment and so on. Once we know where we want to deploy this application to, we run Docker machine. Env with the machine name, so we get the command that we need for activating that machine. We execute this command, good, now this machine is active, so any commands that we execute using this Docker client will be sent to the Docker engine on the target machine. So here we can bring our application using Docker compose up as simple as that.

And if you think this is too tedious and you have to execute multiple commands, you can simply put these commands in a shell script like deploy dot s h and every time you want to deploy you just run that script. So in that script we are going to activate your production machine and then run docker compose auth with the right options.




Codes and Other Notes in this Discussion: 

docker-machine ls 

docker-machine env vidly 

eval $(docker-machine env vidly) 

docker-compose up 



















Publishing Changes: 

Earlier in the course, I told you that you should properly tag your images before deploying them to various environments. So in this window, we are still talking to our production machine. Let's run docker ps, okay, look, on this machine, we have this image called with the underline web, but we don't know what version of our application is included in this image. This is the problem I was talking about. So if we encounter inconsistencies amongst different environments, let's say our application works in a certain way in the staging environment but not the same way in production.

We don't know what version each environment is running, so we want to properly tag our images. How? Back to our production compose file, look at the definition of the web service. Here we have the build instructions, but in addition to build, we can also add the image property to tag this image. So we can say this image is going to be called web, colon, let's say this is version one.

Now similarly, we can add the image property to our API service and say once this is built, it should be called vidnet API version one. Save the changes, now we're going to redeploy. So let's run docker compose again using the build option, because we want to rebuild our images, right? Let's go ahead with that. Alright, our application is redeployed, so let's run docker ps one more time, great.

Now we can clearly see in production we are running Vidly version one. Now tomorrow, if we decide to deploy a new version, we have to go back to our compose file and manually update this and then redeploy. This is not ideal, that's why we use continuous integration and deployment tools. Using continuous integration and deployment tools, we can automate our deployment process. So we can configure our tool to check out the latest version of our code from our GitHub repository, then it will build our image and tag it according to the latest build number.

Or we can have our tool tag our images based on the latest commit. This way we can easily trace what version we are running in each environment. So instead of an arbitrary number here, we will have our git commit ID or the build number. That really goes outside the scope of this course and it's something you need to learn on your own. So that brings us to the end of this course.

I hope you have learned a lot and I'm going to see you in my other courses. If you enjoyed this course, please support me by telling others about it. I really, really appreciate your support. Once again, thank you and I wish you the best.




Codes and Other Notes in this Discussion: 

docker-compose.prod.yml: 

version: "3.8"

services: 
  web: 
    build: 
	  context: ./frontend 
	  dockerfile: Dockerfile.prod 
	image: vidly_web:1
	ports: 
	  - 80:80
	restart: unless-stopped
  api: 
    build: ./backend 
	image: vidly_api:1 
	ports: 
	  - 3001:3001
	environment:
	  DB_URL: mongodb://db/vidly
	command: ./docker-entrypoint.sh
	restart: unless-stopped
  db:  
    image: mongo:4.0-xenial 
	ports:
	  - 27017:27017
    volumes: 
      - vidly:/data/db
	restart: unless-stopped
	
volumes:
  vidly: 
  
  
    
docker-compose -f docker-compose.prod.yml up -d --build 