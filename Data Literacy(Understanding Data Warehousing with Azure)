

Course Overview:

Hi everyone. My name is Thomas LeBlanc, and welcome to my course, Data Literacy: Understanding Data Warehousing with Azure. I am a data solutions architect residing in Baton Rouge, Louisiana. I manage the storage of data and help get data to end users, that is a daily part of my job and I love it. Azure has brought data warehousing to a new level called the modern data warehouse and this has given us a multitude of opportunities as data professionals. In this course, we're going to cover data warehousing services provided by the Azure Cloud platform, some are like the traditional data warehouses we have been building over the years or decades and others are to help speed up getting data to end users. Some of the major topics that we will cover include Azure Data Lake storage, Azure Data Factory and Synapse Analytics, Azure DataBricks, and Azure Analysis Services and Power BI. By the end of this course, you'll know where to store data warehouse data for fastest retrieval, where to run analytical processing, and a go‑to service for creating machine learning models. Before beginning this course, you should be familiar with data warehousing concept, structured, semi‑structured, and unstructured data, as well as Azure fundamentals. I hope you'll join me on this journey to learn the modern data warehouse with Data Literacy: Understanding Data Warehousing with Azure course, at Pluralsight.Azure has 
================================================================================================================================================================

Modern Data warehouse storage:
-----------------------------

Azure Data Storage:

Welcome to Data Literacy: Understanding Data Warehousing with Azure. My name is Thomas LeBlanc. I'm a Data Warehouse architect. You can reach me on Twitter @TheSmilingDBA or my blog, Thomas‑LeBlanc.com. Course Overview. This course is going to talk about Azure data warehousing. It's going to look at the modern data warehousing and Pluralsight's data literacy category. The modern Data Warehouse will start with storage. We'll look at the Gen2, or version 2, of the Azure Data Lake Storage. Next, we'll look at the movement or orchestration of data with Azure Data Factory. Services for the Azure Data Warehouse include Azure HDInsight for Hadoop, Databricks, which is your Spark analytics, which is what most of the modern data warehouse world is moving to, and lastly, Synapse. This is Microsoft's Azure Data Warehouse analytics workspace where you have a combination of storage, compute, SQL, as well as Spark with the orchestration of ADF, and eventually, the visualizations with Power BI. Data storage. Analyze data warehousing with Azure. What is the modern Data Warehouse? First, it's using cloud services to manage data around big data analytics, relational, as in online analytical processing, streaming of real‑time data, and of course, all this leads to storing historical data on cheaper and scaling‑out and scaling‑in‑type storage. It also includes analytics. Along with this storage, we need some sort of analytical processing. This processing needs to be scalable with compute. And for today, business and research need the capabilities of getting to data quickly and being able to produce results very fast. The modern Data Warehouse is built on this foundation, big data, OLAP, and Spark technologies. A good view looks to the left at the data coming in like logs, files, media, which is unstructured data, but then the structured data comes from your business or some sort of custom application. You ingest this data with Azure Data Factory and store it in the Azure Data Lake Storage. Then you really have two paths, one of them is a processing path, the other, PolyBase, is a way that Azure Synapse Analytics can put a schema on top of this raw data storage in the Azure Data Lake Storage. Azure Databricks is this program‑enabled analytics area where notebooks are able to be created, jobs run, the cluster for the Spark can be expanded or contracted, and there's many language to use to get to what some people are referring to as AI, but mostly is what's called machine learning. These models eventually can be visualized through Power BI or stored in Azure Synapse Analytics. The number three for Azure Synapse Analytics is actually like Azure SQL database except in an MPP, a massive parallel processing system for data warehouses. So Azure SQL DW is for your relational and transactional‑type databases, but Synapse, formerly called Azure SQL DW database, is now called Synapse. This can be formulated into Azure Analysis Services, which is external or internal to Power BI. Power BI Premium now has this stored internally. It also exposes it to many things through the XMLA endpoint.


Storage Options:
Our data storage option started in Azure with blob storage, so pretty much you could store any set of data into a blob. Then eventually this Azure SQL data warehouse came along for a relational storage and eventually moved to the word synapse. Databricks, which does have storage, is mainly the analytics engine, but just so you know, you can use a storage feature of Databricks, but mainly the feature of Databricks is in‑memory data, which makes it faster to process. Lastly, is the current Azure Data Lake storage, which is in Gen2. Azure Data Lake storage can take almost any type of file and store it in some sort of structured or unstructured system. Azure Data Factory is the Orchestration tool to ingest the data and then sync it into the Data Lake Storage. ADLS is a short name or acronym and it's the repository for all your data warehouse data, it could stage unstructured data, raw data, and is really just an extension of the blob storage. Gen2 enabled Hadoop so the access is now open to Hadoop, which is a great, great tool for those that are already using that. First, you need to create a storage account, it needs things like your subscription, resource group, a name, the location, your performance, which could be premium or standard, and then in the bottom right, what kind of storage structure you want to use, the Gen2, which is listed as V2, V1, which is previous to V2, and BlobStorage, which is the original storage structure. That blob was initially created when Azure came out and the V1 started building onto some sort of Azure Data Lake. V2, of course, is this next generation and it's the go‑to 1 in today's modern data warehouse. So these storage features are like containers, that's what's mostly used, but there are some other features of the Azure Data Lake storage, which are file shares, table structures, as well as queues. The containers are mostly used because you can create folder structures where you can just dump as much data and files that you need to. File shares and tables are carry over from a previous version and people are still using them, mainly the table structures are used like configuration structures for other work that needs to be done, and queues are message storage for any sort of messaging type application you want to write. The great win in storage is that it can scale in and out, it can store data in raw format so that's when people move from extract, transform, and load to extract, load, and then transform, but the benefits is there is no longer a bottleneck to do the transformation before storing the data. You can grant different permissions on this and there is some diff options that are coming into play with Delta Lake, so you can look at the deltas of the before and after of a storage and there is no infrastructure administration costs. Now this is kind of a testy area because people think that might be losing their jobs, but really you're transitioning from managing your hardware to managing the Azure implementation of the storage. So of course, there are formats that can be stored in here, the first is structure like a CSV file, text file, or some sort of delimited text file. The unstructured is basically a blob of data. The structured or semi‑structured might be your JSON or XML files, your images, and then some of the new formats are more compressed and column store types are ORC, AVRO, and Parquet. Those are some really advanced and very fast moving structures that you'll see more and more in the modern data warehouse


Demo:Azure storage

Let's look at the demonstration of navigating the Azure storage. All right, I'm in my portal and logged into my Azure account, and I'm going to go to the top right where it says Create Resource. I could go there or on my bar already is one called Storage Accounts. I can actually create a new one or look at an existing one. So I'm going to bring up one that's already been created and you'll see that there is a resource group called ThomasML, it's stored its primary available and secondary available, its location is South Central, and then I have a subscription, and to the right, it's the standard performance access tier and it has a replication, and then the storage format in your far right. You can also see in the upper right a JSON view to see how something like this can be created so you can actually automate it. Right in the middle of this are our four types of storage containers massively scalable Data Lake storage, file shares, a serverless SMB or NFS file shares, your table data storage, and your queues for messaging. You also can see tools that are available like the Storage Explorer or PowerShell. You can actually do a CLI language inside of Azure and some other programming languages. It even gives you a quick look at the performance and issues that have been going on throughout your Data Lake Storage. I'll click on Containers and what you'll notice about containers is it's like a folder structure, we have two private and one container, and you can actually drill in there to see what files might be put inside of there. So let's click on Storage accounts, I'm going to create a new one, and we're going to get this prompt, the subscription, do we want to use an existing resource group or create a new one, a storage name, the location, standard or premium storage, all depending on the type of speed you want, the account kind, which is that BlobStorage, Gen1, or Gen2, and then of course, how you want to replicate it and this is, all of this is going to create some sort of cost for you so you need to make sure you research this before you create it. But that's really all there is to creating it other than setting up your network, which is really another part of the Azure network.

================================================================================================================================================================

Flow of Data and Massive Parallel Processing: 
---------------------------------------------

Welcome back to Data Literacy: Understanding Data Warehousing with Azure. My name is Thomas LeBlanc. I'm a data warehouse architect. You can reach me @TheSmilingDBA on Twitter or my blog, Thomas‑LeBlanc.com. Flow of data. The data flow starts in Azure with Data Factory. This is where we extract, load, and transform data. These activities used to be called extract, transform, and load, but with the modern data warehouse, we usually extract the data first, load it somewhere, and then transform it for analysis, also, Azure Synapse is the data warehouse structure for using Azure services for large data warehousing. This also encapsulates reporting, orchestration, and managing data at a data warehouse large scale. Azure Data Factory, or ADF, is a cloud‑based service. You can either extract data from on‑prem sources or in the cloud. There's also a way to integrate SSIS into it, and because of its centralization as Azure's orchestration or ingesting software, it now has connectors to all of the popular data sources. So to ingest data, we're either going to use an ETL or an ELT process. You're either going to determine before or after the load if you're going to transform the data. What does transform mean? Well, it might mean looking up a key from a dimension table for a fact, or you might need to transform a character field to an integer field, or you might need to do some sort of aggregation before loading it into a reporting area. The benefits of ADF for ingestion is it does connect to most sources. And if there's some it does not, I'm sure Microsoft is working to get them implemented. You can scale up or out. So the idea behind Azure or cloud services is that you don't have to buy new hardware. The hardware is managed by Microsoft, and you just access more compute or more storage. There is a scheduler. It's called Triggers, so your mindset about how to execute ETL or ELT has to change a little in Azure. And of course, with Azure, there's plenty of monitoring, and it does most of the job for you. 



Azure Data Factory:
-------------------

Azure Data Factory does the ingestion of data whether it be external data or on‑premise, it brings it into a cloud for further processing. They label this as an orchestration engine because there are many, many things you can do with it. Rather than just ingesting the data and placing it somewhere, you can actually execute some Azure function to do a different processing or to trigger some other application to extract the data. To connect to this data, usually you have what's called a link service and this could be to an Azure Blob or even a SQL service. There is a place where you create datasets, this is the structure, not the connection. Link services is the connection, datasets is the structure of the data that you're ingesting. And then when you create the different components, you're creating what's called a pipeline so it goes from one end of the process to the other end.



Massive Parallel Processing:
Massive parallel processing. Azure Synapse is an analytic platform, it's the compute system that helps generate analytics on large and large dataset. It uses a distributed system for high performance and large compute for doing your analytics in the cloud. Synapse is the go‑to area outside of DataBricks for your parallel processing to help do and perform data warehousing work. Synapse has a workspace, so everything is encapsulated for your analytics in one tool. You connect to the data type, there are many, many, many different types that you can connect to, you either do it through a database or a link service, you develop through what's called notebooks, this is common in analytics and it's a very popular way to structure your "code" whether it be SQL, Python, or Spark into this markup area which could have documentation, visualization, as well as the code. Integrated in there is Azure Data Factory, or ADF, that's in your integrate icon. And believe it or not, Power BI is integrated in here for visualization. So as you can see, you can connect, develop, integrate, and visualize all in one tool in addition to monitoring and managing this platform, which is in one workspace.



Demo(Azure Data Factory and Synapse Analytics):
Let's look at ADF and Synapse in the Azure portal. In the Azure portal, you can see an icon for Azure Synapse Analytics, as well as one for ADF. If I don't see it there, I can go to the search and type in data factory, and I can go to my Azure Data Factories. Here you can see I have two built, one for dev and one for staging. Now if I click on one of these, I get the essentials of the management of Azure of this Data Factory. You can also see we have V2. But to get to the development, I'm going to go to this button called Author and Monitor. And what this is going to do is going to launch a separate website that I have to then log into, and then I'm brought into a workspace. To the far left is where I manage and scroll through the different menu choices, Data Factory, Author, Monitor, and Manage. On the Quick Start, I have easy ways to quickly copy data, which is a common form of ADF. So if I do copy data, I get a wizard to create a pipeline to just copy data from a source to a destination. I'll click Cancel here and go to Author, and you'll see the area for Pipelines, Data flows, Datasets, and even Power Query. What we have to do is create different links in order to develop our ADF process. The nice thing about ADF is I was able to spin up this dev environment very quickly, and I'm not paying for the resources of the dev. I don't pay until I trigger some pipeline to run. This is Azure Data Factory. I'm going to go back to the main portal, and I'm going to search for Synapse. Now, with Synapse, you have the analytics, and then you have a private link hubs. And Synapse can have a data warehouse outside of the analytics area, but I'm going to click on Azure Synapse Analytics, and you'll see I already have one for our POC created. When I click on it, again, I'm managing the resource in the portal, but to get to the development, I can open the Synapse Studio. Just like ADF, it launches a different website where I can go to perform various things. So in my menu that I expand, I have Home, which gives me some quick shortcuts, along with Data, Develop, Integrate, Monitor, and Manage. In development, you can see here I have a SQL script that was created, a Notebook, as well as a Power BI workspace that was created. So, the nice thing about Synapse is I can go in and create T‑SQL scripts, along with a version of Python, or if I roll this down, I can see Spark Scala, C# with .NET Spark, as well as Spark SQL. So, Synapse encapsulates the connections to data, which can be big data and blob storage or Azure Data Lake storage. It doesn't have to be stored in a database engine, but I can query it through the scripts, Notebooks, as well as connections with Power BI. If I go to Data, what I can do in Data is actually create different types of pools. I can actually create a SQL data warehouse database. I can connect to external data, like a blob or Data Lake. I can ingest or integrate a dataset, and there's even a gallery I can select from. I'm going to go back to the portal, and what you'll see, if we scroll down a little, is in this workspace, there's two types of pools. There's a SQL pool and an Apache Spark pool. The SQL pool, first of all, gives you a built‑in serverless SQL pool to run Spark SQL. In the Apache Spark pools, I have to create a Memory Optimized set of clusters, which are nodes that can perform Spark. This compute I could turn on and off. The free SQL pool lets you run Serverless SQL, connecting to many data sources, but if you want a dedicated one with dedicated resources in a data warehouse database, you'll create your own SQL pool. You could turn it on and off, but it does cost by running it. Always in Azure with Synapse or Azure Data Factory, the compute you don't pay for unless you're using it, but the data storage you will always be paying for. Also in each one of these sets of resources, there is access control, as well as different properties and securities to help secure this. I hope this gave you a good idea of Azure Data Factory, our ingestion model, as well as Azure's Synapse Analytics, which does have the integrated way to use ADF to move data around, along with the Notebooks, Power BI, and data connections.